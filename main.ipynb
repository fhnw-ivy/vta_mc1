{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"7b43dc16d5384c3fb18656e08cda8f1a","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# vta: Mini-Challenge Gradient Descent"]},{"cell_type":"markdown","metadata":{"cell_id":"4d9e4df030584cb0a59a22edaa365ebd","deepnote_cell_type":"markdown"},"source":["![image.png](attachment:image.png)\n","\n","Das Ziel dieser Aufgabe besteht darin, dass Sie ein grundlegendes Verständnis für numerische Näherungsverfahren in höheren Dimensionen erlangen, insbesondere für den Gradient Descent und dessen praktische Anwendung. Hierfür sollen Sie ein Jupyter Notebook erstellen und das MNIST Dataset laden und erkunden. Anschließend sollen Sie ein neuronales Netzwerk erstellen und trainieren, um die Bilder korrekt zu klassifizieren. Es dürfen nur die angegebenen Python packages verwendet werden.\n","\n","Ziel dieser Aufgabe ist nicht nur, Ihre mathematischen Kenntnisse unter Beweis zu stellen, sondern auch die entsprechende Kommunikation und Präsentation Ihrer Ergebnisse. Ihre Abgaben sollen also nicht nur mathematisch korrekt, sondern auch leicht verständlich und reproduzierbar sein. Genauere Angaben zu den Erwartungen an die Abgabe finden Sie in den Auswertungskriterien. Dokumentieren Sie ihren Arbeitsfortschritt und Erkenntnisgewinn in Form eines Lerntagebuchs, um Lernfortschritte, Schwierigkeiten und Erkenntnisse festzuhalten.\n","Die folgenden Aufgabenstellungen präzisieren die einzelnen Bearbeitungsschritte und geben die Struktur des Notebooks vor."]},{"cell_type":"markdown","metadata":{"cell_id":"63647788989a4455be819a584c94e15f","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- **Project Members:**\n","- Dominik Filliger\n","- Noah Leuenberger\n","- Nils Fahrni\n","- Oliver Pejic"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.027242Z","start_time":"2023-04-10T16:34:59.025205Z"},"cell_id":"3e5ae331b5a94682aebeb4aaf613df10","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1795,"execution_start":1681321556768,"source_hash":"67275d9a"},"outputs":[],"source":["import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"cell_id":"c76c9c1033d2469597d1df1324ca9d99","deepnote_cell_type":"text-cell-callout","formattedRanges":[],"is_collapsed":false},"source":["## Aufgabe 1\n","> Laden Sie das MNIST-Dataset mithilfe des torchvision-Pakets (Verwenden Sie das torchvision Paket für diese Aufgabe) und verwenden Sie matplotlib, um sich einen Überblick über die Daten zu verschaffen. Beschreiben Sie das grundlegenden Eigenschaften des Datensets, z.B. wie viele und welche Daten es enthält."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.168041Z","start_time":"2023-04-10T16:34:59.028883Z"},"cell_id":"ba102f8b2ade4e2fa64a24402e77ffdd","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":169,"execution_start":1681321558562,"source_hash":"1a2432ae"},"outputs":[],"source":["# Define transformations to be applied to the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load the MNIST dataset\n","train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Extract the data and labels from the datasets\n","X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n","X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n","\n","# Reshape the data to be of size [N x 784]\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","# Normalize the data to be between 0 and 1\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.531688Z","start_time":"2023-04-10T16:34:59.168488Z"},"cell_id":"08b89596eab14998b3f0cac37d9fff41","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":412,"execution_start":1681321558733,"source_hash":"9230fd10"},"outputs":[],"source":["classes = np.unique(y_train)\n","\n","# Plot the images\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(X_train[y_train == classes[i]][0].reshape(28, 28), cmap='gray')\n","    ax.set_title(f\"Class: {classes[i]}\")\n","    ax.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.531965Z","start_time":"2023-04-10T16:34:59.484990Z"},"cell_id":"5ec8b10b20b14f8a81834723632cfd55","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":61,"execution_start":1681321559145,"source_hash":"d82631cb"},"outputs":[],"source":["print('Number of training examples: ', X_train.shape[0])\n","print('Number of testing examples: ', X_test.shape[0])\n","print('Each image is of size: ', X_train.shape[1])\n","print('There are {} classes: {}'.format(len(classes), classes))\n","print('The data is of type: ', X_train.dtype)\n","print('The labels are of type: ', y_train.dtype)\n","print('The range of the pixel values is [{}, {}]'.format(np.min(X_train), np.max(X_train)))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4d3ae3d4087a4c139034db7db86b6290","deepnote_cell_type":"code"},"outputs":[],"source":["for i in range(len(classes)):\n","    print(f\"Class {classes[i]}: {np.sum(y_train == classes[i])} train examples, {np.sum(y_test == classes[i])} test examples\")"]},{"cell_type":"markdown","metadata":{"cell_id":"47b22bf8f8084b0c9219be6cccd3650d","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Aufgabe 2\n","\n","> Erstellen Sie eine Klasse für ein lineares Layer mit beliebig vielen Knoten. Implementieren Sie die Methoden forward, backward und update mithilfe von numpy. Schreiben sie geeignete Unittests, um die Funktionsweise der Funktion zu prüfen."]},{"cell_type":"markdown","metadata":{"cell_id":"210d081489f546a38fa870a82b5f5b57","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Linear Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532012Z","start_time":"2023-04-10T16:34:59.502315Z"},"cell_id":"5e1cca13622741cea3a51730aa623e6c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":21,"execution_start":1681321559205,"source_hash":"abcefd1b"},"outputs":[],"source":["class LinearLayer():\n","    def __init__(self, input_size, output_size):\n","        self.x = None\n","        self.bias_grad = None\n","        self.weights_grad = None\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.weights = np.random.randn(input_size, output_size) * 0.01\n","        self.bias = np.zeros((1, output_size))\n","\n","    def forward(self, x):\n","        self.x = x\n","        return np.dot(x, self.weights) + self.bias\n","\n","    def backward(self, grad_output):\n","        self.weights_grad = np.dot(self.x.T, grad_output)\n","        self.bias_grad = np.sum(grad_output, axis=0, keepdims=True)\n","        return np.dot(grad_output, self.weights.T)\n","\n","    def update(self, lr):\n","        self.weights -= lr * self.weights_grad\n","        self.bias -= lr * self.bias_grad"]},{"cell_type":"markdown","metadata":{"cell_id":"6309c2f8970942f0a47c09422e878ca8","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Unit Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532292Z","start_time":"2023-04-10T16:34:59.506533Z"},"cell_id":"d8a38c8a7ded498a931489c32eb3ec13","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1681321559222,"source_hash":"89fe3fa4"},"outputs":[],"source":["import unittest\n","\n","class TestLinearLayer(unittest.TestCase):\n","    def setUp(self):\n","        self.input_size = 10\n","        self.output_size = 5\n","        self.linear_layer = LinearLayer(self.input_size, self.output_size)\n","\n","    def test_forward(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        self.assertEqual(output.shape, (1, self.output_size))\n","\n","    def test_backward(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        grad_output = np.random.randn(1, self.output_size)\n","        grad_input = self.linear_layer.backward(grad_output)\n","        self.assertEqual(grad_input.shape, (1, self.input_size))\n","\n","    def test_update(self):\n","        self.linear_layer.weights = np.array([[1, 2], [3, 4]], dtype=np.float32)\n","        self.linear_layer.bias = np.array([[1, 2]], dtype=np.float32)\n","        self.linear_layer.weights_grad = np.array([[1, 2], [3, 4]], dtype=np.float32)\n","        self.linear_layer.bias_grad = np.array([[1, 2]], dtype=np.float32)\n","\n","        self.linear_layer.update(0.1)\n","        self.assertTrue(np.allclose(self.linear_layer.weights, np.array([[0.9, 1.8], [2.7, 3.6]])))\n","        self.assertTrue(np.allclose(self.linear_layer.bias, np.array([[0.9, 1.8]])))\n","\n","    def test_shapes(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        grad_output = np.random.randn(1, self.output_size)\n","        grad_input = self.linear_layer.backward(grad_output)\n","\n","        self.assertEqual(self.linear_layer.weights_grad.shape, (self.input_size, self.output_size))\n","        self.assertEqual(self.linear_layer.bias_grad.shape, (1, self.output_size))\n","\n","unittest.main(argv=[''], verbosity=2, exit=False)"]},{"cell_type":"markdown","metadata":{"cell_id":"5849d35ba6864d238d64b0c2fdfc4659","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Aufgabe 3\n","\n","> Erstellen Sie ein neuronales Netzwerk in numpy mit einem Hidden Linear Layer und einem Output Knoten. Trainieren Sie das Netzwerk darauf, die Ziffer 4 korrekt zu identifizieren (d.h. der Output soll 1 für diese Ziffer und 0 für alle anderen Ziffern sein). Trainieren Sie das Netzwerk auf den Trainingsdaten und evaluieren Sie es anhand von Testdaten. Verwenden Sie eine geeignete Loss-Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl mit einer Abwägung der Vor- und Nachteile. Diskutieren Sie kurz weitere Optionen für Loss und Accuracy."]},{"cell_type":"markdown","metadata":{"cell_id":"0b4958d5f99b494abf7c44e175041363","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Non-Linear Activation Functions\n","\n","- https://medium.com/intuitionmath/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532332Z","start_time":"2023-04-10T16:34:59.513311Z"},"cell_id":"a19b8ffca93345bfb6919411ad331710","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10,"execution_start":1681321559226,"source_hash":"51a7f5de"},"outputs":[],"source":["def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_grad(x):\n","    return x > 0\n","\n","def sigmoid(x):\n","    x = np.clip(x, -500, 500) # clip x to prevent overflow\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_grad(x):\n","    return sigmoid(x) * (1 - sigmoid(x))\n","\n","def softmax(x):\n","    x = x - np.max(x, axis=1, keepdims=True) # prevent overflow\n","    x = np.exp(x)\n","    x = x / np.sum(x, axis=1, keepdims=True)\n","    return x\n","\n","def softmax_grad(x):\n","    return softmax(x) * (1 - softmax(x))"]},{"cell_type":"markdown","metadata":{"cell_id":"2c34690839ff454b96ddb9e1dfb30004","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Loss Functions\n","\n","- https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a\n"]},{"cell_type":"markdown","metadata":{"cell_id":"8a903d4b618f47439f8e7bca19994e83","deepnote_cell_type":"markdown"},"source":["#### Mean Squared Error (MSE)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a5fc641f1b7a429e8abe8745d603ab3f","deepnote_cell_type":"code"},"outputs":[],"source":["def mse(y_pred, y_true):\n","    batch_size = y_pred.shape[0]\n","    loss = np.sum((y_pred - y_true)**2) / batch_size\n","    return loss\n","\n","def mse_grad(y_pred, y_true):\n","    batch_size = y_pred.shape[0]\n","    grad = 2 * (y_pred - y_true) / batch_size\n","    return grad"]},{"cell_type":"markdown","metadata":{"cell_id":"406ed272899e43438a4563dcc66b77be","deepnote_cell_type":"markdown"},"source":["#### Binary Cross-Entropy / Log Loss\n","$$\n","H_p(q)=-\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log \\left(p\\left(y_i\\right)\\right)+\\left(1-y_i\\right) \\cdot \\log \\left(1-p\\left(y_i\\right)\\right)\n","$$\n","\n","- https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n","- https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715\n","- https://stats.stackexchange.com/questions/219241/gradient-for-logistic-loss-function\n","\n","Derivative:\n","- https://peterroelants.github.io/posts/cross-entropy-logistic/"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532406Z","start_time":"2023-04-10T16:34:59.517129Z"},"cell_id":"c4e84f7f7a72426eaaa103a9a459b9a3","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":12,"execution_start":1681321559228,"source_hash":"685d0d1d"},"outputs":[],"source":["def binary_cross_entropy(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n","    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n","\n","def binary_cross_entropy_grad(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n","    return (y_pred - y_true) / (y_pred * (1 - y_pred)) # add eps to prevent division by 0"]},{"cell_type":"markdown","metadata":{"cell_id":"ba66da11f7f448c789e9fc56fd04cc87","collapsed":false,"deepnote_cell_type":"markdown"},"source":["### Training Development Tracking\n","In order to track the development of the training process, we define a class that stores the training and validation loss and accuracy for each epoch. We also define a function that plots the training and validation loss and accuracy for each epoch. This allows us to see how the training process develops over time."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e24f2cdcf5ae4ea9afb3978a9d199588","collapsed":false,"deepnote_cell_type":"code"},"outputs":[],"source":["class NetworkDevelopment:\n","    \"\"\"\n","    Stores the training and validation loss and accuracy for each epoch.\n","\n","    Parameters\n","    ----------\n","    total_epochs : int\n","        The total number of epochs the network will be trained.\n","    \"\"\"\n","    def __init__(self, total_epochs):\n","        self.total_epochs = total_epochs\n","        self.losses = []\n","        self.accuracies_train = []\n","        self.accuracies_test = []\n","\n","    def add_epoch(self, epoch_number, loss, acc_train, acc_test):\n","        \"\"\"\n","        Adds a new development step to the network development.\n","\n","        Parameters\n","        ----------\n","        loss : float\n","            The loss of the current epoch.\n","        acc_train : float\n","            The accuracy of the training data of the current epoch.\n","        acc_test : float\n","            The accuracy of the validation data of the current epoch.\n","        \"\"\"\n","        self.losses.append(loss)\n","        self.accuracies_train.append(acc_train)\n","        self.accuracies_test.append(acc_test)\n","\n","        return f'Epoch {epoch_number}/{self.total_epochs} - loss: {loss:.4f} - acc_train: {acc_train:.4f} - acc_test: {acc_test:.4f}'\n","\n","    def plot(self):\n","        \"\"\"\n","        Plots the training and validation loss and accuracy for each epoch.\n","        \"\"\"\n","        epochs = np.arange(1, self.total_epochs + 1)\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","        ax1.plot(epochs, self.losses)\n","        ax1.set_title('Loss over Epochs')\n","        ax1.set_xlabel('Epoch')\n","        ax1.set_ylabel('Loss')\n","        ax2.plot(epochs, self.accuracies_train, label='Training')\n","        ax2.plot(epochs, self.accuracies_test, label='Test')\n","        ax2.set_title('Accuracy over Epochs')\n","        ax2.set_xlabel('Epoch')\n","        ax2.set_ylabel('Accuracy')\n","        ax2.legend()\n","        plt.show()\n","\n","    def summary(self):\n","        \"\"\"\n","        Prints the final loss and accuracy of the training and validation data and the average improvements per epoch.\n","        \"\"\"\n","        self.plot()\n","\n","        print(f'avg acc change / epoch (Training set): {np.mean(np.diff(self.accuracies_train)):.4f}')\n","        print(f'avg acc change / epoch (Test set): {np.mean(np.diff(self.accuracies_test)):.4f}')\n","        print(f'avg loss change / epoch: {np.mean(np.diff(self.losses)):.4f}')"]},{"cell_type":"markdown","metadata":{"cell_id":"2158077846bd4403bde4c7f7c46beaa8","deepnote_cell_type":"markdown"},"source":["#### Batch Creation\n","\n","The function `get_batches(x, y, batch_size)` creates batches of images and labels from the training set. The number of images in a batch is defined by the batch size. The batch size is a hyperparameter that can be tuned to improve the training process. The batch size is a trade-off between the number of images used for training and the number of training steps per epoch. A larger batch size results in fewer training steps per epoch, but the training process is less accurate. A smaller batch size results in more training steps per epoch, but the training process is more accurate."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"52d080134e5e4a29be081caf9a5cfdf7","deepnote_cell_type":"code"},"outputs":[],"source":["def get_batches(x, y, batch_size):\n","    \"\"\"\n","    Returns a generator that yields batches of size batch_size from the given data.\n","\n","    Parameters\n","    ----------\n","    x : numpy.ndarray\n","        The input data.\n","    y : numpy.ndarray\n","        The target data.\n","    batch_size : int\n","        The size of each batch.\n","    \"\"\"\n","    n_batches = len(x) // batch_size\n","    \n","    # shuffle data before creating batches\n","    idx = np.random.permutation(len(x))\n","    x = x[idx]\n","    y = y[idx]\n","\n","    for i in range(0, n_batches * batch_size, batch_size):\n","        x_batch = x[i:i+batch_size]\n","        y_batch = y[i:i+batch_size]\n","        yield x_batch, y_batch"]},{"cell_type":"markdown","metadata":{"cell_id":"b8ea137f77e241af98f1b042adf1b008","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Single-Layer Network "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532515Z","start_time":"2023-04-10T16:34:59.522530Z"},"cell_id":"0e80b9164ffb467ea390f26d3f34f8ec","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1681321559240,"source_hash":"c8f46953"},"outputs":[],"source":["class SingleLayerNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, loss_function, loss_function_grad):\n","        self.dev = None\n","        self.y_pred = None\n","        self.h = None\n","        self.x = None\n","        self.development = None\n","\n","        self.loss_function = loss_function\n","        self.loss_function_grad = loss_function_grad\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.linear1 = LinearLayer(input_size, hidden_size)\n","        self.linear2 = LinearLayer(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        self.x = x\n","        self.h = relu(self.linear1.forward(x))\n","        self.y_pred = sigmoid(self.linear2.forward(self.h))\n","        return self.y_pred\n","\n","    def backward(self, y_true):\n","        y_true = y_true.reshape(-1, 1)\n","        self.y_pred = self.y_pred.reshape(-1, 1)\n","\n","        grad_output = self.loss_function_grad(self.y_pred, y_true)\n","\n","        grad_output = sigmoid_grad(grad_output) * grad_output\n","        grad_output = self.linear2.backward(grad_output)\n","\n","        grad_output = relu_grad(grad_output) * grad_output\n","        grad_output = self.linear1.backward(grad_output)\n","\n","        return grad_output\n","\n","    def update(self, lr):\n","        self.linear1.update(lr)\n","        self.linear2.update(lr)\n","\n","    def train(self, X_train, y_train, X_test, y_test, lr, epochs, batch_size, threshold=0.5, output=True):\n","        self.dev = NetworkDevelopment(total_epochs=epochs)\n","        \n","        for epoch in range(epochs):\n","            loss_list = []\n","            for x_batch, y_batch in get_batches(X_train, y_train, batch_size):\n","                y_pred = self.forward(x_batch)\n","                y_pred = (self.y_pred > 0.5).astype(int)\n","\n","                loss = self.loss_function(y_pred, y_batch)\n","                loss_list.append(loss)\n","\n","                self.backward(y_batch)\n","                self.update(lr)\n","\n","            acc_train = self.evaluate(X_train, y_train, threshold)\n","            acc_test = self.evaluate(X_test, y_test, threshold)\n","            avg_loss = np.mean(loss_list)\n","\n","            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n","            if output:\n","                print(epoch_str)\n","\n","    def predict(self, X, threshold=0.5):\n","        return (self.forward(X) > threshold).astype(int).reshape(-1, 1)\n","\n","    def evaluate(self, X, y, threshold=0.5):\n","        y_pred = self.predict(X, threshold).flatten()\n","        return np.mean(y_pred == y) # TODO: fix deprecation warning\n","\n","    def summary(self):\n","        self.dev.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"b1816d7fb6ef4ccc8c4fad5dc0ddcba1","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Training on digit 4"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.802387Z","start_time":"2023-04-10T16:34:59.023452Z"},"cell_id":"2294e1e6143c4346b424448f9475f656","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":126,"execution_start":1681321559244,"source_hash":"e4541e7f"},"outputs":[],"source":["y_train_4_binary = (y_train == 4).astype(int)\n","y_test_4_binary = (y_test == 4).astype(int)\n","\n","# use a train set with only 4s\n","X_train_4_only = X_train[y_train == 4]\n","y_train_4_only = y_train[y_train == 4]\n","\n","y_train_4_only_binary = (y_train_4_only == 4).astype(int)\n","\n","# Test 4s only\n","X_test_4_only = X_test[y_test == 4]\n","y_test_4_only = y_test[y_test == 4]\n","\n","y_test_4_only_binary = (y_test_4_only == 4).astype(int)\n","\n","# Not 4s\n","X_train_not_4 = X_train[y_train != 4]\n","y_train_not_4 = y_train[y_train != 4]\n","\n","X_test_not_4 = X_test[y_test != 4]\n","y_test_not_4 = y_test[y_test != 4]\n","\n","# mixed train set 50% 4s and 50% not 4s\n","X_train_not_4 = X_train_not_4[:len(X_train_4_only)]\n","y_train_not_4 = y_train_not_4[:len(y_train_4_only)]\n","\n","X_test_not_4 = X_test_not_4[:len(X_test_4_only)]\n","y_test_not_4 = y_test_not_4[:len(y_test_4_only)]\n","\n","# Train for mixed train set\n","X_train_mix = np.concatenate((X_train_4_only, X_train_not_4))\n","y_train_mix = np.concatenate((y_train_4_only, y_train_not_4))\n","\n","# Test for mixed train set\n","X_test_mix = np.concatenate((X_test_4_only, X_test_not_4))\n","y_test_mix = np.concatenate((y_test_4_only, y_test_not_4))\n","\n","y_train_mix_binary = (y_train_mix == 4).astype(int)\n","y_test_mix_binary = (y_test_mix == 4).astype(int)\n","\n","\n","# print count of 4s and not 4s in the train set\n","print(f\"4s: {np.sum(y_train_mix_binary == 1)} - Not 4s: {np.sum(y_train_mix_binary == 0)}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"65a117722ba446c8bbe3d29d72c26c27","deepnote_cell_type":"markdown"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:35:41.520284Z","start_time":"2023-04-10T16:34:59.802503Z"},"cell_id":"ffe3986c790644d78f9cc8a89e36db54","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":44107,"execution_start":1681321559371,"source_hash":"9b3bf8ed"},"outputs":[],"source":["# Train the network on the mixed train set\n","network_single_mixed = SingleLayerNetwork(input_size=784, hidden_size=512, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n","network_single_mixed.train(X_train_mix, y_train_mix_binary, X_test_mix, y_test_mix_binary, lr=0.00001, epochs=100, batch_size=128, threshold=0.5)\n","network_single_mixed.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"b44b39844d864f0194d435b3f90e8453","deepnote_cell_type":"markdown"},"source":["### Other approaches"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:39:41.997162Z","start_time":"2023-04-10T16:35:41.540447Z"},"cell_id":"ff9e3ed4618b42bba4a7a2f8a9003439","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":141936,"execution_start":1681321603479,"source_hash":"f00c58fa"},"outputs":[],"source":["# Train the network on the normal train set\n","network_single_normal = SingleLayerNetwork(input_size=784, hidden_size=256, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n","network_single_normal.train(X_train, y_train_4_binary, X_test, y_test_4_binary, lr=0.00001, epochs=10, batch_size=64, threshold=0.5)\n","network_single_normal.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.319376Z","start_time":"2023-04-10T16:39:41.980759Z"},"cell_id":"25475be179874f6db64e280b4420e062","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":111868589,"execution_start":1681209876827,"source_hash":"33930783"},"outputs":[],"source":["# test 4s only\n","network_single_four = SingleLayerNetwork(input_size=784, hidden_size=256, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n","network_single_four.train(X_train_4_only, y_train_4_only_binary, X_test, y_test_mix_binary, lr=0.00000001, epochs=10, batch_size=20, threshold=0.5)\n","network_single_four.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"1fd1afe6ec2b4806a7b09e76679014e6","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Testing on digit 4"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.319970Z","start_time":"2023-04-10T16:41:02.153983Z"},"cell_id":"e3793cf8f7c449ceafeb3f7e82acf614","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":111850906,"execution_start":1681209894512,"source_hash":"af04c912"},"outputs":[],"source":["# test the network\n","accuracy = network_single_normal.evaluate(X_test, y_test_4_binary, 0.5)\n","accuracy_4 = network_single_four.evaluate(X_test_4_only, y_test_4_only_binary, 0.5)\n","accuracy_mix = network_single_mixed.evaluate(X_test, y_test_mix_binary , 0.5)\n","\n","print(f\"Accuracy: {accuracy:.4f} - Accuracy 4s only: {accuracy_4:.4f} - Accuracy mixed: {accuracy_mix:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3cbdb137c51e4a8e90b38127cecae689","collapsed":false,"deepnote_cell_type":"code"},"outputs":[],"source":["# maybe training more different sets helps\n","#network_single_all = SingleLayerNetwork(input_size=784, hidden_size=256, output_size=1)\n","#network_single_all.train(X_train_mix, y_train_mix_binary, lr=0.00000001, epochs=10, batch_size=20, threshold=0.5)\n","#network_single_all.train(X_test, y_train_4_binary, lr=0.00000001, epochs=10, batch_size=20, threshold=0.5)\n","\n","#accuracy_all = network_single_all.evaluate(X_test, y_test_4_binary, 0.5)\n","#accuracy_all_4 = network_single_all.evaluate(X_test_4_only, y_test_4_only_binary, 0.5)\n","#accuracy_all_mix = network_single_all.evaluate(X_test, y_test_mix_binary , 0.5)\n","\n","#print(f\"Accuracy: {accuracy_all:.4f} - Accuracy 4s only: {accuracy_all_4:.4f} - Accuracy mixed: {accuracy_all_mix:.4f}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"52a2fcaa93e84e568d3dff2be350f8ed","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Aufgabe 4\n","\n","> Trainieren Sie das Netzwerk mit verschiedenen Lernraten und Größen des Hidden Layers. Verfolgen Sie während des Trainings die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen und entscheiden Sie, welche Wahl von Lernrate und Hidden Layer-Größe die besten Ergebnisse in geringster Zeit liefert."]},{"cell_type":"markdown","metadata":{"cell_id":"63fc7695c08f40cd98c5f11711ff07dc","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Finding the right hyperparameters\n","\n","In order to find the right hyperparameters, we define a function that trains the network with different learning rates, epochs, hidden layer sizes and batch sizes. The result for each combination is then stored and analyzed.\n","\n","This brute force approach is not very efficient, but it allows us to find the best hyperparameters for the network.\n","\n","> **Note:** The brute force process takes a long time to complete. Therefore, we have commented out the code that runs the brute force process. The best result is described further below."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"78bb1492853842348a6335d8f09ed5ff","collapsed":false,"deepnote_cell_type":"code"},"outputs":[],"source":["def get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes):\n","    return [(e, b, l, h) for e in epochs for b in batch_sizes for l in learning_rates for h in hidden_sizes]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320020Z","start_time":"2023-04-10T16:41:02.154130Z"},"cell_id":"b961ccc321fa421daeab9d25d610b8f0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1118307,"execution_start":1681322159279,"source_hash":"f01f4d14"},"outputs":[],"source":["import time\n","\n","epochs = [10, 20, 30]\n","batch_sizes = [2**i for i in range(4, 11)]\n","learning_rates = [10**-i for i in range(1, 8)]\n","hidden_sizes = [32, 64, 128, 256, 512]\n","combinations = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","print(f\"Testing {len(combinations)} parameter combinations\")\n","\n","results = []\n","for i in range(len(combinations)):\n","    e, b, l, h = combinations[i]\n","    network = SingleLayerNetwork(input_size=784, hidden_size=h, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n","\n","    start = time.time()\n","    network.train(X_train, y_train_4_binary, X_test, y_test_4_binary, lr=l, epochs=e, batch_size=b, output=False)\n","    end = time.time()\n","\n","    total_time = end - start\n","    accuracy = network.evaluate(X_test, y_test_4_binary)\n","\n","    #accuracy_4 = network.evaluate(X_test_4_only, y_test_4_only_binary)\n","\n","    #network_mix = SingleLayerNetwork(input_size=784, hidden_size=h, output_size=1)\n","    #network_mix.train(X_train_mix, y_train_mix_binary, lr=l, epochs=e, batch_size=b)\n","\n","    #accuracy_mix = network_mix.evaluate(X_test, y_test_4_binary)\n","    #accuracy_mix_4 = network_mix.evaluate(X_test_4_only, y_test_4_only_binary)\n","\n","    # save the results\n","    result = (e, b, l, h, accuracy, total_time)\n","    results.append(result)\n","\n","    print(f\"combo {i+1}/{len(combinations)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {accuracy:.4f} - Time: {total_time:.4f}s\")\n","    #if (accuracy * 0.5 + accuracy_4 * 0.5) > (results[-1][5] + results[-1][4]):\n","    #    results.append((e, b, l, h, accuracy, accuracy_4, end-start))\n","    #    print(f\"Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {accuracy:.4f} - Accuracy_4: {accuracy_4:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6091be865c7240ccae3e8cd47375eb87","collapsed":false,"deepnote_cell_type":"code"},"outputs":[],"source":["results.sort(key=lambda x: x[4], reverse=True) # sort by accuracy\n","for r in results[:10]:\n","    print(f\"Epochs: {r[0]} - Batch size: {r[1]} - Learning rate: {r[2]} - Hidden size: {r[3]} - Accuracy: {r[4]:.4f} - Time: {r[5]:.4f}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"c3916096aa0a4fcc92a7148be3e4f485","deepnote_cell_type":"markdown"},"source":["### Best Result\n","\n","The best result was achieved with the following hyperparameters:"]},{"cell_type":"markdown","metadata":{"cell_id":"4b8f8e56944f434ebf472b63f22cfeab","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Aufgabe 5\n","\n","> Erweitern Sie das Netzwerk auf 3 Hidden Layer mit gleicher Größe und 10 Outputs. Das Ziel ist die korrekte Klassifizierung aller Ziffern. Verwenden Sie eine geeignete Loss-Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl und diskutieren Sie kurz weitere Möglichkeiten. Variieren Sie die Lernrate und die Größe der Hidden Layer und wählen Sie das beste Ergebnis aus."]},{"cell_type":"markdown","metadata":{"cell_id":"0b39f6d0071e40fb92d8c7a8a1e6b8e6","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Cross-Entropy"]},{"cell_type":"markdown","metadata":{"cell_id":"0027eb67c095494397e56e5f0812dff2","deepnote_cell_type":"markdown"},"source":["$$\n","J=-\\frac{1}{N}\\left(\\sum_{i=1}^N \\mathbf{y}_{\\mathbf{i}} \\cdot \\log \\left(\\hat{\\mathbf{y}}_{\\mathbf{i}}\\right)\\right)\n","$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320056Z","start_time":"2023-04-10T16:41:02.154451Z"},"cell_id":"e7474387661e4a16af3739eaae2a108d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1444067,"execution_start":1681320301350,"source_hash":"958b80da"},"outputs":[],"source":["def cross_entropy(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n","    loss = -(y_true * np.log(y_pred)).sum(axis=1).mean()\n","    return loss\n","\n","def cross_entropy_grad(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n","    grad = y_pred - y_true\n","    return grad"]},{"cell_type":"markdown","metadata":{"cell_id":"a751127e0ac847c58f9862236668b1b9","deepnote_cell_type":"markdown"},"source":["### Gradient of the Cross-Entropy loss function\n","The reason why the `cross_entropy_grad` function uses the difference between the predicted probabilities and the true labels, instead of the negative of the gradient of the cross-entropy loss with respect to the predicted probabilities, is that they are equivalent up to a constant factor.\n","\n","Let's first consider the gradient of the cross-entropy loss with respect to the predicted logits, denoted as $\\mathbf{z}$. The logits are related to the predicted probabilities via the softmax function:\n","\n","$$\n","y_{i} = \\frac{e^{z_{i}}}{\\sum_{j} e^{z_{j}}}\n","$$\n","\n","Taking the partial derivative of $\\mathcal{L}$ with respect to $z_{j}$, we have:\n","\n","$$\n","\\frac{\\partial \\mathcal{L}}{\\partial z_{j}} = \\sum_{i} \\frac{\\partial \\mathcal{L}}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial z_{j}}\n","$$\n","\n","Using the chain rule, we have:\n","\n","$$\n","\\frac{\\partial y_{i}}{\\partial z_{j}} = y_{i} (\\delta_{ij} - y_{j})\n","$$\n","\n","where $\\delta_{ij}$ is the Kronecker delta function that is equal to 1 if $i = j$ and 0 otherwise. Substituting this expression into the above equation, we obtain:\n","\n","$$\n","\\frac{\\partial \\mathcal{L}}{\\partial z_{j}} = \\sum_{i} (\\delta_{ij} - y_{j}) t_{i}\n","$$\n","\n","This expression is equivalent to the difference between the predicted probabilities and the true labels. To see this, note that the predicted probabilities can be written as:\n","\n","$$\n","y_{j} = \\frac{e^{z_{j}}}{\\sum_{k} e^{z_{k}}}\n","$$\n","\n","Taking the derivative of $y_{j}$ with respect to $z_{j}$, we have:\n","\n","$$\n","\\frac{\\partial y_{j}}{\\partial z_{j}} = y_{j} (1 - y_{j})\n","$$\n","\n","Substituting this expression into the gradient expression, we obtain:\n","\n","$$\n","\\frac{\\partial \\mathcal{L}}{\\partial z_{j}} = y_{j} - t_{j}\n","$$\n","\n","which is the difference between the predicted probabilities and the true labels. Thus, the `cross_entropy_grad` function is equivalent to the negative of the gradient of the cross-entropy loss with respect to the predicted probabilities, up to a constant factor.\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"a08ccc39db1a4f888ae68620f75f687b","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### One-Hot Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"c23f59a1a24d4d3cb11bb352824ec71f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1440984,"execution_start":1681320304433,"source_hash":"1e3600aa"},"outputs":[],"source":["def one_hot_encode(y):\n","    y_one_hot = np.zeros((len(y), 10))\n","    y_one_hot[np.arange(len(y)), y] = 1\n","    return y_one_hot\n","\n","# one hot encode the y values for the training and test set\n","y_train_one_hot = one_hot_encode(y_train)\n","y_test_one_hot = one_hot_encode(y_test)"]},{"cell_type":"markdown","metadata":{"cell_id":"75a020585d554ddf8ceaf79ba9a5e55d","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Multi-Layer Network"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320088Z","start_time":"2023-04-10T16:41:02.154747Z"},"cell_id":"93f7f79053f744ad97a13166292892e8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1438624,"execution_start":1681320306804,"source_hash":"de349e91"},"outputs":[],"source":["class MultiLayerNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, loss_function, loss_function_grad):\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.loss_function = loss_function\n","        self.loss_function_grad = loss_function_grad\n","\n","        self.input_layer = LinearLayer(input_size, hidden_size)\n","        self.hidden_layer1 = LinearLayer(hidden_size, hidden_size)\n","        self.hidden_layer2 = LinearLayer(hidden_size, hidden_size)\n","        self.output_layer = LinearLayer(hidden_size, output_size)\n","\n","        self.dev = None\n","        self.hidden_output1 = None\n","        self.hidden_output2 = None\n","        self.hidden_output3 = None\n","        self.predicted_output = None\n","\n","    def forward(self, x):\n","        self.hidden_output1 = relu(self.input_layer.forward(x))\n","        self.hidden_output2 = relu(self.hidden_layer1.forward(self.hidden_output1))\n","        self.hidden_output3 = relu(self.hidden_layer2.forward(self.hidden_output2))\n","        self.predicted_output = softmax(self.output_layer.forward(self.hidden_output3))\n","        return self.predicted_output\n","\n","    def backward(self, y_true):\n","        gradient_output = self.loss_function_grad(self.predicted_output, y_true)\n","\n","        gradient_output = softmax_grad(self.predicted_output) * gradient_output\n","        gradient_output = self.output_layer.backward(gradient_output)\n","\n","        gradient_output = relu_grad(self.hidden_output3) * gradient_output\n","        gradient_output = self.hidden_layer2.backward(gradient_output)\n","\n","        gradient_output = relu_grad(self.hidden_output2) * gradient_output\n","        gradient_output = self.hidden_layer1.backward(gradient_output)\n","\n","        gradient_output = relu_grad(self.hidden_output1) * gradient_output\n","        gradient_output = self.input_layer.backward(gradient_output)\n","\n","        return gradient_output\n","\n","    def update(self, learning_rate):\n","        self.input_layer.update(learning_rate)\n","        self.hidden_layer1.update(learning_rate)\n","        self.hidden_layer2.update(learning_rate)\n","        self.output_layer.update(learning_rate)\n","\n","    def train(self, X_train, y_train, X_test, y_test, learning_rate, epochs, batch_size, output=True):\n","        self.dev = NetworkDevelopment(total_epochs=epochs)\n","\n","        for epoch in range(epochs):\n","            loss_list = []\n","            for X_batch, y_batch in get_batches(X_train, y_train, batch_size):\n","                predicted_output = self.forward(X_batch)\n","\n","                loss = self.loss_function(predicted_output, y_batch)\n","                loss_list.append(loss)\n","\n","                self.backward(y_batch)\n","                self.update(learning_rate)\n","\n","            avg_loss = np.mean(loss_list)\n","            acc_train = self.evaluate(X_train, y_train)\n","            acc_test = self.evaluate(X_test, y_test)\n","\n","            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n","            if output:\n","                print(epoch_str)\n","\n","    def predict(self, X):\n","        predicted_output = self.forward(X)\n","        return np.argmax(predicted_output, axis=1)\n","\n","    def evaluate(self, X, y):\n","        predicted_classes = self.predict(X)\n","        true_classes = np.argmax(y, axis=1)\n","        return np.mean(predicted_classes == true_classes)\n","\n","    def summary(self):\n","        self.dev.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"b135465acc7246b49caafc5622dd7371","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:43:57.487217Z","start_time":"2023-04-10T16:41:02.154802Z"},"cell_id":"fd6bb1d5f98f4931847d9033b012b992","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1436132,"execution_start":1681320309296,"source_hash":"3e05c544"},"outputs":[],"source":["network_single_normal = MultiLayerNetwork(input_size=784, hidden_size=256, output_size=10, loss_function=cross_entropy, loss_function_grad=cross_entropy_grad)\n","network_single_normal.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=0.01, epochs=11, batch_size=46)\n","network_single_normal.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"43bf4b5ec5a24c5281b45f9fcc1fdd4a","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:43:57.716061Z","start_time":"2023-04-10T16:43:57.492708Z"},"cell_id":"da164287a917425195a47fab05b3d937","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":111608158,"execution_start":1681210137270,"source_hash":"a08821c2"},"outputs":[],"source":["# evaluate the network on the test set\n","print(f\"Accuracy: {network_single_normal.evaluate(X_test, y_test_one_hot):.4f}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"af5b252da9b54ef0adecce9b028a58da","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Finding the right hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"97ae5e0e0edb490985ac2339f2051b2a","collapsed":false,"deepnote_cell_type":"code"},"outputs":[],"source":["epochs = [10*i for i in range(1, 5)]\n","batch_sizes = [2**i for i in range(4, 11)]\n","learning_rates = [10**-i for i in range(1, 8)]\n","hidden_sizes = [32, 64, 128, 256, 512]\n","combinations = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","\n","print(f\"Testing {len(combinations)} parameter combinations\")\n","\n","results = []\n","for i in range(len(combinations)):\n","    e, b, l, h = combinations[i]\n","    net = MultiLayerNetwork(input_size=784, hidden_size=h, output_size=10, loss_function=cross_entropy, loss_function_grad=cross_entropy_grad)\n","\n","    start = time.time()\n","    net.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=l, epochs=e, batch_size=b, output=False)\n","    end = time.time()\n","\n","    total_time = end - start\n","    accuracy = net.evaluate(X_test, y_test_one_hot)\n","\n","    # save the results\n","    result = (e, b, l, h, accuracy, total_time)\n","    results.append(result)\n","\n","    print(f\"combo {i+1}/{len(combinations)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {accuracy:.4f} - Time: {total_time:.4f}s\")"]},{"cell_type":"markdown","metadata":{"cell_id":"6f4bb2f6c7d44f28b7c4cafa8edf45e2","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Lerntagebuch"]},{"cell_type":"markdown","metadata":{"cell_id":"a8873d9795e34014a8c18fcb5491bf7e","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW13"]},{"cell_type":"markdown","metadata":{"cell_id":"ab8f61905146452e8a8ccde715c1db6b","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"35529779acc54d9bbcba6142c92b93b7","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW14"]},{"cell_type":"markdown","metadata":{"cell_id":"e45befd004fb479baf6b2c6b8a482e55","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Multiplikation Linear Layer vertauscht? input x weights or weights x input"]},{"cell_type":"markdown","metadata":{"cell_id":"77dc66ff97134ec181f91241dcd9dc4b","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW15"]},{"cell_type":"markdown","metadata":{"cell_id":"85385ecd5049472a9868e53e9b9b34a7","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"4a96cca2534842eda17a169e636aa5e4","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW16"]},{"cell_type":"markdown","metadata":{"cell_id":"7955fc196b0f41bdb760aba72a862f85","deepnote_cell_type":"text-cell-p","formattedRanges":[],"number":17,"style":"decimal"},"source":["#### 17.04."]},{"cell_type":"markdown","metadata":{"cell_id":"88cd68a311b84bf6b95dae20de5a63be","deepnote_cell_type":"text-cell-p","formattedRanges":[],"number":17,"style":"decimal"},"source":["SingleLayerModel:"]},{"cell_type":"markdown","metadata":{"cell_id":"c6e068dace6a41999efa0d76d3fdb8e1","deepnote_cell_type":"text-cell-bullet","formattedRanges":[],"number":17,"style":"decimal"},"source":["- Geeignete Loss- und Accuracy-Funktionen\n","- Die Wahl wurde begründet und mit anderen mögliche Funktionen verglichen?\n","- Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angege- ben (gerendert in Latex)?\n","- Die Entwicklung der Loss- und Accuracy-Funktionen wurden auf Trainings- und Testdaten- sätzen korrekt verfolgt und leicht nachvollziehbar dargestellt?\n","- Die Wahl von Lernrate und Hidden Layer-Größe wurde nachvollziehbar entschieden und begründet."]},{"cell_type":"markdown","metadata":{"cell_id":"916bb7cdab574dd78371b42105c7b632","deepnote_cell_type":"text-cell-p","formattedRanges":[],"number":17,"style":"decimal"},"source":["MultiLayerModel:"]},{"cell_type":"markdown","metadata":{"cell_id":"ea4ff46ac649477a8eb00be5865a7090","deepnote_cell_type":"text-cell-bullet","formattedRanges":[],"number":17,"style":"decimal"},"source":["- Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n","- Die Wahl wurde begründet und mit anderen möglichen Funktionen verglichen.\n","- Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angegegeben (gerendert in Latex).\n","- Es wurden verschiedene Lernraten und Größen der Hidden Layer sinnvoll ausprobiert.\n","- Die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen wurde korrekt verfolgt und leicht nachvollziehbar dargestellt?\n","- Die Wahl der Hyperparameter wurde nachvollziehbar entschieden und begründet?"]},{"cell_type":"markdown","metadata":{"cell_id":"f94783032dc644fb8f6f1fe5948eaba7","deepnote_cell_type":"text-cell-p","formattedRanges":[],"number":17,"style":"decimal"},"source":["Form:"]},{"cell_type":"markdown","metadata":{"cell_id":"6d7d231b569c4142870117802317c842","deepnote_cell_type":"text-cell-bullet","formattedRanges":[],"number":17,"style":"decimal"},"source":["- Environment angeben (Python 3.10)\n","- Code comments\n","- Executive Summary"]},{"cell_type":"markdown","metadata":{"cell_id":"cb9d3fe8f1404a0a80860334ff17df84","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["#### 18.04."]},{"cell_type":"markdown","metadata":{"cell_id":"afcf3f0dd4eb4dec8b8763f30b7d0b52","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Fragen an Stefan:"]},{"cell_type":"markdown","metadata":{"cell_id":"7b248e8561cc48648f815ab3fbd796c5","deepnote_cell_type":"text-cell-p","formattedRanges":[],"number":17,"style":"decimal"},"source":["- Verständnis Anzahl Layers\n","- Aufgabe 3: Biased Training? Interpretation von unseren Resultaten?\n","- Keine Gradienten Berechnung im LinearLayer (Punkt 5)\n","- 7) Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n","- Lerntagebuch auch im Notebook? Summary on top?\n","- Time import bei Aufgabe 4 erlaubt? (Zeitmessung oder Epochen gemeint)"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=88bc6171-47da-4cbc-96f1-ee851c7ac9ec' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"edfe21a714fc4bb2be9a11d016a72375","kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
