{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.027242Z","start_time":"2023-04-10T16:34:59.025205Z"},"cell_id":"c5583466464448499e7371cd5c6f2a2c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":18,"execution_start":1682063454514,"source_hash":"67275d9a"},"outputs":[],"source":["import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"cell_id":"f2ee1487628d4256b6f385a7ff2051b3","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# vta: Mini-Challenge Gradient Descent"]},{"cell_type":"markdown","metadata":{"cell_id":"db919d960ab04fc29286db047ae70999","deepnote_cell_type":"markdown"},"source":["![image.png](attachment:image.png)\n","\n","Das Ziel dieser Aufgabe besteht darin, dass Sie ein grundlegendes Verständnis für numerische Näherungsverfahren in höheren Dimensionen erlangen, insbesondere für den Gradient Descent und dessen praktische Anwendung. Hierfür sollen Sie ein Jupyter Notebook erstellen und das MNIST Dataset laden und erkunden. Anschließend sollen Sie ein neuronales Netzwerk erstellen und trainieren, um die Bilder korrekt zu klassifizieren. Es dürfen nur die angegebenen Python packages verwendet werden.\n","\n","Ziel dieser Aufgabe ist nicht nur, Ihre mathematischen Kenntnisse unter Beweis zu stellen, sondern auch die entsprechende Kommunikation und Präsentation Ihrer Ergebnisse. Ihre Abgaben sollen also nicht nur mathematisch korrekt, sondern auch leicht verständlich und reproduzierbar sein. Genauere Angaben zu den Erwartungen an die Abgabe finden Sie in den Auswertungskriterien. Dokumentieren Sie ihren Arbeitsfortschritt und Erkenntnisgewinn in Form eines Lerntagebuchs, um Lernfortschritte, Schwierigkeiten und Erkenntnisse festzuhalten.\n","Die folgenden Aufgabenstellungen präzisieren die einzelnen Bearbeitungsschritte und geben die Struktur des Notebooks vor."]},{"cell_type":"markdown","metadata":{"cell_id":"6278259045a740e6b6886c823200426e","deepnote_cell_type":"markdown"},"source":["## Activation Functions\n","The activation function is a critical component of neural networks that helps introduce non-linearity to the model. It plays a crucial role in determining the output of a neural network for a given input. With the advent of deep learning, researchers and practitioners have explored a wide range of activation functions to improve the performance of neural networks.\n","\n","In this chapter, we will provide an overview of the most commonly used activation functions and compare their strengths and weaknesses. We will delve into the different use cases for each activation function and how they impact the training of neural networks.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"cee52c9c9ae344fd84eb0936ccd08b73","deepnote_cell_type":"markdown"},"source":["### Relu\n","\n","**Advantages of ReLU**:\n","1. Non-linearity: ReLU introduces non-linearity, allowing the model to learn complex patterns.\n","$$\n","f(x) = max(0, x)\n","$$\n","\n","2. Computational efficiency: ReLU is computationally efficient due to its simple form.\n","\n","3. Sparse activation: ReLU promotes sparsity, reducing model complexity.\n","Why? $f(x) = 0$ for $x < 0$, encouraging fewer active neurons.\n","\n","**Disadvantages of ReLU**:\n","1. Dying ReLU: Neurons can \"die\" if inputs are consistently negative.\n","Why? $f(x) = 0$ for $x < 0$, leading to no gradient updates.\n","\n","2. Non-differentiable at 0: ReLU is not differentiable at $x=0$.\n","Why? Discontinuity at $x=0$.\n","\n","3. Non-zero centered: ReLU outputs are not zero-centered, causing optimization issues.\n","Why? Biased gradients can hinder learning."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532332Z","start_time":"2023-04-10T16:34:59.513311Z"},"cell_id":"4ec192055655469084457df17c2874b9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":15,"execution_start":1682063454526,"source_hash":"a03db65f"},"outputs":[],"source":["def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_grad(x):\n","    return x > 0"]},{"cell_type":"markdown","metadata":{"cell_id":"e28ead44816f48f1a7def4ec0bc1033d","deepnote_cell_type":"markdown"},"source":["### Sigmoid\n","\n","**Advantages of Sigmoid**:\n","1. Smooth, differentiable: Sigmoid is a smooth, differentiable function.\n","$$\n","f(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","2. Range: Sigmoid maps input to $(0, 1)$, providing normalized outputs.\n","Why? Useful for probabilistic interpretations and binary classification.\n","\n","3. Non-linearity: Sigmoid introduces non-linearity, allowing complex pattern learning.\n","\n","**Disadvantages of Sigmoid**:\n","1. Vanishing gradient: Sigmoid suffers from vanishing gradient problem.\n","Why? Gradients can be small ($f'(x) \\approx 0$) for large $|x|$, slowing learning.\n","\n","2. Non-zero centered: Sigmoid outputs are not zero-centered, causing optimization issues.\n","Why? Biased gradients can hinder learning.\n","\n","3. Computational complexity: Sigmoid is computationally more complex than ReLU.\n","Why? Exponential calculation in the function."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"57bc4028ad564b3483f596635243b7ef","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10,"execution_start":1682063454535,"source_hash":"30011070"},"outputs":[],"source":["def sigmoid(x):\n","    x = np.clip(x, -500, 500) # clip x to prevent overflow\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_grad(x):\n","    return sigmoid(x) * (1 - sigmoid(x))"]},{"cell_type":"markdown","metadata":{"cell_id":"a3a7eb32b2c04306a5cca8cc91838fd2","deepnote_cell_type":"markdown"},"source":["### Softmax\n","\n","The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. Softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\n","\n","**Advantages of Softmax activation function**:\n","1. Probabilistic interpretation: Softmax produces a probability distribution over the output classes. The sum of the probabilities is 1.\n","\n","$$\n","\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n","$$\n","\n","Where $z_i$ is the input to the Softmax function and $K$ is the number of classes.\n","\n","2. Differentiable: Softmax is differentiable, which is essential for gradient-based optimization methods like gradient descent.\n","\n","3. Suitable for multi-class classification: Softmax is well-suited for multi-class classification problems as it produces a probability distribution over all classes.\n","\n","**Disadvantages of Softmax activation function**:\n","1. Inappropriate for binary classification: While Softmax can be used for binary classification, it is computationally inefficient compared to using a sigmoid activation function.\n","\n","2. Susceptible to vanishing gradients: Softmax may result in vanishing gradients when used with certain loss functions (e.g., mean squared error) in deep networks, slowing down learning.\n","\n","3. Numerical instability: Softmax can lead to numerical instability due to exponentiation of large values. This can be mitigated using the log-sum-exp trick:\n","$$ \n","\\sigma(z)_i = \\frac{e^{z_i - c}}{\\sum_{j=1}^K e^{z_j - c}}\n","$$\n","Where $c = \\max_i z_i$.\n","\n","Source: \n","- https://machinelearningmastery.com/softmax-activation-function-with-python/\n","- https://medium.com/intuitionmath/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6c0c58d4de4047029dc4f3c486277be8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1682063454544,"source_hash":"d7f9a7f0"},"outputs":[],"source":["def softmax(x):\n","    x = x - np.max(x, axis=1, keepdims=True) # prevent overflow\n","    x = np.exp(x)\n","    x = x / np.sum(x, axis=1, keepdims=True)\n","    return x\n","\n","def softmax_grad(x):\n","    return softmax(x) * (1 - softmax(x))"]},{"cell_type":"markdown","metadata":{"cell_id":"b3dae9dcbc9c449cbcf9ea75e9ed2541","deepnote_cell_type":"markdown"},"source":["## Loss Functions\n","\n","The loss function is a key component of neural network training that plays a crucial role in optimizing the model's performance. It is used to measure the difference between the predicted and actual values of the output, and the objective of training is to minimize this difference."]},{"cell_type":"markdown","metadata":{"cell_id":"7923e7bb75a2404a8248a8dd0ba8beb9","deepnote_cell_type":"markdown"},"source":["### Mean Squared Error\n","\n","The Mean Squared Error (MSE) loss function measures the average squared difference between the true values and the predicted values. It is commonly used in regression tasks to quantify the discrepancy between the model's predictions and the ground truth.\n","\n","$$\n","L(y, \\hat{y}) = \\frac{1}{N}(y - \\hat{y})^2 \n","$$\n","\n","\n","**Advantages of MSE loss**:\n","1. Continuity and differentiability: MSE is a continuous and differentiable function.\n","\n","2. Simple to compute: MSE is easy to compute and differentiate.\n","\n","3. Intuitive interpretation: MSE measures the average squared error between predictions and true values.\n","\n","4. Commonly used: MSE is a widely-used loss function for regression tasks.\n","\n","**Disadvantages of MSE loss**:\n","1. Suboptimal for classification: MSE is more suited for regression tasks.\n","Why? It doesn't directly optimize for class probabilities or decision boundaries.\n","\n","2. Sensitivity to outliers: MSE is sensitive to outliers, skewing predictions.\n","Why? Squaring errors exaggerates the impact of extreme values.\n","\n","3. Inconsistent gradient magnitude: MSE gradients depend on the error magnitude.\n","Why? Large errors lead to larger gradients, potentially causing overshooting.\n","\n","4. Non-probabilistic interpretation: MSE doesn't provide probability calibration.\n","Why? It doesn't focus on optimizing probabilities, which are useful for thresholding and classification tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"208b6016a8a7464e879d591ab63a5885","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1682063454546,"source_hash":"182b28c1"},"outputs":[],"source":["def mse(y_pred, y_true):\n","    batch_size = y_pred.shape[0]\n","    loss = np.sum((y_pred - y_true)**2) / batch_size\n","    return loss\n","\n","def mse_grad(y_pred, y_true):\n","    batch_size = y_pred.shape[0]\n","    grad = 2 * (y_pred - y_true) / batch_size\n","    return grad"]},{"cell_type":"markdown","metadata":{"cell_id":"75879110e6ed4aebaedc1f86150ef2aa","deepnote_cell_type":"markdown"},"source":["### Binary Cross-Entropy\n","**Advantages of BCE loss**:\n","1. Probabilistic interpretation: BCE measures the error between true binary labels and predicted probabilities, optimizing for class probabilities.\n","$$\n","L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n","$$\n","\n","2. Suitable for classification: BCE is designed for binary classification tasks, directly optimizing decision boundaries.\n","\n","3. Robust to outliers: BCE is less sensitive to outliers compared to MSE, as it doesn't square errors.\n","\n","4. Stable gradients: BCE provides stable and informative gradient signals for classification tasks, facilitating learning.\n","\n","**Disadvantages of BCE loss**:\n","1. Not suitable for multi-class problems: BCE is designed for binary classification and requires modification (e.g., categorical cross-entropy) for multi-class problems.\n","\n","2. Requires probability input: BCE loss expects predicted probabilities as input, requiring an activation function (e.g., sigmoid) to convert raw model outputs.\n","\n","3. Logarithm computation: BCE involves logarithm computations, which can be computationally more expensive than simple arithmetic operations (e.g., in MSE).\n","\n","The gradient:\n","\n","$$\n","\\begin{aligned}\n","\\frac{\\partial L(y)}{\\partial y} & =\\frac{\\partial(-t \\log (y)-(1-t) \\log (1-y))}{\\partial y}=\\frac{\\partial(-t \\log (y))}{\\partial y}+\\frac{\\partial(-(1-t) \\log (1-y))}{\\partial y} \\\\\n","& =-\\frac{t}{y}+\\frac{1-t}{1-y}=\\frac{y-t}{y(1-y)}\n","\\end{aligned}\n","$$\n","\n","Sources:\n","- https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n","- https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715\n","- https://stats.stackexchange.com/questions/219241/gradient-for-logistic-loss-function\n","- https://peterroelants.github.io/posts/cross-entropy-logistic/"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532406Z","start_time":"2023-04-10T16:34:59.517129Z"},"cell_id":"0d411d323b6c460ab440471371f91295","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1682063454556,"source_hash":"339a6c15"},"outputs":[],"source":["def binary_cross_entropy(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n","    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n","\n","def binary_cross_entropy_grad(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n","    return (y_pred - y_true) / (y_pred * (1 - y_pred)) # add eps to prevent division by 0"]},{"cell_type":"markdown","metadata":{"cell_id":"f43d6160102a4fcbb13db08b76b9bc30","deepnote_cell_type":"markdown"},"source":["### Focal Loss\n","$$\n","FL(p_t)=-\\alpha(1-p_t)^\\gamma \\cdot log(p_t)\n","$$\n","\n","Focal Loss is an extension of Cross-Entropy loss, primarily designed to address class imbalance issues in classification tasks. It was introduced by Lin et al. in the paper \"Focal Loss for Dense Object Detection.\"\n","\n","**Advantages of Focal Loss**:\n","\n","1. Handles class imbalance: Focal Loss is designed to handle class imbalance by down-weighting the contribution of easy-to-classify examples and focusing on hard-to-classify examples.\n","$$L(y, \\hat{y}) = -(1 - \\hat{y})^\\gamma y \\log(\\hat{y}) - \\hat{y}^\\gamma (1 - y) \\log(1 - \\hat{y})$$\n","Where $\\gamma$ is the focusing parameter.\n","\n","1. Adjustable focusing parameter: The focusing parameter $\\gamma$ controls the degree of down-weighting for easy examples, allowing users to fine-tune the balance between classes.\n","\n","2. Compatible with multi-class tasks: Focal Loss can be easily extended to multi-class tasks by adapting the categorical cross-entropy loss.\n","\n","3. Improved performance: Focal Loss can improve the performance of classification models, especially when class imbalance is present, by helping the model focus on difficult examples.\n","\n","**Disadvantages of Focal Loss**:\n","\n","1. Complexity: Focal Loss is more complex compared to traditional cross-entropy loss due to the additional focusing parameter and the computation of the modulating factor.\n","\n","2. Hyperparameter tuning: The focusing parameter $\\gamma$ may require hyperparameter tuning to find the best value for a specific task, adding complexity to the training process.\n","\n","3. Limited benefits in balanced datasets: The main advantage of Focal Loss is its ability to handle class imbalance. In balanced datasets, it might not provide significant benefits over traditional cross-entropy loss.\n","\n","In summary, Focal Loss is advantageous when dealing with class imbalance, as it helps the model focus on hard-to-classify examples. However, it introduces additional complexity and requires hyperparameter tuning for the focusing parameter, which might not be beneficial for balanced datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d1c5cd48f7a148c99c46631db918d5df","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1682063454566,"source_hash":"a0697eab"},"outputs":[],"source":["alpha, gamma = .9, 2\n","\n","def focal_loss(y_pred, y_true):\n","    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n","    loss = -alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred) - (1 - y_true) * alpha * y_pred ** gamma * np.log(1 - y_pred)\n","    return loss\n","\n","def focal_loss_grad(y_pred, y_true):\n","    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n","    grad = alpha * (1 - y_pred) ** (gamma - 1) * (gamma * y_true * y_pred - (gamma - 1) * y_true * (1 - y_pred) + y_pred * (1 - y_true) * gamma)\n","    return grad\n","    \n","ALPHA = 0.7\n","GAMMA = 3.0\n","\n","def binary_focal_loss(y_true, y_pred):\n","    p = np.clip(y_pred, 1e-7, 1 - 1e-7)\n","    loss_pos = -ALPHA * np.power(1 - p, GAMMA) * np.log(p) * y_true\n","    loss_neg = -(1 - ALPHA) * np.power(p, GAMMA) * np.log(1 - p) * (1 - y_true)\n","    return np.sum(loss_pos + loss_neg)\n","\n","def binary_focal_loss_gradient(y_true, y_pred):\n","    p = np.clip(y_pred, 1e-7, 1 - 1e-7)\n","    term1 = (1 - p)**GAMMA\n","    term2 = GAMMA * p * (1 - p)**(GAMMA - 1)\n","    grad_pos = -ALPHA * (term1 * (1 - y_true) - term2 * y_true * np.log(p))\n","    grad_neg = (1 - ALPHA) * (term1 * y_true - term2 * (1 - y_true) * np.log(1 - p))\n","    return grad_pos + grad_neg\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"7d7ee838de2546c69929d2ef9e48dd89","deepnote_cell_type":"markdown"},"source":["### Cross-Entropy\n","$$\n"," L =-\\frac{1}{N}\\left(\\sum_{i=1}^N \\mathbf{y}_{\\mathbf{i}} \\cdot \\log \\left(\\hat{\\mathbf{y}}_{\\mathbf{i}}\\right)\\right)\n","$$\n","\n","Cross-entropy is the default loss function to use for multi-class classification problems. \n","\n","Cross entropy computes a score that summarizes the average difference between the actual and predicted probability distributions for all classes of the problem. The score is minimized and a perfect cross entropy value is 0.\n","\n","The ground truth $y$ gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from the estimate $\\hat{\\mathbf{y}}$. For example:\n","\n","$$\n","\\begin{aligned}\n","& L =-(1 \\times \\log (0.1)+0 \\times \\log (0.5)+\\ldots) \\\\\n","& L =-\\log (0.1) \\approx 2.303\n","\\end{aligned}\n","$$\n","\n","The main feature of this loss function is that only the probabilities of the correct classes are rewarded/punished. The cross-entropy loss function is often averaged over the batch size, so that the loss function is independent of the batch size.\n","\n","**Advantages of cross-entropy loss function**:\n","1. Probabilistic interpretation: Cross-entropy loss measures the difference between two probability distributions, making it suitable for classification tasks where the output is a probability distribution.\n","\n","2. Faster convergence: Cross-entropy loss generally converges faster than other loss functions (e.g., mean squared error) when used with logarithmic activation functions like sigmoid or softmax, as it avoids the vanishing gradient problem.\n","\n","3. Differentiable: Cross-entropy loss is differentiable, which is crucial for gradient-based optimization methods like gradient descent.\n","\n","$$\n","H(y, \\hat{y}) = -\\sum_{i=1}^N y_i \\log(\\hat{y}_i)\n","$$\n","\n","**Disadvantages of cross-entropy loss function**:\n","1. Inapplicable to non-probabilistic tasks: Cross-entropy loss is not suitable for regression tasks, where the output is a continuous value rather than a probability distribution.\n","\n","2. Numerical instability: Cross-entropy loss involves logarithmic operations, which can lead to numerical instability when predicted probabilities are very close to 0 or 1. This issue can be mitigated by clipping the predicted probabilities to a small range (e.g., [1e-8, 1-1e-8]).\n","\n","3. Requires normalized outputs: Cross-entropy loss assumes that the model's output is a probability distribution. This requires the use of appropriate activation functions, such as softmax for multi-class classification or sigmoid for binary classification, in the output layer.\n","\n","When using the cross-entropy loss function, the gradient of the loss function is given by:\n","\n","$$\n","\\frac{\\partial L}{\\partial z_{j}} = \\hat{\\mathbf{y}}_i-\\mathbf{y}_i\n","$$\n","\n","This partial derivative take advantage of the fact that the Softmax activation function is a normalized exponential function. Therefore, the gradient of the cross-entropy loss function is simplified to the difference between the predicted and the actual class probabilities.\n","\n","Calculating the gradient of the cross-entropy loss function is nicely illustrated in the following image:\n","\n","![image.png](attachment:image.png)\n","\n","Sources: \n","- https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/\n","- https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n","- https://machinelearningmastery.com/cross-entropy-for-machine-learning/"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9dce370599c94c62bfa1a39d2bf82793","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":14,"execution_start":1682063454570,"source_hash":"4a6ffaf5"},"outputs":[],"source":["def cross_entropy(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n","    loss = -(y_true * np.log(y_pred)).sum(axis=1).mean()\n","    return loss\n","\n","def cross_entropy_grad(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n","    grad = y_pred - y_true\n","    return grad"]},{"cell_type":"markdown","metadata":{"cell_id":"40d1c48f431c440a91ac2fc9c5b54c0b","deepnote_cell_type":"markdown"},"source":["### Kullback–Leibler divergence\n","Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n","\n","A KL divergence loss of 0 indicates that the distributions are identical. In practice, the behavior of KL divergence is very similar to cross entropy. It calculates how much information is lost when the predicted probability distribution is used to approximate the desired target probability distribution.\n","\n","**Advantages of KL divergence loss function**:\n","1. Probabilistic interpretation: KL divergence measures the difference between two probability distributions, making it suitable for tasks where the output is a probability distribution.\n","\n","2. Asymmetric: KL divergence is asymmetric, making it sensitive to the ordering of the true and predicted distributions. This can be advantageous when the direction of the divergence matters.\n","\n","$$\n","D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n","$$\n","\n","3. Differentiable: KL divergence is differentiable, which is important for gradient-based optimization methods like gradient descent.\n","\n","**Disadvantages of KL divergence loss function**:\n","1. Non-negative: KL divergence is non-negative, which may not be ideal for certain optimization algorithms that rely on negative values to indicate convergence.\n","\n","2. Inapplicable to non-probabilistic tasks: KL divergence is not suitable for regression tasks, where the output is a continuous value rather than a probability distribution.\n","\n","3. Not symmetric: KL divergence is not symmetric, meaning that $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$. In some applications, a symmetric measure of divergence might be more appropriate.\n","\n","4. Requires normalized outputs: KL divergence assumes that the model's output is a probability distribution. This requires the use of appropriate activation functions, such as softmax for multi-class classification or sigmoid for binary classification, in the output layer.\n","\n","5. Numerical instability: KL divergence involves logarithmic operations, which can lead to numerical instability when predicted probabilities are very close to 0. This issue can be mitigated by adding a small constant to the predicted probabilities or using a modified version of the KL divergence that avoids division by zero."]},{"cell_type":"markdown","metadata":{"cell_id":"40893596d52d4f55a365faf1af8ebfd3","deepnote_cell_type":"markdown"},"source":["## Aufgabe 1\n","> Laden Sie das MNIST-Dataset mithilfe des torchvision-Pakets (Verwenden Sie das torchvision Paket für diese Aufgabe) und verwenden Sie matplotlib, um sich einen Überblick über die Daten zu verschaffen. Beschreiben Sie das grundlegenden Eigenschaften des Datensets, z.B. wie viele und welche Daten es enthält.\n","\n","This code performs several data preprocessing steps for the MNIST dataset. \n","\n","First, it defines a set of transformations to be applied to the data using the `transforms.Compose` function. The `ToTensor()` function converts the input data from a numpy array to a PyTorch tensor, and the `Normalize()` function normalizes the input tensor with a mean of 0.5 and a standard deviation of 0.5.\n","\n","Then, it loads the MNIST dataset using the `datasets.MNIST` function and applies the defined transformations to the data. It separates the training and testing data and labels into separate arrays `X_train`, `y_train`, `X_test`, and `y_test`.\n","\n","Next, it reshapes the data arrays `X_train` and `X_test` from 2D arrays of shape [N, 28, 28] to 1D arrays of shape [N, 784]. This step flattens each image into a 1D vector of 784 values.\n","\n","Finally, it normalizes the data arrays `X_train` and `X_test` to be between 0 and 1 by dividing each pixel value by 255.0, which is the maximum pixel value.\n","\n","The final line of code prints the shape of the data and label arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.168041Z","start_time":"2023-04-10T16:34:59.028883Z"},"cell_id":"209eaf05856a401cb39ba3585d6e962f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":183,"execution_start":1682063454572,"source_hash":"b7e213bc"},"outputs":[],"source":["# Define transformations to be applied to the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load the MNIST dataset\n","train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Extract the data and labels from the datasets\n","X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n","X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n","\n","# Reshape the data to be of size [N x 784]\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","# Normalize the data to be between 0 and 1\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.531688Z","start_time":"2023-04-10T16:34:59.168488Z"},"cell_id":"8203bde2e9da4982b4187ef1acba6c38","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":585,"execution_start":1682063454745,"source_hash":"9230fd10"},"outputs":[],"source":["classes = np.unique(y_train)\n","\n","# Plot the images\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(X_train[y_train == classes[i]][0].reshape(28, 28), cmap='gray')\n","    ax.set_title(f\"Class: {classes[i]}\")\n","    ax.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.531965Z","start_time":"2023-04-10T16:34:59.484990Z"},"cell_id":"e0349b7d4db04215af69d2fff54afae4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":60,"execution_start":1682063455285,"source_hash":"d82631cb"},"outputs":[],"source":["print('Number of training examples: ', X_train.shape[0])\n","print('Number of testing examples: ', X_test.shape[0])\n","print('Each image is of size: ', X_train.shape[1])\n","print('There are {} classes: {}'.format(len(classes), classes))\n","print('The data is of type: ', X_train.dtype)\n","print('The labels are of type: ', y_train.dtype)\n","print('The range of the pixel values is [{}, {}]'.format(np.min(X_train), np.max(X_train)))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9c6f85e4b6094d728e15a411e74e7812","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10,"execution_start":1682063455348,"source_hash":"73db116a"},"outputs":[],"source":["for i in range(len(classes)):\n","    print(f\"Class {classes[i]}: {np.sum(y_train == classes[i])} train examples, {np.sum(y_test == classes[i])} test examples\")"]},{"cell_type":"markdown","metadata":{"cell_id":"8ff0ef298848430c95545da177a6e994","deepnote_cell_type":"markdown"},"source":["## Aufgabe 2\n","\n","> Erstellen Sie eine Klasse für ein lineares Layer mit beliebig vielen Knoten. Implementieren Sie die Methoden forward, backward und update mithilfe von numpy. Schreiben sie geeignete Unittests, um die Funktionsweise der Funktion zu prüfen."]},{"cell_type":"markdown","metadata":{"cell_id":"1fb86fc3090541e6aa248047381a8efe","deepnote_cell_type":"markdown"},"source":["### Linear Layer\n","\n","This class represents a linear layer in a neural network. The `__init__` function initializes the layer's parameters, including the input size, output size, activation function, and its derivative. It also initializes the weights and biases randomly with small values.\n","\n","The `forward` function computes the output of the layer by taking the dot product of the input with the layer's weights and adding the bias. If the activation function is not `None`, it applies it to the linear output and returns the result.\n","\n","The `backward` function computes the gradient of the loss with respect to the layer's weights, biases, and inputs. If an activation function exists, it computes the gradient of the loss with respect to the output before applying the activation function. Then it computes the gradient of the loss with respect to the input by taking the dot product of the gradient of the loss with respect to the output and the transpose of the layer's weights.\n","\n","The `update` function updates the weights and biases using gradient descent by subtracting the product of the learning rate and the gradients from the current values.\n","\n","Overall, this class implements a fully connected layer with an optional activation function that can be used in a neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532012Z","start_time":"2023-04-10T16:34:59.502315Z"},"cell_id":"ff78f245510d42528c41d5385a701c77","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1682063455359,"source_hash":"b2289b69"},"outputs":[],"source":["class LinearLayer:\n","    def __init__(self, input_size, output_size, activation_fn, activation_fn_grad):\n","        self.x = None\n","        self.bias_grad = None\n","        self.weights_grad = None\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.activation_fn = activation_fn\n","        self.activation_fn_grad = activation_fn_grad\n","        self.weights = np.random.randn(input_size, output_size) * 0.01\n","        self.bias = np.zeros((1, output_size))\n","\n","    def forward(self, x):\n","        self.x = x\n","        linear_output = np.dot(x, self.weights) + self.bias\n","        \n","        if self.activation_fn is None:\n","            return linear_output\n","        \n","        return self.activation_fn(linear_output)\n","\n","    def backward(self, grad_output, hidden_output=None):\n","        if self.activation_fn_grad is not None:\n","            if hidden_output is not None:\n","                grad_output = self.activation_fn_grad(hidden_output) * grad_output\n","            else:\n","                grad_output = self.activation_fn_grad(grad_output) * grad_output\n","        \n","        self.weights_grad = np.dot(self.x.T, grad_output)\n","        self.bias_grad = np.sum(grad_output, axis=0, keepdims=True)\n","        return np.dot(grad_output, self.weights.T)\n","\n","    def update(self, lr):\n","        self.weights -= lr * self.weights_grad\n","        self.bias -= lr * self.bias_grad"]},{"cell_type":"markdown","metadata":{"cell_id":"a301d95430f948728a69b1f939684cbd","deepnote_cell_type":"markdown"},"source":["### Unit Testing\n","\n","This code defines a unit test for the `LinearLayer` class using the `unittest` module. The test class inherits from `unittest.TestCase`, and it contains several test functions:\n","\n","- `setUp` function initializes the parameters of the `LinearLayer` class for testing purposes.\n","- `test_forward` function tests if the `forward` function of the `LinearLayer` class returns an output with the correct shape.\n","- `test_backward` function tests if the `backward` function of the `LinearLayer` class returns an input gradient with the correct shape.\n","- `test_update` function tests if the `update` function of the `LinearLayer` class updates the weights and biases correctly.\n","- `test_shapes` function tests the shapes of the weight and bias gradients returned by the `backward` function.\n","\n","The `unittest.main` function runs the test cases and returns the results. By running these test cases, we can ensure that the `LinearLayer` class and its functions are working correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532292Z","start_time":"2023-04-10T16:34:59.506533Z"},"cell_id":"f63f452d804744fca93759dbd1f7b0ae","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1682063455368,"source_hash":"59ba2948"},"outputs":[],"source":["import unittest\n","\n","class TestLinearLayer(unittest.TestCase):\n","    def setUp(self):\n","        self.input_size = 10\n","        self.output_size = 5\n","        self.linear_layer = LinearLayer(self.input_size, self.output_size, None, None)\n","\n","    def test_forward(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        self.assertEqual(output.shape, (1, self.output_size))\n","\n","    def test_backward(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        grad_output = np.random.randn(1, self.output_size)\n","        grad_input = self.linear_layer.backward(grad_output)\n","        self.assertEqual(grad_input.shape, (1, self.input_size))\n","\n","    def test_update(self):\n","        self.linear_layer.weights = np.array([[1, 2], [3, 4]], dtype=np.float32)\n","        self.linear_layer.bias = np.array([[1, 2]], dtype=np.float32)\n","        self.linear_layer.weights_grad = np.array([[1, 2], [3, 4]], dtype=np.float32)\n","        self.linear_layer.bias_grad = np.array([[1, 2]], dtype=np.float32)\n","\n","        self.linear_layer.update(0.1)\n","        self.assertTrue(np.allclose(self.linear_layer.weights, np.array([[0.9, 1.8], [2.7, 3.6]])))\n","        self.assertTrue(np.allclose(self.linear_layer.bias, np.array([[0.9, 1.8]])))\n","\n","    def test_shapes(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        grad_output = np.random.randn(1, self.output_size)\n","        grad_input = self.linear_layer.backward(grad_output)\n","\n","        self.assertEqual(self.linear_layer.weights_grad.shape, (self.input_size, self.output_size))\n","        self.assertEqual(self.linear_layer.bias_grad.shape, (1, self.output_size))\n","\n","unittest.main(argv=[''], verbosity=2, exit=False)"]},{"cell_type":"markdown","metadata":{"cell_id":"6654738999234a93a6a0b07b8693ef1f","deepnote_cell_type":"markdown"},"source":["## Aufgabe 3\n","\n","> Erstellen Sie ein neuronales Netzwerk in numpy mit einem Hidden Linear Layer und einem Output Knoten. Trainieren Sie das Netzwerk darauf, die Ziffer 4 korrekt zu identifizieren (d.h. der Output soll 1 für diese Ziffer und 0 für alle anderen Ziffern sein). Trainieren Sie das Netzwerk auf den Trainingsdaten und evaluieren Sie es anhand von Testdaten. Verwenden Sie eine geeignete Loss-Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl mit einer Abwägung der Vor- und Nachteile. Diskutieren Sie kurz weitere Optionen für Loss und Accuracy."]},{"cell_type":"markdown","metadata":{"cell_id":"b8a9e7c4641e4501880a1e96b5d540da","deepnote_cell_type":"markdown"},"source":["### Loss function\n","**Binary Cross Entropy (BCE)** is a suitable loss function for the MNIST binary classification task of classifying if a digit is a 4 or not, mainly because it is specifically designed for binary classification problems. Mathematically, BCE calculates the negative log likelihood of the correct class, penalizing the model heavily when its prediction is far from the ground truth. This characteristic makes BCE sensitive to the difference between predicted probabilities and actual labels, thereby promoting accurate classification.\n","\n","In comparison, Focal loss is designed to address class imbalance issues by down-weighting the loss contribution from easily classified examples. While this property makes Focal loss an optimal choice for the given task with a 9:91 class distribution, the inability to implement it necessitated the use of BCE. Nonetheless, BCE still provides a reasonable performance in such scenarios. To address the imbalance issue, remixing the training and test set can be considered. \n","\n","Mean Squared Error (MSE) loss, on the other hand, is suboptimal for classification tasks, as it focuses on the squared difference between predictions and ground truth. This leads to less effective gradient updates, particularly in the presence of saturating activation functions like sigmoid. Moreover, MSE is more suitable for regression tasks rather than binary classification, and its use may result in subpar performance for the given task.\n","\n","In summary, Binary Cross Entropy is a suitable choice for the MNIST binary classification task, especially when Focal loss is not implementable. While BCE may not directly address class imbalance, its mathematical properties make surely a solid choice.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"af745b5401fc43eabb7aeae24ee219cd","deepnote_cell_type":"markdown"},"source":["### Accuracy function\n","\n","The F1 score is an appropriate accuracy metric for the MNIST binary classification task of identifying 4s due to the imbalanced nature of the dataset, where 4s constitute approximately 9% and non-4s make up 91%. The F1 score combines precision (the proportion of true positive 4s among all predicted 4s) and recall (the proportion of true positive 4s among all actual 4s) through their harmonic mean. This balances false positives and false negatives, ensuring that both are adequately addressed.\n","\n","In contrast, metrics like naive-accuracy can be misleading, as a classifier that predicts all non-4s will achieve high accuracy (91%) despite failing to identify any 4s. While other metrics like precision, recall, or the area under the ROC curve (AUC-ROC) can provide useful insights, the F1 score remains a more comprehensive and balanced choice for this specific classification task. The F1 score is calculated like this:\n","\n","$$\n","\\begin{aligned}\n","& \\text { Precision }=\\frac{T P}{T P+F P} \\\\\n","& \\text { Recall }=\\frac{T P}{T P+F N} \\\\\n","& F 1 \\text { score }=2 * \\frac{\\text { Precision } * \\text { Recall }}{\\text { Precision }+ \\text { Recall }}\n","\\end{aligned}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ecc04e426ba64b438da2e296cf5a6f32","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1682063455388,"source_hash":"ce1396e4"},"outputs":[],"source":["def f1_score(y_pred, y_true):\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n","\n","    y_true = y_true.flatten()\n","    y_pred = y_pred.flatten()\n","\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","    fn = np.sum((y_true == 1) & (y_pred == 0))\n","\n","    if tp + fp == 0 or tp + fn == 0:\n","        return 0\n","\n","    precision = tp / (tp + fp)\n","    recall = tp / (tp + fn)\n","\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","    return f1\n","\n","def plot_confusion_matrix(cm, class_labels=None, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        fmt = '.2f'\n","    else:\n","        fmt = 'd'\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","\n","    if class_labels is None:\n","        class_labels = ['0', '1']\n","\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           xticklabels=class_labels, yticklabels=class_labels,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    fig.tight_layout()\n","    plt.show()\n","\n","def confusion_matrix(y_true, y_pred, plot=True, plot_title='Confusion Matrix'):\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n","\n","    y_true = y_true.flatten()\n","    y_pred = y_pred.flatten()\n","\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","    tn = np.sum((y_true == 0) & (y_pred == 0))\n","    fn = np.sum((y_true == 1) & (y_pred == 0))\n","\n","    cm = np.array([[tn, fp], [fn, tp]])\n","\n","    if plot:\n","        plot_confusion_matrix(cm, title=plot_title)\n","\n","    return cm"]},{"cell_type":"markdown","metadata":{"cell_id":"394f85cd5bc041cea1e14830031786b3","collapsed":false,"deepnote_cell_type":"markdown"},"source":["### Tracking network development\n","In order to track the development of the training process, we define a class that stores the training and validation loss and accuracy for each epoch. We also define a function that plots the training and validation loss and accuracy for each epoch. This allows us to see how the training process develops over time.\n","\n","The `NetworkDevelopment` class is responsible for storing the training and validation loss and accuracy for each epoch during the training process of a neural network. It has the following methods:\n","\n","- `__init__(self, total_epochs)`: Initializes the class with the total number of epochs the network will be trained for. It initializes empty lists to store the training and validation losses and accuracies for each epoch.\n","- `add_epoch(self, epoch_number, loss, acc_train, acc_test)`: Adds a new development step to the network development by appending the loss, accuracy for training data, and accuracy for validation data of the current epoch to their respective lists. This method returns a string containing the epoch number, loss, and accuracies for training and validation data.\n","- `plot(self)`: Plots the training and validation loss and accuracy for each epoch using matplotlib. It creates two subplots, one for loss over epochs and another for accuracy over epochs. The method returns the plot.\n","- `summary(self)`: Prints the final loss and accuracy of the training and validation data, and the average improvements per epoch. It calls the `plot()` method to plot the loss and accuracy data, and then prints the average accuracy change per epoch for both the training and validation data, and the average loss change per epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"c788cc468dc74a6aabd12e877f6ecdd6","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1682063455395,"source_hash":"b4e13549"},"outputs":[],"source":["class NetworkDevelopment:\n","    def __init__(self, total_epochs):\n","        self.total_epochs = total_epochs\n","        self.losses = []\n","        self.accuracies_train = []\n","        self.accuracies_test = []\n","\n","    def add_epoch(self, epoch_number, loss, acc_train, acc_test):\n","        self.losses.append(loss)\n","        self.accuracies_train.append(acc_train)\n","        self.accuracies_test.append(acc_test)\n","\n","        return f'Epoch {epoch_number}/{self.total_epochs} - loss: {loss:.4f} - acc_train: {acc_train:.4f} - acc_test: {acc_test:.4f}'\n","\n","    def plot(self):\n","        epochs = np.arange(1, self.total_epochs + 1)\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","        ax1.plot(epochs, self.losses)\n","        ax1.set_title('Loss over Epochs')\n","        ax1.set_xlabel('Epoch')\n","        ax1.set_ylabel('Loss')\n","        ax2.plot(epochs, self.accuracies_train, label='Training')\n","        ax2.plot(epochs, self.accuracies_test, label='Test')\n","        ax2.set_title('Accuracy over Epochs')\n","        ax2.set_xlabel('Epoch')\n","        ax2.set_ylabel('Accuracy')\n","        ax2.legend()\n","        plt.show()\n","\n","    def summary(self):\n","        self.plot()\n","\n","        print(f'avg acc change / epoch (Training set): {np.mean(np.diff(self.accuracies_train)):.4f}')\n","        print(f'avg acc change / epoch (Test set): {np.mean(np.diff(self.accuracies_test)):.4f}')\n","        print(f'avg loss change / epoch: {np.mean(np.diff(self.losses)):.4f}')"]},{"cell_type":"markdown","metadata":{"cell_id":"1dd560de11eb4ddaba47ffb63f4bbf99","deepnote_cell_type":"markdown"},"source":["### Batch Training\n","\n","Training a neural network on large datasets can be computationally expensive and time-consuming. One way to mitigate this challenge is to use batch training, where instead of updating the model's weights after every single data point, the model's parameters are updated after processing a fixed number of data points, known as a batch.\n","\n","The function `get_batches(x, y, batch_size)` creates batches of images and labels from the training set. The number of images in a batch is defined by the batch size. The batch size is a hyperparameter that can be tuned to improve the training process. The batch size is a trade-off between the number of images used for training and the number of training steps per epoch. A larger batch size results in fewer training steps per epoch, but the training process is less accurate. A smaller batch size results in more training steps per epoch, but the training process is more accurate.\n","\n","- `x`: A numpy array containing the input data.\n","- `y`: A numpy array containing the target data.\n","- `batch_size`: An integer representing the size of each batch.\n","\n","The function first calculates the number of batches needed to cover the entire dataset by dividing the length of the input data by the batch size. The function then shuffles the input and target data using a random permutation of indices to ensure that the data is not ordered in any particular way. \n","\n","The function then uses a generator to yield batches of input and target data. It does this by iterating over the range of indices from 0 to the number of batches times the batch size, with a step size of batch size. For each iteration, it slices the input and target data arrays to extract a batch of size batch_size, and then yields this batch as a tuple of (x_batch, y_batch)."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"65cc35292c084294b90ce4d9bbd34210","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1682063455395,"source_hash":"5365661a"},"outputs":[],"source":["def get_batches(x, y, batch_size):\n","    n_batches = len(x) // batch_size\n","    \n","    # shuffle data before creating batches\n","    idx = np.random.permutation(len(x))\n","    x = x[idx]\n","    y = y[idx]\n","\n","    for i in range(0, n_batches * batch_size, batch_size):\n","        x_batch = x[i:i+batch_size]\n","        y_batch = y[i:i+batch_size]\n","        yield x_batch, y_batch"]},{"cell_type":"markdown","metadata":{"cell_id":"77137bfe3dea48a6b8ff9f3d519569a9","deepnote_cell_type":"markdown"},"source":["### Single-Layer Network\n","\n","This code defines a single-layer neural network class `SingleLayerNetwork` that inherits from the `LinearLayer` class. The `SingleLayerNetwork` class has several methods:\n","\n","- `__init__` initializes the network's parameters, including the input size, hidden layer size, output size, loss function, and its derivative. It also initializes the hidden and output layers using `LinearLayer` with the appropriate activation functions.\n","- `forward` computes the forward pass of the network by propagating the input through the hidden and output layers using their `forward` functions.\n","- `backward` computes the backward pass of the network by propagating the gradient of the loss with respect to the output through the output and hidden layers using their `backward` functions.\n","- `update` updates the weights and biases of the hidden and output layers using their `update` functions and the learning rate.\n","- `train` trains the network on the given training data `X_train` and labels `y_train` for a specified number of epochs using mini-batch gradient descent. It computes the accuracy and loss on the training and test data at each epoch and outputs the results if `output` is `True`.\n","- `predict` makes predictions on the given input data `X` by calling the `forward` function and returning the predictions.\n","- `evaluate` evaluates the accuracy of the network's predictions on the given input data `X` and labels `y`.\n","- `summary` prints a summary of the network's development using the `NetworkDevelopment` class.\n","\n","Overall, this class defines a simple single-layer neural network and its training and evaluation methods."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532515Z","start_time":"2023-04-10T16:34:59.522530Z"},"cell_id":"d4a53e40c37d474985d03fb839f6cf7f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1682063455442,"source_hash":"bd597d4b"},"outputs":[],"source":["class SingleLayerNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, loss_fn, loss_fn_grad, acc_fn):\n","        self.loss_fn = loss_fn\n","        self.loss_fn_grad = loss_fn_grad\n","        self.acc_fn = acc_fn\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.hidden_layer = LinearLayer(input_size, hidden_size, relu, relu_grad)\n","        self.output_layer = LinearLayer(hidden_size, output_size, sigmoid, sigmoid_grad)\n","\n","    def forward(self, x):\n","        self.x = x\n","        self.h = self.hidden_layer.forward(x)\n","        self.y_pred = self.output_layer.forward(self.h)\n","        return self.y_pred\n","\n","    def backward(self, y_true):\n","        y_true = y_true.reshape(-1, 1)\n","        self.y_pred = self.y_pred.reshape(-1, 1)\n","\n","        grad_output = self.loss_fn_grad(self.y_pred, y_true)\n","\n","        grad_output = self.output_layer.backward(grad_output)\n","        grad_output = self.hidden_layer.backward(grad_output)\n","\n","        return grad_output\n","\n","    def update(self, lr):\n","        self.hidden_layer.update(lr)\n","        self.output_layer.update(lr)\n","\n","    def train(self, X_train, y_train, X_test, y_test, lr, epochs, batch_size, output=True):\n","        self.dev = NetworkDevelopment(total_epochs=epochs)\n","        \n","        for epoch in range(epochs):\n","            loss_list = []\n","            for x_batch, y_batch in get_batches(X_train, y_train, batch_size):\n","                y_pred = self.forward(x_batch)\n","\n","                loss = self.loss_fn(y_pred, y_batch)\n","                loss_list.append(loss)\n","\n","                self.backward(y_batch)\n","                self.update(lr)\n","\n","            acc_train = self.evaluate(X_train, y_train)\n","            acc_test = self.evaluate(X_test, y_test)\n","            avg_loss = np.mean(loss_list)\n","\n","            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n","            if output:\n","                print(epoch_str)\n","\n","    def predict(self, X):\n","        return (self.forward(X) > .5).astype(int).reshape(-1, 1)\n","\n","    def evaluate(self, X, y):\n","        y_pred = self.predict(X).flatten()\n","        # return np.mean(y_pred == y)\n","        return self.acc_fn(y_pred, y)\n","\n","    def summary(self):\n","        self.dev.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"b8955f0e3f284d81b03a6b78bd58d611","deepnote_cell_type":"markdown"},"source":["### Training\n","\n","In order to train the model for the task at hand, we need to convert the $y$ data into a binary format. Each label representing a $4$ is converted into an $1$. Every non-$4$ label is converted to a $0$."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4a3a373c100148058c81c8180d0109e9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1682063455441,"source_hash":"2f634c8"},"outputs":[],"source":["y_train_4_binary = (y_train == 4).astype(int)\n","y_test_4_binary = (y_test == 4).astype(int)"]},{"cell_type":"markdown","metadata":{"cell_id":"0a7205f081974d539a6dafd8d8016745","deepnote_cell_type":"markdown"},"source":["We now train the Single-Layer Network on the train and test data with binary encoded labels."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7098f61168294f9c907badc6e204f4b4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":172582,"execution_start":1682063458123,"source_hash":"4df15bb3"},"outputs":[],"source":["slp = SingleLayerNetwork(input_size=784, hidden_size=512, output_size=1, loss_fn=binary_cross_entropy, loss_fn_grad=binary_cross_entropy_grad, acc_fn=f1_score)\n","slp.train(X_train, y_train_4_binary, X_test , y_test_4_binary, lr=0.00001, epochs=15, batch_size=256)\n","slp.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"b8f631242a8c4385852b8b7721554dc4","deepnote_cell_type":"markdown"},"source":["### Modifying datasets to counter bias\n","\n","Given the nature of the default test set, there is a natural bias against 4s, since they are only making up around 9% of train and test set. If the model would be trained on this default set, it would develop a strong bias against 4s and reach a 90%> accuracy even though it is not capable of actually identifying a 4.\n","\n","To make sure that our model can identify 4s and non-4s equally well we created some subsets which will allow to train and test the model in a less biased way. To accomplish this, a mixed train and test set with equal distribution of 4s and non-4s is created (mix). All non-4 digits have equal representation in the set. \n","\n","The model will be trained on this mixed set and then evaluated against two other sets: 4s only and non-4s. The goal is to reach a high accuracy in all 3 test sets, since this means that the model can correctly identify 4s and non-4s and doesn't have strong bias towards either one. Afterwards the model will evaluate the default test set.\n","\n","Only after having a high accuracy in all 4 test sets(mix, 4s, non-4s and default), can we conclude the models success.\n","\n","\n","The code is preparing three datasets, one containing only images of the number 4 (y=1), one with only images of other digits (y=0), and one with an equal representation of both. \n","\n","\n","It first converts the original labels to binary values where 1 represents the number 4, and 0 represents any other digit. \n","\n","Then, it separates the images of each digit in the training and testing sets. It calculates the number of samples to be selected from each class, which will be the same as the number of images of 4 in each subset, and selects those from all digits except for 4. This results in two subsets containing an equal number of 4 and non-4 images. \n","\n","The code shuffles the two subsets separately and converts the labels to binary. \n","\n","Afterward, the code selects only the 4 images from the original training and testing sets, shuffles them, and also converts their labels to binary. \n","\n","Finally, the code creates a new mixed dataset by taking an equal number of samples from both the 4s-only and non-4s-only subsets, shuffles it, and converts its labels to binary. The output of the total number of 4s and non-4s in the training set is printed."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.802387Z","start_time":"2023-04-10T16:34:59.023452Z"},"cell_id":"bc5685db370e44bbbe199afa9419cd1a","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":197,"execution_start":1682063634424,"source_hash":"4fb260e1"},"outputs":[],"source":["# Separate the digits\n","digits = [X_train[y_train == i] for i in range(10)]\n","digits_test = [X_test[y_test == i] for i in range(10)]\n","\n","# Calculate the number of samples for each not 4 class\n","samples_per_class = len(digits[4]) // 9\n","samples_per_class_test = len(digits_test[4]) // 9\n","\n","# Prepare the not 4s subset with equal representation of every other digit\n","X_train_not_4 = np.concatenate([digits[i][:samples_per_class] for i in range(10) if i != 4])\n","y_train_not_4 = np.concatenate([np.full(samples_per_class, i) for i in range(10) if i != 4])\n","\n","X_test_not_4 = np.concatenate([digits_test[i][:samples_per_class_test] for i in range(10) if i != 4])\n","y_test_not_4 = np.concatenate([np.full(samples_per_class_test, i) for i in range(10) if i != 4])\n","\n","# Shuffle the not 4s subset\n","indices_not_4 = np.random.permutation(len(X_train_not_4))\n","X_train_not_4 = X_train_not_4[indices_not_4]\n","y_train_not_4 = y_train_not_4[indices_not_4]\n","\n","indices_not_4_test = np.random.permutation(len(X_test_not_4))\n","X_test_not_4 = X_test_not_4[indices_not_4_test]\n","y_test_not_4 = y_test_not_4[indices_not_4_test]\n","\n","y_train_not_4_binary = (y_train_not_4 == 4).astype(int)\n","y_test_not_4_binary = (y_test_not_4 == 4).astype(int)\n","\n","\n","# Shuffle the 4s subset\n","indices_4 = np.random.permutation(len(digits[4]))\n","X_train_4_only = digits[4][indices_4]\n","y_train_4_only = np.full(len(digits[4]), 4)\n","\n","\n","indices_4_test = np.random.permutation(len(digits_test[4]))\n","X_test_4_only = digits_test[4][indices_4_test]\n","y_test_4_only = np.full(len(digits_test[4]), 4)\n","\n","y_train_4_only_binary = (y_train_4_only == 4).astype(int)\n","y_test_4_only_binary = (y_test_4_only == 4).astype(int)\n","\n","\n","# Create the mixed dataset with a 50:50 split\n","X_train_mix = np.concatenate((X_train_4_only[:len(X_train_not_4)], X_train_not_4))\n","y_train_mix = np.concatenate((y_train_4_only[:len(y_train_not_4)], y_train_not_4))\n","\n","X_test_mix = np.concatenate((X_test_4_only[:len(X_test_not_4)], X_test_not_4))\n","y_test_mix = np.concatenate((y_test_4_only[:len(y_test_not_4)], y_test_not_4))\n","\n","\n","# Shuffle the mixed dataset\n","indices_mix = np.random.permutation(len(X_train_mix))\n","X_train_mix = X_train_mix[indices_mix]\n","y_train_mix = y_train_mix[indices_mix]\n","\n","indices_mix_test = np.random.permutation(len(X_test_mix))\n","X_test_mix = X_test_mix[indices_mix_test]\n","y_test_mix = y_test_mix[indices_mix_test]\n","\n","# Convert the labels to binary\n","y_train_mix_binary = (y_train_mix == 4).astype(int)\n","y_test_mix_binary = (y_test_mix == 4).astype(int)\n","\n","# Print the count of 4s and not 4s in the train set\n","print(f\"4s: {np.sum(y_train_mix_binary == 1)} - Not 4s: {np.sum(y_train_mix_binary == 0)}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"0176af549a324b66a7a632c57be63d59","deepnote_cell_type":"markdown"},"source":["### Training with modified datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:35:41.520284Z","start_time":"2023-04-10T16:34:59.802503Z"},"cell_id":"3f466c7f2b0d4bc6ace40ff71cc7b9a2","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":422895,"execution_start":1682063639467,"source_hash":"17f866f0"},"outputs":[],"source":["slpm = SingleLayerNetwork(input_size=784, hidden_size=512, output_size=1, loss_fn=binary_cross_entropy, loss_fn_grad=binary_cross_entropy_grad, acc_fn=f1_score)\n","slpm.train(X_train_mix, y_train_mix_binary, X_test_mix, y_test_mix_binary, lr=0.00001, epochs=30, batch_size=128)\n","slpm.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"7c1cf4566d4d48edb718186eb66f5913","deepnote_cell_type":"markdown"},"source":["### Testing\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f01934a2834243619f9e3d1654095362","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":768,"execution_start":1682015102368,"source_hash":"dab5f7ab"},"outputs":[],"source":["confusion_matrix(y_test_4_binary, slp.predict(X_test), plot_title=\"SLP\")\n","confusion_matrix(y_test_4_binary, slpm.predict(X_test), plot_title=\"SLPM\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"222858f6169742c590f609e3d1d51507","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":406,"execution_start":1682015103149,"source_hash":"d081887"},"outputs":[],"source":["f1_score(y_test_4_binary, slp.predict(X_test)), f1_score(y_test_4_binary, slpm.predict(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5d5f794210b14783a0894a83467c65d2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":296,"execution_start":1682015104468,"source_hash":"51859bf3"},"outputs":[],"source":["acc_4_only = slp.evaluate(X_test_4_only, y_test_4_only_binary)\n","acc_not_4 = slp.evaluate(X_test_not_4, y_test_not_4_binary)\n","print(f\"Accuracy on 4s: {acc_4_only} - Accuracy on not 4s: {acc_not_4}\")\n","\n","normal_acc = slp.evaluate(X_test, y_test_4_binary)\n","print(f\"Accuracy on normal test set: {normal_acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"cfb351a16d0f4b4f88bfaf7b1d7cbfd9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":683,"execution_start":1682015104766,"source_hash":"256caf4b"},"outputs":[],"source":["acc_4_only = slpm.evaluate(X_test_4_only, y_test_4_only_binary)\n","acc_not_4 = slpm.evaluate(X_test_not_4, y_test_not_4_binary)\n","print(f\"Accuracy on 4s: {acc_4_only} - Accuracy on not 4s: {acc_not_4}\")\n","\n","normal_acc = slpm.evaluate(X_test, y_test_4_binary)\n","print(f\"Accuracy on normal test set: {normal_acc}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"5c501aa2c6eb44ad9c7da9bd08a70572","deepnote_cell_type":"markdown"},"source":["## Aufgabe 4\n","\n","> Trainieren Sie das Netzwerk mit verschiedenen Lernraten und Größen des Hidden Layers. Verfolgen Sie während des Trainings die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen und entscheiden Sie, welche Wahl von Lernrate und Hidden Layer-Größe die besten Ergebnisse in geringster Zeit liefert."]},{"cell_type":"markdown","metadata":{"cell_id":"6b64ede0fcd7432fa2e9139efd645c7a","deepnote_cell_type":"markdown"},"source":["In order to find the right hyperparameters, we define a function that trains the network with different learning rates, epochs, hidden layer sizes and batch sizes. The result for each combination is then stored and analyzed.\n","\n","This brute force approach is not very efficient, but it allows us to find the best hyperparameters for the network.\n","\n","> **Note:** The brute force process takes a long time to complete. Therefore, we have commented out the code that runs the brute force process. The best result is described further below."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320020Z","start_time":"2023-04-10T16:41:02.154130Z"},"cell_id":"e510d22b32464fcb89ff035c0a2b224f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":82,"execution_start":1682090808750,"source_hash":"77ecda2"},"outputs":[],"source":["def get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes):\n","    return [(e, b, l, h) for e in epochs for b in batch_sizes for l in learning_rates for h in hidden_sizes]\n","\n","epochs = [10, 20, 30]\n","batch_sizes = [2**i for i in range(5, 11)]\n","learning_rates = [10**-i for i in range(1, 8)]\n","hidden_sizes = [64, 128, 256, 512]\n","\n","def test_brute_force_single_layer(param_combos, X_train, y_train, X_test, y_test):\n","    results = []\n","    best_acc = 0\n","    best_param_combo = None\n","\n","    for i in range(len(param_combos)):\n","        try:\n","            e, b, l, h = param_combos[i]\n","            network = SingleLayerNetwork(input_size=784, hidden_size=h, output_size=1, \n","                                        loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad,\n","                                        acc_fn=f1_score)\n","\n","            network.train(X_train, y_train, X_test, y_test, lr=l, epochs=e, batch_size=b, output=False)\n","            acc = network.evaluate(X_test, y_test)\n","\n","            results.append((e, b, l, h, accuracy))\n","            if acc > best_acc:\n","                best_acc = acc\n","                best_param_combo = param_combos[i]\n","\n","            print(f\"combo {i+1}/{len(param_combos)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {acc:.4f}\")\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            continue\n","\n","    return best_acc, best_param_combo\n","\n","\n","sl_combos = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","print(f\"Testing {len(sl_combos)} parameter combinations\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"10444483c1bb4e37b88134e35ceaa6f4","deepnote_cell_type":"code"},"outputs":[],"source":["test_brute_force_single_layer(sl_combos, X_train, y_test_4_binary, X_test, y_test_4_binary)"]},{"cell_type":"markdown","metadata":{"cell_id":"7ee4ce1696c1483192a104b82247b7f7","deepnote_cell_type":"markdown"},"source":["The best result was achieved with the following hyperparameters:"]},{"cell_type":"markdown","metadata":{"cell_id":"86da99f6f334417d8b7ff16b4f084d3c","deepnote_cell_type":"markdown"},"source":["## Aufgabe 5\n","\n","> Erweitern Sie das Netzwerk auf 3 Hidden Layer mit gleicher Größe und 10 Outputs. Das Ziel ist die korrekte Klassifizierung aller Ziffern. Verwenden Sie eine geeignete Loss-Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl und diskutieren Sie kurz weitere Möglichkeiten. Variieren Sie die Lernrate und die Größe der Hidden Layer und wählen Sie das beste Ergebnis aus."]},{"cell_type":"markdown","metadata":{"cell_id":"41a3d77563ca47cd893c6e44cad3d6f9","deepnote_cell_type":"markdown"},"source":["### Loss function\n","\n","**Cross Entropy loss** is a preferred choice for the MNIST classification task due to its ability to handle probabilistic outputs effectively. This is especially useful when dealing with multi-class classification problems like MNIST, which involves 10 different digit classes. Mathematically, Cross Entropy loss quantifies the difference between the predicted probability distribution and the true distribution, incentivizing the model to increase the probability of correct predictions.\n","\n","Comparatively, Mean Squared Error (MSE) is more suited for regression problems and lacks the same discriminative power as Cross Entropy when handling classification tasks. MSE doesn't exploit the probabilistic nature of class predictions, which can lead to slower convergence and less accurate results.\n","\n","While Kullback-Leibler (KL) Divergence also measures the dissimilarity between two probability distributions, it is asymmetric and can be computationally expensive. Cross Entropy, on the other hand, is a symmetric and more computationally efficient alternative for classification tasks. \n","\n","In conclusion, Cross Entropy loss is an optimal choice for the MNIST classification problem due to its capability to handle probabilistic outputs, faster convergence, and computational efficiency compared to alternatives like MSE and KL Divergence."]},{"cell_type":"markdown","metadata":{"cell_id":"d13fba949ac64b8abe4d2a78886efdc4","deepnote_cell_type":"markdown"},"source":["### Accurcay function\n","The standard accuracy metric is the most suitable choice for the MNIST classification task, as it effectively captures the proportion of correctly classified examples. Mathematically, accuracy is calculated by dividing the number of correct predictions by the total number of predictions, yielding a value between 0 and 1. This simple metric is easy to interpret and understand, making it ideal for evaluating the performance of a classifier on a balanced dataset like MNIST, where each digit class is equally represented.\n","\n","$$\n","\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{\\sum_{i=1}^{n} I(y_i = \\hat{y}_i)}{n}\n","$$\n","\n","Alternatives like precision, recall, and F1-score are generally more informative for imbalanced datasets, where certain classes are underrepresented. However, since the MNIST dataset for all classes (see exercise 1) is balanced, these metrics provide little additional value compared to standard accuracy. Moreover, the Area Under the Receiver Operating Characteristic curve (AUROC) and Area Under the Precision-Recall curve (AUPRC) are more relevant for binary classification problems or when a probabilistic output is required, which is not the case for MNIST.\n","\n","In conclusion, the standard accuracy metric is the most appropriate choice for the MNIST classification task due to its simplicity, interpretability, and effectiveness in capturing the classifier's performance on balanced datasets. Alternative metrics like precision, recall, F1-score, AUROC, and AUPRC provide limited additional insight for this specific problem."]},{"cell_type":"markdown","metadata":{"cell_id":"4305bf4b69cb421d8a58e6d04eb496df","deepnote_cell_type":"markdown"},"source":["### One-Hot Encoding\n","\n","One-hot encoding is a way to represent categorical data in a numerical format. In the case of MNIST, the target variable is the digit that each image represents, which can take on values from 0 to 9. One-hot encoding converts each digit into a binary vector of length 10, where each element of the vector represents a possible digit value. For example, the digit 3 would be represented as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], where the fourth element is a 1 and the other elements are 0."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"76fd3967a0ba4ab5ba26b8336e9723df","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":44,"execution_start":1682015105855,"source_hash":"52b66065"},"outputs":[],"source":["def one_hot_encode(y):\n","    y_one_hot = np.zeros((len(y), 10))\n","    y_one_hot[np.arange(len(y)), y] = 1\n","    return y_one_hot\n","\n","y_train_one_hot = one_hot_encode(y_train)\n","y_test_one_hot = one_hot_encode(y_test)"]},{"cell_type":"markdown","metadata":{"cell_id":"be1a27ed1f4d49edac376a910c1cc917","deepnote_cell_type":"markdown"},"source":["### Multi-Layer Network\n","\n","The `MultiLayerNetwork` class represents a neural network with multiple layers, and is used for training and evaluating the model on a given dataset. The class implements the functions necessary for forward and backward propagation in the network. \n","\n","The `__init__` function initializes the network with the given input size, hidden size, output size, loss function, and loss function gradient. It also initializes the various layers of the network, including three hidden layers and an output layer.\n","\n","The `forward` function performs forward propagation on a given input `x`, computing the predicted output of the network. The `backward` function performs backward propagation to compute the gradients of the network's weights and biases with respect to the loss, given the true output `y_true`. The `update` function updates the weights and biases of the network based on the gradients computed in the backward pass.\n","\n","The `train` function trains the network on the given training data `X_train` and `y_train`, using the given learning rate, number of epochs, and batch size. It also computes the training and validation accuracy and loss for each epoch using the `evaluate` function, and stores these values in a `NetworkDevelopment` object. \n","\n","The `predict` function takes a set of input data `X` and returns the predicted classes of each input based on the current weights and biases of the network. The `evaluate` function takes a set of input data `X` and corresponding true output `y`, and computes the accuracy of the predicted classes relative to the true classes.\n","\n","Finally, the `summary` function prints a summary of the network's training and validation accuracy and loss for each epoch, as well as the average improvements per epoch, using the `NetworkDevelopment` object."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320088Z","start_time":"2023-04-10T16:41:02.154747Z"},"cell_id":"c5d885e98da7469ca0eed1b95c47c241","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1682015105898,"source_hash":"4a620a2e"},"outputs":[],"source":["class MultiLayerNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, loss_function, loss_function_grad):\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.loss_function = loss_function\n","        self.loss_function_grad = loss_function_grad\n","\n","        self.hidden_layer1 = LinearLayer(input_size, hidden_size, relu, relu_grad)\n","        self.hidden_layer2 = LinearLayer(hidden_size, hidden_size, relu, relu_grad)\n","        self.hidden_layer3 = LinearLayer(hidden_size, hidden_size, relu, relu_grad)\n","        self.output_layer = LinearLayer(hidden_size, output_size, softmax, softmax_grad)\n","\n","        self.dev = None\n","        self.hidden_output1 = None\n","        self.hidden_output2 = None\n","        self.hidden_output3 = None\n","        self.predicted_output = None\n","\n","    def forward(self, x):\n","        self.hidden_output1 = self.hidden_layer1.forward(x)\n","        self.hidden_output2 = self.hidden_layer2.forward(self.hidden_output1)\n","        self.hidden_output3 = self.hidden_layer3.forward(self.hidden_output2)\n","        self.predicted_output = self.output_layer.forward(self.hidden_output3)\n","        return self.predicted_output\n","\n","    def backward(self, y_true):\n","        gradient_output = self.loss_function_grad(self.predicted_output, y_true)\n","\n","        gradient_output = self.output_layer.backward(gradient_output, self.predicted_output)\n","        gradient_output = self.hidden_layer3.backward(gradient_output, self.hidden_output3)\n","        gradient_output = self.hidden_layer2.backward(gradient_output, self.hidden_output2)\n","        gradient_output = self.hidden_layer1.backward(gradient_output, self.hidden_output1)\n","\n","        return gradient_output\n","\n","    def update(self, learning_rate):\n","        self.hidden_layer1.update(learning_rate)\n","        self.hidden_layer2.update(learning_rate)\n","        self.hidden_layer3.update(learning_rate)\n","        self.output_layer.update(learning_rate)\n","\n","    def train(self, X_train, y_train, X_test, y_test, learning_rate, epochs, batch_size, output=True):\n","        self.dev = NetworkDevelopment(total_epochs=epochs)\n","\n","        for epoch in range(epochs):\n","            loss_list = []\n","            for X_batch, y_batch in get_batches(X_train, y_train, batch_size):\n","                predicted_output = self.forward(X_batch)\n","\n","                loss = self.loss_function(predicted_output, y_batch)\n","                loss_list.append(loss)\n","\n","                self.backward(y_batch)\n","                self.update(learning_rate)\n","\n","            avg_loss = np.mean(loss_list)\n","            acc_train = self.evaluate(X_train, y_train)\n","            acc_test = self.evaluate(X_test, y_test)\n","\n","            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n","            if output:\n","                print(epoch_str)\n","\n","    def predict(self, X):\n","        predicted_output = self.forward(X)\n","        return np.argmax(predicted_output, axis=1)\n","\n","    def evaluate(self, X, y):\n","        predicted_classes = self.predict(X)\n","        true_classes = np.argmax(y, axis=1)\n","        return np.mean(predicted_classes == true_classes)\n","\n","    def summary(self):\n","        self.dev.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"a360044ff2ca4884932585ca914cc45c","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:43:57.487217Z","start_time":"2023-04-10T16:41:02.154802Z"},"cell_id":"c870dc0df92f4042a62247d340a22862","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":220211,"execution_start":1682015105898,"source_hash":"9f197965"},"outputs":[],"source":["mln = MultiLayerNetwork(input_size=784, hidden_size=256, output_size=10, loss_function=cross_entropy, loss_function_grad=cross_entropy_grad)\n","mln.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=0.01, epochs=10, batch_size=64)\n","mln.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"5f9c0b62cb2b430883a05e3f99e3029e","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:43:57.716061Z","start_time":"2023-04-10T16:43:57.492708Z"},"cell_id":"b592172aff58428fa6cfeee99d04fe79","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":157,"execution_start":1682015326111,"source_hash":"ce22d680"},"outputs":[],"source":["# evaluate the network on the test set\n","print(f\"Accuracy: {mln.evaluate(X_test, y_test_one_hot):.4f}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"c901c6afd83642c0ba8c1fe92ca7e7d0","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Finding the right hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"15fd2770623649bcbe3ca79b1787988d","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":381,"execution_start":1682062015847,"source_hash":"b78a5adc"},"outputs":[],"source":["epochs = [10*i for i in range(1, 5)]\n","batch_sizes = [2**i for i in range(5, 11)]\n","learning_rates = [10**-i for i in range(1, 6)]\n","hidden_sizes = [64, 128, 256, 512]\n","combinations = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","\n","def test_brute_force_multi_layer(param_combos, X_train, y_train, X_test, y_test):\n","    results = []\n","    best_acc = 0\n","    best_param_combo = None\n","\n","    for i in range(len(param_combos)):\n","        try:\n","            e, b, l, h = param_combos[i]\n","            net = MultiLayerNetwork(input_size=784, hidden_size=h, output_size=10, loss_function=cross_entropy, loss_function_grad=cross_entropy_grad)\n","\n","            net.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=l, epochs=e, batch_size=b, output=False)\n","            acc = net.evaluate(X_test, y_test_one_hot)\n","\n","            results.append((e, b, l, h, accuracy))\n","            if acc > best_acc:\n","                best_acc = acc\n","                best_param_combo = param_combos[i]\n","\n","            print(f\"combo {i+1}/{len(param_combos)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {acc:.4f}\")\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            continue\n","\n","    return best_acc, best_param_combo\n","\n","ml_combos = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","print(f\"Testing {len(ml_combos)} parameter combinations\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"be4d93b634e048d7a1486dd818ef508f","deepnote_cell_type":"code"},"outputs":[],"source":["test_brute_force_multi_layer(ml_combos, X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"cell_id":"1a645e0c5a8349cc944715c1da0c088d","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Lerntagebuch"]},{"cell_type":"markdown","metadata":{"cell_id":"2bb80b580bf84a24a3ba5de91fdaf568","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW13"]},{"cell_type":"markdown","metadata":{"cell_id":"97850683cd274af699833210ce95c639","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"ff454b5016af42a0ac8465f803efeba5","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW14"]},{"cell_type":"markdown","metadata":{"cell_id":"7192f1b1f86d47b486894d3a7414798f","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Multiplikation Linear Layer vertauscht? input x weights or weights x input"]},{"cell_type":"markdown","metadata":{"cell_id":"3d3a8aea20d2499396ad10dd3969e199","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW15"]},{"cell_type":"markdown","metadata":{"cell_id":"1e5d23ef3b4c47d694fa00be3b6cd39c","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"cfcd3714d0b241fa9d325a770a40a6b0","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### KW16"]},{"cell_type":"markdown","metadata":{"cell_id":"0c291ebc9ef34e1c8d8073e301671d81","deepnote_cell_type":"markdown"},"source":["#### 17.04.2023\n","SingleLayerModel:\n","\n","Loss-Function Decision: We found that there are a bunch of different loss functions that are used in the context of neural networks. We wanted to take a deeper look into how each of those functions would perform in the case of our Single-Layer-Network and Multi-Layer-Network.\n","\n","Focal: \n","\n","Hinge:\n","\n","Final: Binary-Cross-Entropy.\n","\n","Explanation: \n","\n","---\n","Geeignete Loss- und Accuracy-Funktionen\n","- Die Wahl wurde begründet und mit anderen mögliche Funktionen verglichen?\n","- Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angege- ben (gerendert in Latex)?\n","- Die Entwicklung der Loss- und Accuracy-Funktionen wurden auf Trainings- und Testdaten- sätzen korrekt verfolgt und leicht nachvollziehbar dargestellt?\n","- Die Wahl von Lernrate und Hidden Layer-Größe wurde nachvollziehbar entschieden und begründet.\n","\n","\n","---\n","\n","For the Muli-class classification problem:\n","\n","**Loss Function Choice**: \n","Kullback: Complicated integration due to requirements\n","MSE: Rather weak, due to the focus on regression\n","\n","Cross-Entropy is ideal for Tasks 4 and 5 because it handles classification problems with probability predictions well. Other loss functions, such as Mean Squared Error, are less suitable because they are more focused on regression and have no direct connection to the classification of probabilities. Therefore, Cross-Entropy is the best choice.\n","\n","**Accuracy Functions**:\n","Accuracy: Correct / False. Basic and sufficient.\n","F1-Score: Perhaps better for exercise 3 to counteract imbalance in the dataset (90% accuracy due to few 4s != good) but not necessary for exercise 5.\n","\n","Confusion Matrix: Also probably more helpful for exercise 3. \n","\n","**Activation Functions**:\n","\n","ReLU: The best practice in the current age. Very efficient and doesn't share the problems that sigmoid has (vanishing gradients leading to slower learning and/or convergence)\n","\n","Leaky ReLU: ReLU but with a better focus on making sure that dead neurons (neurons that are updated, so that they can never be activated again) are taken care of.\n","\n","Parametric ReLU:\n","PReLU is another variation of ReLU that generalizes Leaky ReLU by making the negative slope a learnable parameter. Higher risk of overfitting.\n","\n","Sigmoid: Is just bad now, no reason to ever consider it except for an educational purpose. The main problem lies in vanishing gradients leading to slower learning and/or convergence: The vanishing gradient problem is a difficulty encountered during the training of deep neural networks using gradient-based optimization methods, such as gradient descent and its variants. This issue arises when the gradients of the loss function with respect to the network's parameters become very small as they propagate through the layers, leading to slow or stalled learning. This can happen at any stage in the training process, which makes it more difficult to counteract it properly.\n","\n","---\n","\n","Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n","- Die Wahl wurde begründet und mit anderen möglichen Funktionen verglichen.\n","- Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angegegeben (gerendert in Latex).\n","- Es wurden verschiedene Lernraten und Größen der Hidden Layer sinnvoll ausprobiert.\n","- Die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen wurde korrekt verfolgt und leicht nachvollziehbar dargestellt?\n","- Die Wahl der Hyperparameter wurde nachvollziehbar entschieden und begründet?\n","\n","Form:\n","\n","Environment angeben (Python 3.10)\n","- Code comments\n","- Executive Summary\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"35a7f005a61d45ae820758fc571da697","deepnote_cell_type":"markdown"},"source":["#### 18.04.2023\n","\n","Fragen an Stefan:\n","- Verständnis Anzahl Layers\n","- Aufgabe 3: Biased Training? Interpretation von unseren Resultaten?\n","- Keine Gradienten Berechnung im LinearLayer (Punkt 5)\n","- Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n","- Lerntagebuch auch im Notebook? Summary on top?\n","- Time import bei Aufgabe 4 erlaubt? (Zeitmessung oder Epochen gemeint)"]},{"cell_type":"markdown","metadata":{"cell_id":"70c41c553e74439ca0e2f19f2a591534","deepnote_cell_type":"markdown"},"source":["#### 19.04.2023\n","\n","* Based on the talk with Stefan it appears as if our accuracy decision for the single layer network is not ideal. So we tried out some accuracy functions, namely: F1-Score, Precision"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=88bc6171-47da-4cbc-96f1-ee851c7ac9ec' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"80ab8adbb81c461ba6b892c5c02ccdaf","kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
