{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# vta: Mini-Challenge Gradient Descent"
   ],
   "metadata": {
    "cell_id": "7b43dc16d5384c3fb18656e08cda8f1a",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Das Ziel dieser Aufgabe besteht darin, dass Sie ein grundlegendes Verständnis für numerische Näherungsverfahren in höheren Dimensionen erlangen, insbesondere für den Gradient Descent und dessen praktische Anwendung. Hierfür sollen Sie ein Jupyter Notebook erstellen und das MNIST Dataset laden und erkunden. Anschließend sollen Sie ein neuronales Netzwerk erstellen und trainieren, um die Bilder korrekt zu klassifizieren. Es dürfen nur die angegebenen Python packages verwendet werden.\n",
    "\n",
    "Ziel dieser Aufgabe ist nicht nur, Ihre mathematischen Kenntnisse unter Beweis zu stellen, sondern auch die entsprechende Kommunikation und Präsentation Ihrer Ergebnisse. Ihre Abgaben sollen also nicht nur mathematisch korrekt, sondern auch leicht verständlich und reproduzierbar sein. Genauere Angaben zu den Erwartungen an die Abgabe finden Sie in den Auswertungskriterien. Dokumentieren Sie ihren Arbeitsfortschritt und Erkenntnisgewinn in Form eines Lerntagebuchs, um Lernfortschritte, Schwierigkeiten und Erkenntnisse festzuhalten.\n",
    "Die folgenden Aufgabenstellungen präzisieren die einzelnen Bearbeitungsschritte und geben die Struktur des Notebooks vor.\n",
    "\n",
    "**Project Members:**\n",
    "- Dominik Filliger\n",
    "- Noah Leuenberger\n",
    "- Nils Fahrni\n",
    "- Oliver Pejic"
   ],
   "metadata": {
    "cell_id": "34fada14379d4a62b8c0362346edaf2c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "cell_id": "3e5ae331b5a94682aebeb4aaf613df10",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.027242Z",
     "start_time": "2023-04-10T16:34:59.025205Z"
    },
    "source_hash": "67275d9a",
    "execution_start": 1681842560550,
    "execution_millis": 3337,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe 1\n",
    "> Laden Sie das MNIST-Dataset mithilfe des torchvision-Pakets (Verwenden Sie das torchvision Paket für diese Aufgabe) und verwenden Sie matplotlib, um sich einen Überblick über die Daten zu verschaffen. Beschreiben Sie das grundlegenden Eigenschaften des Datensets, z.B. wie viele und welche Daten es enthält."
   ],
   "metadata": {
    "cell_id": "699c76c6494149bf99a3dfd734b37b37",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define transformations to be applied to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Extract the data and labels from the datasets\n",
    "X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n",
    "X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n",
    "\n",
    "# Reshape the data to be of size [N x 784]\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalize the data to be between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "cell_id": "ba102f8b2ade4e2fa64a24402e77ffdd",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.168041Z",
     "start_time": "2023-04-10T16:34:59.028883Z"
    },
    "source_hash": "1a2432ae",
    "execution_start": 1681842563817,
    "execution_millis": 277,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "classes = np.unique(y_train)\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[y_train == classes[i]][0].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Class: {classes[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "cell_id": "08b89596eab14998b3f0cac37d9fff41",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.531688Z",
     "start_time": "2023-04-10T16:34:59.168488Z"
    },
    "source_hash": "9230fd10",
    "execution_start": 1681842564095,
    "execution_millis": 570,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print('Number of training examples: ', X_train.shape[0])\n",
    "print('Number of testing examples: ', X_test.shape[0])\n",
    "print('Each image is of size: ', X_train.shape[1])\n",
    "print('There are {} classes: {}'.format(len(classes), classes))\n",
    "print('The data is of type: ', X_train.dtype)\n",
    "print('The labels are of type: ', y_train.dtype)\n",
    "print('The range of the pixel values is [{}, {}]'.format(np.min(X_train), np.max(X_train)))"
   ],
   "metadata": {
    "cell_id": "5ec8b10b20b14f8a81834723632cfd55",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.531965Z",
     "start_time": "2023-04-10T16:34:59.484990Z"
    },
    "source_hash": "d82631cb",
    "execution_start": 1681842564667,
    "execution_millis": 55,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(classes)):\n",
    "    print(f\"Class {classes[i]}: {np.sum(y_train == classes[i])} train examples, {np.sum(y_test == classes[i])} test examples\")"
   ],
   "metadata": {
    "cell_id": "4d3ae3d4087a4c139034db7db86b6290",
    "source_hash": "73db116a",
    "execution_start": 1681842564723,
    "execution_millis": 6,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "> Erstellen Sie eine Klasse für ein lineares Layer mit beliebig vielen Knoten. Implementieren Sie die Methoden forward, backward und update mithilfe von numpy. Schreiben sie geeignete Unittests, um die Funktionsweise der Funktion zu prüfen."
   ],
   "metadata": {
    "cell_id": "92ec82d02b0f415badc461b13356cc7f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Layer"
   ],
   "metadata": {
    "cell_id": "210d081489f546a38fa870a82b5f5b57",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.x = None\n",
    "        self.bias_grad = None\n",
    "        self.weights_grad = None\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.weights_grad = np.dot(self.x.T, grad_output)\n",
    "        self.bias_grad = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        return np.dot(grad_output, self.weights.T)\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.weights -= lr * self.weights_grad\n",
    "        self.bias -= lr * self.bias_grad"
   ],
   "metadata": {
    "cell_id": "5e1cca13622741cea3a51730aa623e6c",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.532012Z",
     "start_time": "2023-04-10T16:34:59.502315Z"
    },
    "source_hash": "7c8aad3a",
    "execution_start": 1681842564749,
    "execution_millis": 2,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unit Testing"
   ],
   "metadata": {
    "cell_id": "6309c2f8970942f0a47c09422e878ca8",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import unittest\n",
    "\n",
    "class TestLinearLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.input_size = 10\n",
    "        self.output_size = 5\n",
    "        self.linear_layer = LinearLayer(self.input_size, self.output_size)\n",
    "\n",
    "    def test_forward(self):\n",
    "        x = np.random.randn(1, self.input_size)\n",
    "        output = self.linear_layer.forward(x)\n",
    "        self.assertEqual(output.shape, (1, self.output_size))\n",
    "\n",
    "    def test_backward(self):\n",
    "        x = np.random.randn(1, self.input_size)\n",
    "        output = self.linear_layer.forward(x)\n",
    "        grad_output = np.random.randn(1, self.output_size)\n",
    "        grad_input = self.linear_layer.backward(grad_output)\n",
    "        self.assertEqual(grad_input.shape, (1, self.input_size))\n",
    "\n",
    "    def test_update(self):\n",
    "        self.linear_layer.weights = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "        self.linear_layer.bias = np.array([[1, 2]], dtype=np.float32)\n",
    "        self.linear_layer.weights_grad = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "        self.linear_layer.bias_grad = np.array([[1, 2]], dtype=np.float32)\n",
    "\n",
    "        self.linear_layer.update(0.1)\n",
    "        self.assertTrue(np.allclose(self.linear_layer.weights, np.array([[0.9, 1.8], [2.7, 3.6]])))\n",
    "        self.assertTrue(np.allclose(self.linear_layer.bias, np.array([[0.9, 1.8]])))\n",
    "\n",
    "    def test_shapes(self):\n",
    "        x = np.random.randn(1, self.input_size)\n",
    "        output = self.linear_layer.forward(x)\n",
    "        grad_output = np.random.randn(1, self.output_size)\n",
    "        grad_input = self.linear_layer.backward(grad_output)\n",
    "\n",
    "        self.assertEqual(self.linear_layer.weights_grad.shape, (self.input_size, self.output_size))\n",
    "        self.assertEqual(self.linear_layer.bias_grad.shape, (1, self.output_size))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "metadata": {
    "cell_id": "d8a38c8a7ded498a931489c32eb3ec13",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.532292Z",
     "start_time": "2023-04-10T16:34:59.506533Z"
    },
    "source_hash": "f0c4634e",
    "execution_start": 1681842564749,
    "execution_millis": 7,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe 3\n",
    "\n",
    "> Erstellen Sie ein neuronales Netzwerk in numpy mit einem Hidden Linear Layer und einem Output Knoten. Trainieren Sie das Netzwerk darauf, die Ziffer 4 korrekt zu identifizieren (d.h. der Output soll 1 für diese Ziffer und 0 für alle anderen Ziffern sein). Trainieren Sie das Netzwerk auf den Trainingsdaten und evaluieren Sie es anhand von Testdaten. Verwenden Sie eine geeignete Loss-Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl mit einer Abwägung der Vor- und Nachteile. Diskutieren Sie kurz weitere Optionen für Loss und Accuracy."
   ],
   "metadata": {
    "cell_id": "ed2a6ac4e7774c9d844cd388d27c5894",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-Linear Activation Functions\n",
    "\n",
    "- https://medium.com/intuitionmath/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d"
   ],
   "metadata": {
    "cell_id": "24b94ff860dc4bc4a06e3c4f46eac393",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return x > 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500) # clip x to prevent overflow\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ],
   "metadata": {
    "cell_id": "a19b8ffca93345bfb6919411ad331710",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.532332Z",
     "start_time": "2023-04-10T16:34:59.513311Z"
    },
    "source_hash": "785d5fb",
    "execution_start": 1681842564758,
    "execution_millis": 4,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss Functions\n",
    "\n",
    "- https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a"
   ],
   "metadata": {
    "cell_id": "e3c318a6599a40f2b951bb52c726198f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mean Squared Error (MSE)"
   ],
   "metadata": {
    "cell_id": "8a903d4b618f47439f8e7bca19994e83",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def mse(y_pred, y_true):\n",
    "    batch_size = y_pred.shape[0]\n",
    "    loss = np.sum((y_pred - y_true)**2) / batch_size\n",
    "    return loss\n",
    "\n",
    "def mse_grad(y_pred, y_true):\n",
    "    batch_size = y_pred.shape[0]\n",
    "    grad = 2 * (y_pred - y_true) / batch_size\n",
    "    return grad"
   ],
   "metadata": {
    "cell_id": "a5fc641f1b7a429e8abe8745d603ab3f",
    "source_hash": "182b28c1",
    "execution_start": 1681842564761,
    "execution_millis": 5,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Binary Cross-Entropy / Log Loss\n",
    "$$\n",
    "H_p(y)=-\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log \\left(p\\left(y_i\\right)\\right)+\\left(1-y_i\\right) \\cdot \\log \\left(1-p\\left(y_i\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The gradient:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial H_p(y)}{\\partial y} & =\\frac{\\partial(-t \\log (y)-(1-t) \\log (1-y))}{\\partial y}=\\frac{\\partial(-t \\log (y))}{\\partial y}+\\frac{\\partial(-(1-t) \\log (1-y))}{\\partial y} \\\\\n",
    "& =-\\frac{t}{y}+\\frac{1-t}{1-y}=\\frac{y-t}{y(1-y)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "- https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715\n",
    "- https://stats.stackexchange.com/questions/219241/gradient-for-logistic-loss-function\n",
    "x\n",
    "Derivative:\n",
    "- https://peterroelants.github.io/posts/cross-entropy-logistic/"
   ],
   "metadata": {
    "cell_id": "406ed272899e43438a4563dcc66b77be",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "\n",
    "def binary_cross_entropy_grad(y_pred, y_true):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred)) # add eps to prevent division by 0"
   ],
   "metadata": {
    "cell_id": "c4e84f7f7a72426eaaa103a9a459b9a3",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.532406Z",
     "start_time": "2023-04-10T16:34:59.517129Z"
    },
    "source_hash": "339a6c15",
    "execution_start": 1681842564765,
    "execution_millis": 11,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Focal Loss\n",
    "$$\n",
    "FL(p_t)=-\\alpha(1-p_t)^\\gamma \\cdot log(p_t)\n",
    "$$"
   ],
   "metadata": {
    "cell_id": "f45a375392874bad9a99b21cbae7c64b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def focal_loss(y_pred, y_true, gamma=2, alpha=0.25):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n",
    "    loss = -alpha * (y_true * ((1 - y_pred)**gamma) * np.log(y_pred) + (1 - y_true) * (y_pred**gamma) * np.log(1 - y_pred))\n",
    "    return loss.mean()\n",
    "\n",
    "def focal_loss_grad(y_pred, y_true, gamma=2, alpha=0.25):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    loss_grad = -alpha_t * (1 - pt)**(gamma-1) * (gamma*y_true*np.log(y_pred) + (1 - y_true)*np.log(1 - y_pred))\n",
    "    loss_grad /= y_pred.shape[0]\n",
    "\n",
    "    return loss_grad"
   ],
   "metadata": {
    "cell_id": "dd003affaf334bd0a4a23f854cfcf2fc",
    "source_hash": "47dd32d3",
    "execution_start": 1681842564776,
    "execution_millis": 5,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tracking network development\n",
    "In order to track the development of the training process, we define a class that stores the training and validation loss and accuracy for each epoch. We also define a function that plots the training and validation loss and accuracy for each epoch. This allows us to see how the training process develops over time."
   ],
   "metadata": {
    "cell_id": "ba66da11f7f448c789e9fc56fd04cc87",
    "collapsed": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NetworkDevelopment:\n",
    "    \"\"\"\n",
    "    Stores the training and validation loss and accuracy for each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_epochs : int\n",
    "        The total number of epochs the network will be trained.\n",
    "    \"\"\"\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.losses = []\n",
    "        self.accuracies_train = []\n",
    "        self.accuracies_test = []\n",
    "\n",
    "    def add_epoch(self, epoch_number, loss, acc_train, acc_test):\n",
    "        \"\"\"\n",
    "        Adds a new development step to the network development.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : float\n",
    "            The loss of the current epoch.\n",
    "        acc_train : float\n",
    "            The accuracy of the training data of the current epoch.\n",
    "        acc_test : float\n",
    "            The accuracy of the validation data of the current epoch.\n",
    "        \"\"\"\n",
    "        self.losses.append(loss)\n",
    "        self.accuracies_train.append(acc_train)\n",
    "        self.accuracies_test.append(acc_test)\n",
    "\n",
    "        return f'Epoch {epoch_number}/{self.total_epochs} - loss: {loss:.4f} - acc_train: {acc_train:.4f} - acc_test: {acc_test:.4f}'\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation loss and accuracy for each epoch.\n",
    "        \"\"\"\n",
    "        epochs = np.arange(1, self.total_epochs + 1)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.plot(epochs, self.losses)\n",
    "        ax1.set_title('Loss over Epochs')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax2.plot(epochs, self.accuracies_train, label='Training')\n",
    "        ax2.plot(epochs, self.accuracies_test, label='Test')\n",
    "        ax2.set_title('Accuracy over Epochs')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Prints the final loss and accuracy of the training and validation data and the average improvements per epoch.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "        print(f'avg acc change / epoch (Training set): {np.mean(np.diff(self.accuracies_train)):.4f}')\n",
    "        print(f'avg acc change / epoch (Test set): {np.mean(np.diff(self.accuracies_test)):.4f}')\n",
    "        print(f'avg loss change / epoch: {np.mean(np.diff(self.losses)):.4f}')"
   ],
   "metadata": {
    "cell_id": "e24f2cdcf5ae4ea9afb3978a9d199588",
    "collapsed": false,
    "source_hash": "9698ff3b",
    "execution_start": 1681842564781,
    "execution_millis": 3,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Batch Creation\n",
    "\n",
    "The function `get_batches(x, y, batch_size)` creates batches of images and labels from the training set. The number of images in a batch is defined by the batch size. The batch size is a hyperparameter that can be tuned to improve the training process. The batch size is a trade-off between the number of images used for training and the number of training steps per epoch. A larger batch size results in fewer training steps per epoch, but the training process is less accurate. A smaller batch size results in more training steps per epoch, but the training process is more accurate."
   ],
   "metadata": {
    "cell_id": "2158077846bd4403bde4c7f7c46beaa8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Returns a generator that yields batches of size batch_size from the given data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        The input data.\n",
    "    y : numpy.ndarray\n",
    "        The target data.\n",
    "    batch_size : int\n",
    "        The size of each batch.\n",
    "    \"\"\"\n",
    "    n_batches = len(x) // batch_size\n",
    "    \n",
    "    # shuffle data before creating batches\n",
    "    idx = np.random.permutation(len(x))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    for i in range(0, n_batches * batch_size, batch_size):\n",
    "        x_batch = x[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        yield x_batch, y_batch"
   ],
   "metadata": {
    "cell_id": "52d080134e5e4a29be081caf9a5cfdf7",
    "source_hash": "dbfdce53",
    "execution_start": 1681842564786,
    "execution_millis": 1,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single-Layer Network "
   ],
   "metadata": {
    "cell_id": "b8ea137f77e241af98f1b042adf1b008",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SingleLayerNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, loss_function, loss_function_grad):\n",
    "        self.dev = None\n",
    "        self.y_pred = None\n",
    "        self.h = None\n",
    "        self.x = None\n",
    "        self.development = None\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_grad = loss_function_grad\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.linear1 = LinearLayer(input_size, hidden_size)\n",
    "        self.linear2 = LinearLayer(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h = relu(self.linear1.forward(x))\n",
    "        self.y_pred = sigmoid(self.linear2.forward(self.h))\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "        self.y_pred = self.y_pred.reshape(-1, 1)\n",
    "\n",
    "        grad_output = self.loss_function_grad(self.y_pred, y_true)\n",
    "\n",
    "        grad_output = sigmoid_grad(grad_output) * grad_output\n",
    "        grad_output = self.linear2.backward(grad_output)\n",
    "\n",
    "        grad_output = relu_grad(grad_output) * grad_output\n",
    "        grad_output = self.linear1.backward(grad_output)\n",
    "\n",
    "        return grad_output\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.linear1.update(lr)\n",
    "        self.linear2.update(lr)\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, lr, epochs, batch_size, threshold=0.5, output=True):\n",
    "        self.dev = NetworkDevelopment(total_epochs=epochs)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss_list = []\n",
    "            for x_batch, y_batch in get_batches(X_train, y_train, batch_size):\n",
    "                y_pred = self.forward(x_batch)\n",
    "                y_pred = (self.y_pred > 0.5).astype(int)\n",
    "\n",
    "                loss = self.loss_function(y_pred, y_batch)\n",
    "                loss_list.append(loss)\n",
    "\n",
    "                self.backward(y_batch)\n",
    "                self.update(lr)\n",
    "\n",
    "            acc_train = self.evaluate(X_train, y_train, threshold)\n",
    "            acc_test = self.evaluate(X_test, y_test, threshold)\n",
    "            avg_loss = np.mean(loss_list)\n",
    "\n",
    "            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n",
    "            if output:\n",
    "                print(epoch_str)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.forward(X) > threshold).astype(int).reshape(-1, 1)\n",
    "\n",
    "    def evaluate(self, X, y, threshold=0.5):\n",
    "        y_pred = self.predict(X, threshold).flatten()\n",
    "        return np.mean(y_pred == y) # TODO: fix deprecation warning\n",
    "\n",
    "    def summary(self):\n",
    "        self.dev.summary()"
   ],
   "metadata": {
    "cell_id": "0e80b9164ffb467ea390f26d3f34f8ec",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.532515Z",
     "start_time": "2023-04-10T16:34:59.522530Z"
    },
    "source_hash": "f0689035",
    "execution_start": 1681842564794,
    "execution_millis": 6,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training on digit 4"
   ],
   "metadata": {
    "cell_id": "b1816d7fb6ef4ccc8c4fad5dc0ddcba1",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y_train_4_binary = (y_train == 4).astype(int)\n",
    "y_test_4_binary = (y_test == 4).astype(int)\n",
    "\n",
    "# Separate the digits\n",
    "digits = [X_train[y_train == i] for i in range(10)]\n",
    "digits_test = [X_test[y_test == i] for i in range(10)]\n",
    "\n",
    "# Calculate the number of samples for each not 4 class\n",
    "samples_per_class = len(digits[4]) // 9\n",
    "samples_per_class_test = len(digits_test[4]) // 9\n",
    "\n",
    "# Prepare the not 4s subset with equal representation of every other digit\n",
    "X_train_not_4 = np.concatenate([digits[i][:samples_per_class] for i in range(10) if i != 4])\n",
    "y_train_not_4 = np.concatenate([np.full(samples_per_class, i) for i in range(10) if i != 4])\n",
    "\n",
    "X_test_not_4 = np.concatenate([digits_test[i][:samples_per_class_test] for i in range(10) if i != 4])\n",
    "y_test_not_4 = np.concatenate([np.full(samples_per_class_test, i) for i in range(10) if i != 4])\n",
    "\n",
    "# Shuffle the not 4s subset\n",
    "indices_not_4 = np.random.permutation(len(X_train_not_4))\n",
    "X_train_not_4 = X_train_not_4[indices_not_4]\n",
    "y_train_not_4 = y_train_not_4[indices_not_4]\n",
    "\n",
    "indices_not_4_test = np.random.permutation(len(X_test_not_4))\n",
    "X_test_not_4 = X_test_not_4[indices_not_4_test]\n",
    "y_test_not_4 = y_test_not_4[indices_not_4_test]\n",
    "\n",
    "y_train_not_4_binary = (y_train_not_4 == 4).astype(int)\n",
    "y_test_not_4_binary = (y_test_not_4 == 4).astype(int)\n",
    "\n",
    "\n",
    "# Shuffle the 4s subset\n",
    "indices_4 = np.random.permutation(len(digits[4]))\n",
    "X_train_4_only = digits[4][indices_4]\n",
    "y_train_4_only = np.full(len(digits[4]), 4)\n",
    "\n",
    "\n",
    "indices_4_test = np.random.permutation(len(digits_test[4]))\n",
    "X_test_4_only = digits_test[4][indices_4_test]\n",
    "y_test_4_only = np.full(len(digits_test[4]), 4)\n",
    "\n",
    "y_train_4_only_binary = (y_train_4_only == 4).astype(int)\n",
    "y_test_4_only_binary = (y_test_4_only == 4).astype(int)\n",
    "\n",
    "\n",
    "# Create the mixed dataset with a 50:50 split\n",
    "X_train_mix = np.concatenate((X_train_4_only[:len(X_train_not_4)], X_train_not_4))\n",
    "y_train_mix = np.concatenate((y_train_4_only[:len(y_train_not_4)], y_train_not_4))\n",
    "\n",
    "X_test_mix = np.concatenate((X_test_4_only[:len(X_test_not_4)], X_test_not_4))\n",
    "y_test_mix = np.concatenate((y_test_4_only[:len(y_test_not_4)], y_test_not_4))\n",
    "\n",
    "\n",
    "# Shuffle the mixed dataset\n",
    "indices_mix = np.random.permutation(len(X_train_mix))\n",
    "X_train_mix = X_train_mix[indices_mix]\n",
    "y_train_mix = y_train_mix[indices_mix]\n",
    "\n",
    "indices_mix_test = np.random.permutation(len(X_test_mix))\n",
    "X_test_mix = X_test_mix[indices_mix_test]\n",
    "y_test_mix = y_test_mix[indices_mix_test]\n",
    "\n",
    "# Convert the labels to binary\n",
    "y_train_mix_binary = (y_train_mix == 4).astype(int)\n",
    "y_test_mix_binary = (y_test_mix == 4).astype(int)\n",
    "\n",
    "# Print the count of 4s and not 4s in the train set\n",
    "print(f\"4s: {np.sum(y_train_mix_binary == 1)} - Not 4s: {np.sum(y_train_mix_binary == 0)}\")\n"
   ],
   "metadata": {
    "cell_id": "2294e1e6143c4346b424448f9475f656",
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-10T16:34:59.802387Z",
     "start_time": "2023-04-10T16:34:59.023452Z"
    },
    "source_hash": "7736d4a7",
    "execution_start": 1681842817005,
    "execution_millis": 214,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "cell_id": "65a117722ba446c8bbe3d29d72c26c27",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "network_single_mixed = SingleLayerNetwork(input_size=784, hidden_size=512, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n",
    "network_single_mixed.train(X_train_mix, y_train_mix_binary, X_test_mix , y_test_mix_binary, lr=0.00001, epochs=28, batch_size=128, threshold=0.5)\n",
    "network_single_mixed.summary()\n",
    "\n",
    "acc_4_only = network_single_mixed.evaluate(X_test_4_only, y_test_4_only_binary)\n",
    "acc_not_4 = network_single_mixed.evaluate(X_test_not_4, y_test_not_4_binary)\n",
    "print(f\"Accuracy on 4s: {acc_4_only} - Accuracy on not 4s: {acc_not_4}\")\n",
    "\n"
   ],
   "metadata": {
    "cell_id": "ffe3986c790644d78f9cc8a89e36db54",
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-10T16:35:41.520284Z",
     "start_time": "2023-04-10T16:34:59.802503Z"
    },
    "source_hash": "fa25d955",
    "execution_start": 1681842565040,
    "execution_millis": 104167,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "normal_acc = network_single_mixed.evaluate(X_test, y_test_binary)\n",
    "print(f\"Accuracy on normal test set: {normal_acc}\")"
   ],
   "metadata": {
    "cell_id": "8d5e1162a36346c3983f27859838db21",
    "source_hash": "3019dea2",
    "execution_start": 1681842669091,
    "execution_millis": 412,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other approaches"
   ],
   "metadata": {
    "cell_id": "b44b39844d864f0194d435b3f90e8453",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the network on the normal train set\n",
    "network_single_normal = SingleLayerNetwork(input_size=784, hidden_size=256, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n",
    "network_single_normal.train(X_train, y_train_4_binary, X_test, y_train_4_binary , lr=0.00001, epochs=10, batch_size=64, threshold=0.5)\n",
    "network_single_normal.summary()"
   ],
   "metadata": {
    "cell_id": "ff9e3ed4618b42bba4a7a2f8a9003439",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:39:41.997162Z",
     "start_time": "2023-04-10T16:35:41.540447Z"
    },
    "source_hash": "f0832a35",
    "execution_start": 1681842842984,
    "execution_millis": 8013,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# test 4s only\n",
    "network_single_four = SingleLayerNetwork(input_size=784, hidden_size=256, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n",
    "network_single_four.train(X_train_4_only, y_train_4_only_binary, X_test, y_test_4_binary, lr=0.00000001, epochs=10, batch_size=20, threshold=0.5)\n",
    "network_single_four.summary()"
   ],
   "metadata": {
    "cell_id": "25475be179874f6db64e280b4420e062",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:41:02.319376Z",
     "start_time": "2023-04-10T16:39:41.980759Z"
    },
    "source_hash": "fe17bff0",
    "execution_start": 1681842900492,
    "execution_millis": 40347,
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing on digit 4"
   ],
   "metadata": {
    "cell_id": "1fd1afe6ec2b4806a7b09e76679014e6",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# test the network\n",
    "accuracy = network_single_normal.evaluate(X_test, y_test_4_binary, 0.5)\n",
    "accuracy_4 = network_single_four.evaluate(X_test_4_only, y_test_4_only_binary, 0.5)\n",
    "accuracy_mix = network_single_mixed.evaluate(X_test, y_test_mix_binary , 0.5)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f} - Accuracy 4s only: {accuracy_4:.4f} - Accuracy mixed: {accuracy_mix:.4f}\")"
   ],
   "metadata": {
    "cell_id": "e3793cf8f7c449ceafeb3f7e82acf614",
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-10T16:41:02.319970Z",
     "start_time": "2023-04-10T16:41:02.153983Z"
    },
    "source_hash": "3e0d78a3",
    "execution_start": 1681209894512,
    "execution_millis": 632774995,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# maybe training more different sets helps\n",
    "#network_single_all = SingleLayerNetwork(input_size=784, hidden_size=256, output_size=1)\n",
    "#network_single_all.train(X_train_mix, y_train_mix_binary, lr=0.00000001, epochs=10, batch_size=20, threshold=0.5)\n",
    "#network_single_all.train(X_test, y_train_4_binary, lr=0.00000001, epochs=10, batch_size=20, threshold=0.5)\n",
    "\n",
    "#accuracy_all = network_single_all.evaluate(X_test, y_test_4_binary, 0.5)\n",
    "#accuracy_all_4 = network_single_all.evaluate(X_test_4_only, y_test_4_only_binary, 0.5)\n",
    "#accuracy_all_mix = network_single_all.evaluate(X_test, y_test_mix_binary , 0.5)\n",
    "\n",
    "#print(f\"Accuracy: {accuracy_all:.4f} - Accuracy 4s only: {accuracy_all_4:.4f} - Accuracy mixed: {accuracy_all_mix:.4f}\")"
   ],
   "metadata": {
    "cell_id": "3cbdb137c51e4a8e90b38127cecae689",
    "collapsed": false,
    "source_hash": "6f561307",
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe 4\n",
    "\n",
    "> Trainieren Sie das Netzwerk mit verschiedenen Lernraten und Größen des Hidden Layers. Verfolgen Sie während des Trainings die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen und entscheiden Sie, welche Wahl von Lernrate und Hidden Layer-Größe die besten Ergebnisse in geringster Zeit liefert."
   ],
   "metadata": {
    "cell_id": "0da28de0baec4c7581de9dbd3bbae77b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the right hyperparameters\n",
    "\n",
    "In order to find the right hyperparameters, we define a function that trains the network with different learning rates, epochs, hidden layer sizes and batch sizes. The result for each combination is then stored and analyzed.\n",
    "\n",
    "This brute force approach is not very efficient, but it allows us to find the best hyperparameters for the network.\n",
    "\n",
    "> **Note:** The brute force process takes a long time to complete. Therefore, we have commented out the code that runs the brute force process. The best result is described further below."
   ],
   "metadata": {
    "cell_id": "3b2d9758a4ea428a95bf4ab9ad582a4c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes):\n",
    "    return [(e, b, l, h) for e in epochs for b in batch_sizes for l in learning_rates for h in hidden_sizes]"
   ],
   "metadata": {
    "cell_id": "78bb1492853842348a6335d8f09ed5ff",
    "collapsed": false,
    "source_hash": "fe3eb418",
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "epochs = [10, 20, 30]\n",
    "batch_sizes = [2**i for i in range(4, 11)]\n",
    "learning_rates = [10**-i for i in range(1, 8)]\n",
    "hidden_sizes = [32, 64, 128, 256, 512]\n",
    "combinations = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n",
    "print(f\"Testing {len(combinations)} parameter combinations\")\n",
    "\n",
    "results = []\n",
    "for i in range(len(combinations)):\n",
    "    e, b, l, h = combinations[i]\n",
    "    network = SingleLayerNetwork(input_size=784, hidden_size=h, output_size=1, loss_function=binary_cross_entropy, loss_function_grad=binary_cross_entropy_grad)\n",
    "\n",
    "    start = time.time()\n",
    "    network.train(X_train, y_train_4_binary, X_test, y_test_4_binary, lr=l, epochs=e, batch_size=b, output=False)\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    accuracy = network.evaluate(X_test, y_test_4_binary)\n",
    "\n",
    "    #accuracy_4 = network.evaluate(X_test_4_only, y_test_4_only_binary)\n",
    "\n",
    "    #network_mix = SingleLayerNetwork(input_size=784, hidden_size=h, output_size=1)\n",
    "    #network_mix.train(X_train_mix, y_train_mix_binary, lr=l, epochs=e, batch_size=b)\n",
    "\n",
    "    #accuracy_mix = network_mix.evaluate(X_test, y_test_4_binary)\n",
    "    #accuracy_mix_4 = network_mix.evaluate(X_test_4_only, y_test_4_only_binary)\n",
    "\n",
    "    # save the results\n",
    "    result = (e, b, l, h, accuracy, total_time)\n",
    "    results.append(result)\n",
    "\n",
    "    print(f\"combo {i+1}/{len(combinations)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {accuracy:.4f} - Time: {total_time:.4f}s\")\n",
    "    #if (accuracy * 0.5 + accuracy_4 * 0.5) > (results[-1][5] + results[-1][4]):\n",
    "    #    results.append((e, b, l, h, accuracy, accuracy_4, end-start))\n",
    "    #    print(f\"Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {accuracy:.4f} - Accuracy_4: {accuracy_4:.4f}\")"
   ],
   "metadata": {
    "cell_id": "b961ccc321fa421daeab9d25d610b8f0",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:41:02.320020Z",
     "start_time": "2023-04-10T16:41:02.154130Z"
    },
    "source_hash": "3ab6bb76",
    "execution_start": 1681322159279,
    "execution_millis": 520510230,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "results.sort(key=lambda x: x[4], reverse=True) # sort by accuracy\n",
    "for r in results[:10]:\n",
    "    print(f\"Epochs: {r[0]} - Batch size: {r[1]} - Learning rate: {r[2]} - Hidden size: {r[3]} - Accuracy: {r[4]:.4f} - Time: {r[5]:.4f}\")"
   ],
   "metadata": {
    "cell_id": "6091be865c7240ccae3e8cd47375eb87",
    "collapsed": false,
    "source_hash": "c625401a",
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Result\n",
    "\n",
    "The best result was achieved with the following hyperparameters:"
   ],
   "metadata": {
    "cell_id": "c3916096aa0a4fcc92a7148be3e4f485",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe 5\n",
    "\n",
    "> Erweitern Sie das Netzwerk auf 3 Hidden Layer mit gleicher Größe und 10 Outputs. Das Ziel ist die korrekte Klassifizierung aller Ziffern. Verwenden Sie eine geeignete Loss-Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl und diskutieren Sie kurz weitere Möglichkeiten. Variieren Sie die Lernrate und die Größe der Hidden Layer und wählen Sie das beste Ergebnis aus."
   ],
   "metadata": {
    "cell_id": "d806feaaaee6433d9ae06b10241af8ed",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-Linear Activation Functions\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. Softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{z})_i=\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "Source: https://machinelearningmastery.com/softmax-activation-function-with-python/"
   ],
   "metadata": {
    "cell_id": "e12825bec60840b29a304a87887cf3b5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True) # prevent overflow\n",
    "    x = np.exp(x)\n",
    "    x = x / np.sum(x, axis=1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "def softmax_grad(x):\n",
    "    return softmax(x) * (1 - softmax(x))"
   ],
   "metadata": {
    "cell_id": "5b6ba5c2f3c04cd7b35ef7c5cc866dda",
    "source_hash": "d7f9a7f0",
    "execution_start": 1681748633985,
    "execution_millis": 94035528,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss functions"
   ],
   "metadata": {
    "cell_id": "bb3345c62b4c43fa82e66c871e6c841e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cross-Entropy\n",
    "$$\n",
    " L =-\\frac{1}{N}\\left(\\sum_{i=1}^N \\mathbf{y}_{\\mathbf{i}} \\cdot \\log \\left(\\hat{\\mathbf{y}}_{\\mathbf{i}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Cross-entropy is the default loss function to use for multi-class classification problems. \n",
    "\n",
    "Cross entropy computes a score that summarizes the average difference between the actual and predicted probability distributions for all classes of the problem. The score is minimized and a perfect cross entropy value is 0.\n",
    "\n",
    "The ground truth $y$ gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from the estimate $\\hat{\\mathbf{y}}$. For example:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& L =-(1 \\times \\log (0.1)+0 \\times \\log (0.5)+\\ldots) \\\\\n",
    "& L =-\\log (0.1) \\approx 2.303\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The main feature of this loss function is that only the probabilities of the correct classes are rewarded/punished. The cross-entropy loss function is often averaged over the batch size, so that the loss function is independent of the batch size.\n",
    "\n",
    "When using the cross-entropy loss function, the gradient of the loss function is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{j}} = \\hat{\\mathbf{y}}_i-\\mathbf{y}_i\n",
    "$$\n",
    "\n",
    "This partial derivative take advantage of the fact that the Softmax activation function is a normalized exponential function. Therefore, the gradient of the cross-entropy loss function is simplified to the difference between the predicted and the actual class probabilities.\n",
    "\n",
    "Calculating the gradient of the cross-entropy loss function is nicely illustrated in the following image:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "**Advantages**:\n",
    "- Why effective? \n",
    "    It measures difference between predicted probabilities and true labels, suitable for multi-class classification.\n",
    "- Why stable? \n",
    "    Softmax prevents saturation, allowing gradient flow.\n",
    "- Why faster convergence? \n",
    "    It penalizes incorrect predictions heavily, promoting quicker learning.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Why sensitive to imbalance? \n",
    "    It's an average loss over classes; imbalance can skew learning.\n",
    "- Why not for non-probabilistic outputs? \n",
    "    Assumes predicted output is a probability distribution.\n",
    "\n",
    "Sources: \n",
    "- https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/\n",
    "- https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "- https://machinelearningmastery.com/cross-entropy-for-machine-learning/"
   ],
   "metadata": {
    "cell_id": "0027eb67c095494397e56e5f0812dff2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n",
    "    loss = -(y_true * np.log(y_pred)).sum(axis=1).mean()\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_grad(y_pred, y_true):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n",
    "    grad = y_pred - y_true\n",
    "    return grad"
   ],
   "metadata": {
    "cell_id": "96e9ccccb0b1468a9188ef43b289f484",
    "source_hash": "4a6ffaf5",
    "execution_start": 1681747914483,
    "execution_millis": 94755031,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kullback–Leibler divergence\n",
    "Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
    "\n",
    "$$\n",
    "D_{K L}(P \\| Q)=\\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "A KL divergence loss of 0 indicates that the distributions are identical. In practice, the behavior of KL divergence is very similar to cross entropy. It calculates how much information is lost when the predicted probability distribution is used to approximate the desired target probability distribution.\n",
    "\n",
    "The Kullback-Leibler divergence (KL divergence) is not be ideal for the use case at hand due to the following mathematical properties:\n",
    "\n",
    "- **Asymmetry**: $L_{KL}(P || Q) \\neq L_{KL}(Q || P)$, which can lead to inconsistency in measuring the divergence between the true probability distribution $P$ and the predicted distribution $Q$.\n",
    "\n",
    "- **Sensitivity to outliers**: The KL divergence can be largely influenced by extreme values, which may affect the model's performance on noisy or imbalanced datasets."
   ],
   "metadata": {
    "cell_id": "aa78548f2bd94de796df9a8e070f40bc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choice of the loss function"
   ],
   "metadata": {
    "cell_id": "6d5f09795a6149689f9b13290a4e4eba",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a way to represent categorical data in a numerical format. In the case of MNIST, the target variable is the digit that each image represents, which can take on values from 0 to 9. One-hot encoding converts each digit into a binary vector of length 10, where each element of the vector represents a possible digit value. For example, the digit 3 would be represented as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], where the fourth element is a 1 and the other elements are 0."
   ],
   "metadata": {
    "cell_id": "70e4a21486884685a28c8e9d012666f8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def one_hot_encode(y):\n",
    "    y_one_hot = np.zeros((len(y), 10))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train)\n",
    "y_test_one_hot = one_hot_encode(y_test)"
   ],
   "metadata": {
    "cell_id": "c23f59a1a24d4d3cb11bb352824ec71f",
    "source_hash": "52b66065",
    "execution_start": 1681747914489,
    "execution_millis": 94755025,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-Layer Network"
   ],
   "metadata": {
    "cell_id": "75a020585d554ddf8ceaf79ba9a5e55d",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiLayerNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, loss_function, loss_function_grad):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_grad = loss_function_grad\n",
    "\n",
    "        self.input_layer = LinearLayer(input_size, hidden_size)\n",
    "        self.hidden_layer1 = LinearLayer(hidden_size, hidden_size)\n",
    "        self.hidden_layer2 = LinearLayer(hidden_size, hidden_size)\n",
    "        self.output_layer = LinearLayer(hidden_size, output_size)\n",
    "\n",
    "        self.dev = None\n",
    "        self.hidden_output1 = None\n",
    "        self.hidden_output2 = None\n",
    "        self.hidden_output3 = None\n",
    "        self.predicted_output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden_output1 = relu(self.input_layer.forward(x))\n",
    "        self.hidden_output2 = relu(self.hidden_layer1.forward(self.hidden_output1))\n",
    "        self.hidden_output3 = relu(self.hidden_layer2.forward(self.hidden_output2))\n",
    "        self.predicted_output = softmax(self.output_layer.forward(self.hidden_output3))\n",
    "        return self.predicted_output\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        gradient_output = self.loss_function_grad(self.predicted_output, y_true)\n",
    "\n",
    "        gradient_output = softmax_grad(self.predicted_output) * gradient_output\n",
    "        gradient_output = self.output_layer.backward(gradient_output)\n",
    "\n",
    "        gradient_output = relu_grad(self.hidden_output3) * gradient_output\n",
    "        gradient_output = self.hidden_layer2.backward(gradient_output)\n",
    "\n",
    "        gradient_output = relu_grad(self.hidden_output2) * gradient_output\n",
    "        gradient_output = self.hidden_layer1.backward(gradient_output)\n",
    "\n",
    "        gradient_output = relu_grad(self.hidden_output1) * gradient_output\n",
    "        gradient_output = self.input_layer.backward(gradient_output)\n",
    "\n",
    "        return gradient_output\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.input_layer.update(learning_rate)\n",
    "        self.hidden_layer1.update(learning_rate)\n",
    "        self.hidden_layer2.update(learning_rate)\n",
    "        self.output_layer.update(learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, learning_rate, epochs, batch_size, output=True):\n",
    "        self.dev = NetworkDevelopment(total_epochs=epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss_list = []\n",
    "            for X_batch, y_batch in get_batches(X_train, y_train, batch_size):\n",
    "                predicted_output = self.forward(X_batch)\n",
    "\n",
    "                loss = self.loss_function(predicted_output, y_batch)\n",
    "                loss_list.append(loss)\n",
    "\n",
    "                self.backward(y_batch)\n",
    "                self.update(learning_rate)\n",
    "\n",
    "            avg_loss = np.mean(loss_list)\n",
    "            acc_train = self.evaluate(X_train, y_train)\n",
    "            acc_test = self.evaluate(X_test, y_test)\n",
    "\n",
    "            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n",
    "            if output:\n",
    "                print(epoch_str)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted_output = self.forward(X)\n",
    "        return np.argmax(predicted_output, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predicted_classes = self.predict(X)\n",
    "        true_classes = np.argmax(y, axis=1)\n",
    "        return np.mean(predicted_classes == true_classes)\n",
    "\n",
    "    def summary(self):\n",
    "        self.dev.summary()"
   ],
   "metadata": {
    "cell_id": "93f7f79053f744ad97a13166292892e8",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:41:02.320088Z",
     "start_time": "2023-04-10T16:41:02.154747Z"
    },
    "source_hash": "59add435",
    "execution_start": 1681747914503,
    "execution_millis": 94755011,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "cell_id": "b135465acc7246b49caafc5622dd7371",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mln = MultiLayerNetwork(input_size=784, hidden_size=256, output_size=10, loss_function=cross_entropy, loss_function_grad=cross_entropy_grad)\n",
    "mln.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=0.01, epochs=11, batch_size=46)\n",
    "mln.summary()"
   ],
   "metadata": {
    "cell_id": "fd6bb1d5f98f4931847d9033b012b992",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:43:57.487217Z",
     "start_time": "2023-04-10T16:41:02.154802Z"
    },
    "source_hash": "cfb45cd8",
    "execution_start": 1681748527970,
    "execution_millis": 94141545,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "cell_id": "43bf4b5ec5a24c5281b45f9fcc1fdd4a",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluate the network on the test set\n",
    "print(f\"Accuracy: {mln.evaluate(X_test, y_test_one_hot):.4f}\")"
   ],
   "metadata": {
    "cell_id": "da164287a917425195a47fab05b3d937",
    "ExecuteTime": {
     "end_time": "2023-04-10T16:43:57.716061Z",
     "start_time": "2023-04-10T16:43:57.492708Z"
    },
    "source_hash": "ce22d680",
    "execution_start": 1681210137270,
    "execution_millis": 632532246,
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the right hyperparameters"
   ],
   "metadata": {
    "cell_id": "af5b252da9b54ef0adecce9b028a58da",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = [10*i for i in range(1, 5)]\n",
    "batch_sizes = [2**i for i in range(4, 11)]\n",
    "learning_rates = [10**-i for i in range(1, 8)]\n",
    "hidden_sizes = [32, 64, 128, 256, 512]\n",
    "combinations = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n",
    "\n",
    "print(f\"Testing {len(combinations)} parameter combinations\")\n",
    "\n",
    "results = []\n",
    "for i in range(len(combinations)):\n",
    "    e, b, l, h = combinations[i]\n",
    "    net = MultiLayerNetwork(input_size=784, hidden_size=h, output_size=10, loss_function=cross_entropy, loss_function_grad=cross_entropy_grad)\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=l, epochs=e, batch_size=b, output=False)\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    accuracy = net.evaluate(X_test, y_test_one_hot)\n",
    "\n",
    "    # save the results\n",
    "    result = (e, b, l, h, accuracy, total_time)\n",
    "    results.append(result)\n",
    "\n",
    "    print(f\"combo {i+1}/{len(combinations)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {accuracy:.4f} - Time: {total_time:.4f}s\")"
   ],
   "metadata": {
    "cell_id": "97ae5e0e0edb490985ac2339f2051b2a",
    "collapsed": false,
    "source_hash": "63205ad0",
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lerntagebuch"
   ],
   "metadata": {
    "cell_id": "6f4bb2f6c7d44f28b7c4cafa8edf45e2",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KW13"
   ],
   "metadata": {
    "cell_id": "a8873d9795e34014a8c18fcb5491bf7e",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "cell_id": "ab8f61905146452e8a8ccde715c1db6b",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KW14"
   ],
   "metadata": {
    "cell_id": "35529779acc54d9bbcba6142c92b93b7",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Multiplikation Linear Layer vertauscht? input x weights or weights x input"
   ],
   "metadata": {
    "cell_id": "e45befd004fb479baf6b2c6b8a482e55",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-bullet"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KW15"
   ],
   "metadata": {
    "cell_id": "77dc66ff97134ec181f91241dcd9dc4b",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "cell_id": "85385ecd5049472a9868e53e9b9b34a7",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KW16"
   ],
   "metadata": {
    "cell_id": "4a96cca2534842eda17a169e636aa5e4",
    "formattedRanges": [],
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 17.04.2023\n",
    "SingleLayerModel:\n",
    "\n",
    "Loss-Function Decision: We found that there are a bunch of different loss functions that are used in the context of neural networks. We wanted to take a deeper look into how each of those functions would perform in the case of our Single-Layer-Network and Multi-Layer-Network.\n",
    "\n",
    "Focal: \n",
    "\n",
    "Hinge:\n",
    "\n",
    "Final: Binary-Cross-Entropy.\n",
    "\n",
    "Explanation: \n",
    "\n",
    "---\n",
    "Geeignete Loss- und Accuracy-Funktionen\n",
    "- Die Wahl wurde begründet und mit anderen mögliche Funktionen verglichen?\n",
    "- Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angege- ben (gerendert in Latex)?\n",
    "- Die Entwicklung der Loss- und Accuracy-Funktionen wurden auf Trainings- und Testdaten- sätzen korrekt verfolgt und leicht nachvollziehbar dargestellt?\n",
    "- Die Wahl von Lernrate und Hidden Layer-Größe wurde nachvollziehbar entschieden und begründet.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "For the Muli-class classification problem:\n",
    "\n",
    "**Loss Function Choice**: \n",
    "Kullback: Complicated integration due to requirements\n",
    "MSE: Rather weak, due to the focus on regression\n",
    "\n",
    "Cross-Entropy is ideal for Tasks 4 and 5 because it handles classification problems with probability predictions well. Other loss functions, such as Mean Squared Error, are less suitable because they are more focused on regression and have no direct connection to the classification of probabilities. Therefore, Cross-Entropy is the best choice.\n",
    "\n",
    "**Accuracy Functions**:\n",
    "Accuracy: Correct / False. Basic and sufficient.\n",
    "F1-Score: Perhaps better for exercise 3 to counteract imbalance in the dataset (90% accuracy due to few 4s != good) but not necessary for exercise 5.\n",
    "\n",
    "Confusion Matrix: Also probably more helpful for exercise 3. \n",
    "\n",
    "**Activation Functions**:\n",
    "\n",
    "ReLU: The best practice in the current age. Very efficient and doesn't share the problems that sigmoid has (vanishing gradients leading to slower learning and/or convergence)\n",
    "\n",
    "Leaky ReLU: ReLU but with a better focus on making sure that dead neurons (neurons that are updated, so that they can never be activated again) are taken care of.\n",
    "\n",
    "Parametric ReLU:\n",
    "PReLU is another variation of ReLU that generalizes Leaky ReLU by making the negative slope a learnable parameter. Higher risk of overfitting.\n",
    "\n",
    "Sigmoid: Is just bad now, no reason to ever consider it except for an educational purpose. The main problem lies in vanishing gradients leading to slower learning and/or convergence: The vanishing gradient problem is a difficulty encountered during the training of deep neural networks using gradient-based optimization methods, such as gradient descent and its variants. This issue arises when the gradients of the loss function with respect to the network's parameters become very small as they propagate through the layers, leading to slow or stalled learning. This can happen at any stage in the training process, which makes it more difficult to counteract it properly.\n",
    "\n",
    "---\n",
    "\n",
    "Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n",
    "- Die Wahl wurde begründet und mit anderen möglichen Funktionen verglichen.\n",
    "- Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angegegeben (gerendert in Latex).\n",
    "- Es wurden verschiedene Lernraten und Größen der Hidden Layer sinnvoll ausprobiert.\n",
    "- Die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen wurde korrekt verfolgt und leicht nachvollziehbar dargestellt?\n",
    "- Die Wahl der Hyperparameter wurde nachvollziehbar entschieden und begründet?\n",
    "\n",
    "Form:\n",
    "\n",
    "Environment angeben (Python 3.10)\n",
    "- Code comments\n",
    "- Executive Summary\n",
    "\n"
   ],
   "metadata": {
    "cell_id": "737a81d68f5a46379dc06c2cdef09313",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 18.04.2023\n",
    "\n",
    "Fragen an Stefan:\n",
    "- Verständnis Anzahl Layers\n",
    "- Aufgabe 3: Biased Training? Interpretation von unseren Resultaten?\n",
    "- Keine Gradienten Berechnung im LinearLayer (Punkt 5)\n",
    "- Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n",
    "- Lerntagebuch auch im Notebook? Summary on top?\n",
    "- Time import bei Aufgabe 4 erlaubt? (Zeitmessung oder Epochen gemeint)"
   ],
   "metadata": {
    "cell_id": "ec3e1c4a25a749e6b5055ec290c80056",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=88bc6171-47da-4cbc-96f1-ee851c7ac9ec' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ],
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote": {},
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbconvert_exporter": "python"
  },
  "orig_nbformat": 4,
  "deepnote_notebook_id": "edfe21a714fc4bb2be9a11d016a72375",
  "deepnote_execution_queue": []
 }
}
