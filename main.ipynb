{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.027242Z","start_time":"2023-04-10T16:34:59.025205Z"},"cell_id":"3027984785e644f893749d93667f61a7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1852,"execution_start":1682257935033,"source_hash":"1d332d42"},"outputs":[],"source":["import torchvision.datasets as datasets\n","import torchvision.transforms as transforms # to convert to numpy array\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"cell_id":"68a7e30274d34bf69b4a2970cbce8ad5","deepnote_cell_type":"markdown"},"source":["# vta: Mini-Challenge Gradient Descent\n","\n","The goal of this assignment is for you to gain a basic understanding of numerical approximation methods in higher dimensions, specifically Gradient Descent and its practical applications. For this, you are to create a Jupyter Notebook and load and explore the MNIST dataset. Then you are to create and train a neural network to correctly classify the images. Only the specified Python packages may be used.\n","\n","The goal of this assignment is not only to demonstrate your mathematical knowledge, but also to communicate and present your results appropriately. Thus, your submissions should not only be mathematically correct, but also easy to understand and reproduce. For more specifics on what is expected of the submission, please refer to the Evaluation Criteria. Document your work progress and knowledge gain in the form of a learning diary to record learning progress, difficulties and insights.\n","The following tasks specify the individual processing steps and provide the structure of the notebook.\n","\n","*Translated with [DeepL](https://www.DeepL.com/Translator)*"]},{"cell_type":"markdown","metadata":{"cell_id":"aa738f2bb0de49009cb8a0b677c4057f","deepnote_cell_type":"markdown"},"source":["## Activation functions\n","The activation function is a critical component of neural networks that helps introduce **non-linearity** to the model. It plays a crucial role in determining the output of a neural network for a given input. With the advent of deep learning, researchers and practitioners have explored a wide range of activation functions to improve the performance of neural networks.\n","\n","In this chapter, we will provide an overview of the most commonly used activation functions and compare their strengths and weaknesses. We will delve into the different use cases for each activation function and how they impact the training of neural networks.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"86d2eb2c8de74c8e8e478b3cc10998c9","deepnote_cell_type":"markdown"},"source":["### ReLU\n","\n","#### Advantages of ReLU:\n","1. **Non-linearity**: ReLU introduces non-linearity, allowing the model to learn complex patterns.\n","$$\n","f(x) = max(0, x)\n","$$\n","\n","2. **Computational efficiency**: ReLU is computationally efficient due to its simple form.\n","\n","3. **Sparse activation**: ReLU promotes sparsity, reducing model complexity.\n","Why? $f(x) = 0$ for $x < 0$, encouraging fewer active neurons.\n","\n","#### Disadvantages of ReLU:\n","1. **Dying ReLU**: Neurons can \"die\" if inputs are consistently negative.\n","Why? $f(x) = 0$ for $x < 0$, leading to no gradient updates.\n","\n","2. **Non-differentiable at 0**: ReLU is not differentiable at $x=0$.\n","Why? Discontinuity at $x=0$.\n","\n","3. **Non-zero centered**: ReLU outputs are not zero-centered, causing optimization issues.\n","Why? Biased gradients can hinder learning.\n","\n","**Sources**:\n","* [Top 10 Activation Function's Advantages & Disadvantages](https://www.linkedin.com/pulse/top-10-activation-functions-advantages-disadvantages-dash/)\n","\n","* [A Gentle Introduction to the Rectified Linear Unit (ReLU) - MachineLearningMastery.com](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\n","\n","* [Activation Functions and Loss Functions for neural networks — How to pick the right one? | by Indraneel Dutta Baruah | Analytics Vidhya | Medium](https://medium.com/analytics-vidhya/activation-functions-and-loss-functions-for-neural-networks-how-to-pick-the-right-one-542e1dd523e0)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532332Z","start_time":"2023-04-10T16:34:59.513311Z"},"cell_id":"d5dee252b3c94822af6f94215d2a780d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":32,"execution_start":1682257936885,"source_hash":"a03db65f"},"outputs":[],"source":["def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_grad(x):\n","    return x > 0"]},{"cell_type":"markdown","metadata":{"cell_id":"64d4eab6e3794e92aa2b1bdd702637d1","deepnote_cell_type":"markdown"},"source":["### Sigmoid\n","\n","#### Advantages of Sigmoid:\n","1. **Smooth and differentiable**: Sigmoid is a smooth, differentiable function.\n","$$\n","f(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","2. **Range**: Sigmoid maps input to $(0, 1)$, providing normalized outputs.\n","Why? Useful for probabilistic interpretations and binary classification.\n","\n","3. **Non-linearity**: Sigmoid introduces non-linearity, allowing complex pattern learning.\n","\n","#### Disadvantages of Sigmoid:\n","1. **Vanishing gradient**: Sigmoid suffers from vanishing gradient problem.\n","Why? Gradients can be small ($f'(x) \\approx 0$) for large $|x|$, slowing learning.\n","\n","2. **Non-zero centered**: Sigmoid outputs are not zero-centered, causing optimization issues.\n","Why? Biased gradients can hinder learning.\n","\n","3. **Computational complexity**: Sigmoid is computationally more complex than ReLU.\n","Why? Exponential calculation in the function.\n","\n","**Sources**:\n","* [Sigmoid Activation Function](https://insideaiml.com/blog/Sigmoid-Activation-Function-1031)\n","\n","* [Top 10 Activation Function's Advantages & Disadvantages](https://www.linkedin.com/pulse/top-10-activation-functions-advantages-disadvantages-dash/)\n","\n","* [Sigmoid Function – LearnDataSci](https://www.learndatasci.com/glossary/sigmoid-function/#:~:text=A%20sigmoid%20function%20is%20a,number%20between%200%20and%201)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"aefc36760dbc4824b85bdd02ce327b64","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1682257936914,"source_hash":"30011070"},"outputs":[],"source":["def sigmoid(x):\n","    x = np.clip(x, -500, 500) # clip x to prevent overflow\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_grad(x):\n","    return sigmoid(x) * (1 - sigmoid(x))"]},{"cell_type":"markdown","metadata":{"cell_id":"e6df633608734927afc3b555769245b9","deepnote_cell_type":"markdown"},"source":["### Softmax\n","\n","The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. Softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\n","\n","#### Advantages of Softmax activation function:\n","1. **Probabilistic interpretation**: Softmax produces a probability distribution over the output classes. The sum of all the probabilities is 1 (100%).\n","\n","$$\n","\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n","$$\n","\n","Where $z_i$ is the input to the Softmax function and $K$ is the number of classes.\n","\n","2. **Differentiable**: Softmax is differentiable, which is essential for gradient-based optimization methods like gradient descent.\n","\n","3. **Suitable for multi-class classification**: Softmax is well-suited for multi-class classification problems as it produces a probability distribution over all classes.\n","\n","#### Disadvantages of Softmax activation function:\n","1. **Inappropriate for binary classification**: While Softmax can be used for binary classification, it is computationally inefficient compared to using a sigmoid activation function.\n","\n","2. **Susceptible to vanishing gradients**: Softmax may result in vanishing gradients when used with certain loss functions (e.g., mean squared error) in deep networks, slowing down learning.\n","\n","3. **Numerical instability**: Softmax can lead to numerical instability due to exponentiation of large values. This can be mitigated using the log-sum-exp trick:\n","$$ \n","\\sigma(z)_i = \\frac{e^{z_i - c}}{\\sum_{j=1}^K e^{z_j - c}}\n","$$\n","Where $c = \\max_i z_i$.\n","\n","**Source**:\n","* [Softmax Activation Function with Python - MachineLearningMastery.com](https://machinelearningmastery.com/softmax-activation-function-with-python/)\n","\n","* [How to implement the derivative of Softmax independently from any loss function | by Ms Aerin | IntuitionMath | Medium](https://medium.com/intuitionmath/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4593e1f7c8c04db7a6f45556fa40439e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1682257936914,"source_hash":"d7f9a7f0"},"outputs":[],"source":["def softmax(x):\n","    x = x - np.max(x, axis=1, keepdims=True) # prevent overflow\n","    x = np.exp(x)\n","    x = x / np.sum(x, axis=1, keepdims=True)\n","    return x\n","\n","def softmax_grad(x):\n","    return softmax(x) * (1 - softmax(x))"]},{"cell_type":"markdown","metadata":{"cell_id":"eb5f20d70a3e4b74b7cb87e680477d75","deepnote_cell_type":"markdown"},"source":["## Loss functions\n","\n","The loss function is a key component of neural network training that plays a crucial role in optimizing the model's performance. It is used to measure the difference between the predicted and actual values of the output, and the objective of training is to minimize this difference.\n","\n","In this chapter, we will provide an overview of the most commonly used loss functions for both binary- and multi-classification tasks and compare their strengths and weaknesses. We will delve into the different use cases for each loss function and how they impact the training of neural networks."]},{"cell_type":"markdown","metadata":{"cell_id":"591ffdc5f1794c4b8322cc580d448fd7","deepnote_cell_type":"markdown"},"source":["### Mean Squared Error\n","\n","The Mean Squared Error (MSE) loss function measures the average squared difference between the true values and the predicted values. It is commonly used in regression tasks to quantify the discrepancy between the model's predictions and the ground truth.\n","\n","$$\n","L(y, \\hat{y}) = \\frac{1}{N}(y - \\hat{y})^2 \n","$$\n","\n","\n","**Advantages of MSE loss**:\n","1. Continuity and differentiability: MSE is a continuous and differentiable function.\n","\n","2. Simple to compute: MSE is easy to compute and differentiate.\n","\n","3. Intuitive interpretation: MSE measures the average squared error between predictions and true values.\n","\n","4. Commonly used: MSE is a widely-used loss function for regression tasks.\n","\n","**Disadvantages of MSE loss**:\n","1. Suboptimal for classification: MSE is more suited for regression tasks.\n","Why? It doesn't directly optimize for class probabilities or decision boundaries.\n","\n","2. Sensitivity to outliers: MSE is sensitive to outliers, skewing predictions.\n","Why? Squaring errors exaggerates the impact of extreme values.\n","\n","3. Inconsistent gradient magnitude: MSE gradients depend on the error magnitude.\n","Why? Large errors lead to larger gradients, potentially causing overshooting.\n","\n","4. Non-probabilistic interpretation: MSE doesn't provide probability calibration.\n","Why? It doesn't focus on optimizing probabilities, which are useful for thresholding and classification tasks.\n","\n","Sources:\n","* [Mean squared error - Wikiwand](https://www.wikiwand.com/en/Mean_squared_error)\n","\n","* [Mean Squared Error: Definition and Example - Statistics How To](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"edeae3db38bc41c2aee1ee9013ae11b9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1682257936914,"source_hash":"182b28c1"},"outputs":[],"source":["def mse(y_pred, y_true):\n","    batch_size = y_pred.shape[0]\n","    loss = np.sum((y_pred - y_true)**2) / batch_size\n","    return loss\n","\n","def mse_grad(y_pred, y_true):\n","    batch_size = y_pred.shape[0]\n","    grad = 2 * (y_pred - y_true) / batch_size\n","    return grad"]},{"cell_type":"markdown","metadata":{"cell_id":"d8cbcbd3146e4290ade00d0dbbd08af7","deepnote_cell_type":"markdown"},"source":["### Binary Cross-Entropy\n","**Advantages of BCE loss**:\n","1. Probabilistic interpretation: BCE measures the error between true binary labels and predicted probabilities, optimizing for class probabilities.\n","$$\n","L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n","$$\n","\n","2. Suitable for classification: BCE is designed for binary classification tasks, directly optimizing decision boundaries.\n","\n","3. Robust to outliers: BCE is less sensitive to outliers compared to MSE, as it doesn't square errors.\n","\n","4. Stable gradients: BCE provides stable and informative gradient signals for classification tasks, facilitating learning.\n","\n","**Disadvantages of BCE loss**:\n","1. Not suitable for multi-class problems: BCE is designed for binary classification and requires modification (e.g., categorical cross-entropy) for multi-class problems.\n","\n","2. Requires probability input: BCE loss expects predicted probabilities as input, requiring an activation function (e.g., sigmoid) to convert raw model outputs.\n","\n","3. Logarithm computation: BCE involves logarithm computations, which can be computationally more expensive than simple arithmetic operations (e.g., in MSE).\n","\n","The gradient:\n","\n","$$\n","\\begin{aligned}\n","\\frac{\\partial L(y)}{\\partial y} & =\\frac{\\partial(-t \\log (y)-(1-t) \\log (1-y))}{\\partial y}=\\frac{\\partial(-t \\log (y))}{\\partial y}+\\frac{\\partial(-(1-t) \\log (1-y))}{\\partial y} \\\\\n","& =-\\frac{t}{y}+\\frac{1-t}{1-y}=\\frac{y-t}{y(1-y)}\n","\\end{aligned}\n","$$\n","\n","Sources:\n","- [Understanding binary cross-entropy / log loss: a visual explanation | by Daniel Godoy | Towards Data Science](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n","\n","- [Where did the Binary Cross-Entropy Loss Function come from? | by Rafay Khan | Towards Data Science](https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715)\n","\n","- [r - Gradient for logistic loss function - Cross Validated](https://stats.stackexchange.com/questions/219241/gradient-for-logistic-loss-function)\n","\n","- [Logistic classification with cross-entropy (1/2)](https://peterroelants.github.io/posts/cross-entropy-logistic/)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532406Z","start_time":"2023-04-10T16:34:59.517129Z"},"cell_id":"756717937c8241e284a66c887dc46f7e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1682257936916,"source_hash":"722f55b5"},"outputs":[],"source":["def binary_cross_entropy(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n","    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n","\n","def binary_cross_entropy_grad(y_pred, y_true):\n","    eps = 1e-15\n","    # y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n","    return (y_pred - y_true) / (y_pred * (1 - y_pred) + eps) # add eps to prevent division by 0"]},{"cell_type":"markdown","metadata":{"cell_id":"8eb7da7bc57a453bb1d688b940e1ab1a","deepnote_cell_type":"markdown"},"source":["### Focal Loss\n","$$\n","FL(p_t)=-\\alpha(1-p_t)^\\gamma \\cdot log(p_t)\n","$$\n","\n","Focal Loss is an extension of Cross-Entropy loss, primarily designed to address class imbalance issues in classification tasks. It was introduced by Lin et al. in the paper \"Focal Loss for Dense Object Detection.\"\n","\n","**Advantages of Focal Loss**:\n","\n","1. Handles class imbalance: Focal Loss is designed to handle class imbalance by down-weighting the contribution of easy-to-classify examples and focusing on hard-to-classify examples.\n","$$L(y, \\hat{y}) = -(1 - \\hat{y})^\\gamma y \\log(\\hat{y}) - \\hat{y}^\\gamma (1 - y) \\log(1 - \\hat{y})$$\n","Where $\\gamma$ is the focusing parameter.\n","\n","1. Adjustable focusing parameter: The focusing parameter $\\gamma$ controls the degree of down-weighting for easy examples, allowing users to fine-tune the balance between classes.\n","\n","2. Compatible with multi-class tasks: Focal Loss can be easily extended to multi-class tasks by adapting the categorical cross-entropy loss.\n","\n","3. Improved performance: Focal Loss can improve the performance of classification models, especially when class imbalance is present, by helping the model focus on difficult examples.\n","\n","**Disadvantages of Focal Loss**:\n","\n","1. Complexity: Focal Loss is more complex compared to traditional cross-entropy loss due to the additional focusing parameter and the computation of the modulating factor.\n","\n","2. Hyperparameter tuning: The focusing parameter $\\gamma$ may require hyperparameter tuning to find the best value for a specific task, adding complexity to the training process.\n","\n","3. Limited benefits in balanced datasets: The main advantage of Focal Loss is its ability to handle class imbalance. In balanced datasets, it might not provide significant benefits over traditional cross-entropy loss.\n","\n","In summary, Focal Loss is advantageous when dealing with class imbalance, as it helps the model focus on hard-to-classify examples. However, it introduces additional complexity and requires hyperparameter tuning for the focusing parameter, which might not be beneficial for balanced datasets.\n","\n","Sources:\n","* [Focal Loss: A better alternative for Cross-Entropy | by Roshan Nayak | Towards Data Science](https://towardsdatascience.com/focal-loss-a-better-alternative-for-cross-entropy-1d073d92d075)\n","\n","* [Understanding Focal Loss in 5 mins | Medium | VisionWizard](https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7)\n","\n","* [Focal loss implementation for LightGBM • Max Halford](https://maxhalford.github.io/blog/lightgbm-focal-loss/#formulas-for-focal-loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d0c0433162fd47df979538dc4a76d795","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1682257936921,"source_hash":"a0697eab"},"outputs":[],"source":["alpha, gamma = .9, 2\n","\n","def at(y):\n","    if alpha is None:\n","        return np.ones_like(y)\n","    return np.where(y, alpha, 1 - alpha)\n","\n","def pt(y, p):\n","    p = np.clip(p, 1e-15, 1 - 1e-15)\n","    return np.where(y, p, 1 - p)\n","\n","def focal_loss(y_pred, y_true):\n","    atv = at(y_true)\n","    ptv = pt(y_true, y_pred)\n","    return -atv * (1 - ptv) * gamma * np.log(ptv)\n","\n","def focal_loss_grad(y_pred, y_true):\n","    y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n","    atv = at(y_true)\n","    ptv = pt(y_true, y_pred)\n","    g = gamma\n","    return atv * y * (1 - ptv) ** g * (g * ptv * np.log(ptv) + ptv - 1)"]},{"cell_type":"markdown","metadata":{"cell_id":"8f24eac136534b9d8f1367b91ee84cb9","deepnote_cell_type":"markdown"},"source":["### Cross-Entropy\n","$$\n"," L =-\\frac{1}{N}\\left(\\sum_{i=1}^N \\mathbf{y}_{\\mathbf{i}} \\cdot \\log \\left(\\hat{\\mathbf{y}}_{\\mathbf{i}}\\right)\\right)\n","$$\n","\n","Cross-entropy is the default loss function to use for multi-class classification problems. \n","\n","Cross entropy computes a score that summarizes the average difference between the actual and predicted probability distributions for all classes of the problem. The score is minimized and a perfect cross entropy value is 0.\n","\n","The ground truth $y$ gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from the estimate $\\hat{\\mathbf{y}}$. For example:\n","\n","$$\n","\\begin{aligned}\n","& L =-(1 \\times \\log (0.1)+0 \\times \\log (0.5)+\\ldots) \\\\\n","& L =-\\log (0.1) \\approx 2.303\n","\\end{aligned}\n","$$\n","\n","The main feature of this loss function is that only the probabilities of the correct classes are rewarded/punished. The cross-entropy loss function is often averaged over the batch size, so that the loss function is independent of the batch size.\n","\n","**Advantages of cross-entropy loss function**:\n","1. Probabilistic interpretation: Cross-entropy loss measures the difference between two probability distributions, making it suitable for classification tasks where the output is a probability distribution.\n","\n","2. Faster convergence: Cross-entropy loss generally converges faster than other loss functions (e.g., mean squared error) when used with logarithmic activation functions like sigmoid or softmax, as it avoids the vanishing gradient problem.\n","\n","3. Differentiable: Cross-entropy loss is differentiable, which is crucial for gradient-based optimization methods like gradient descent.\n","\n","$$\n","H(y, \\hat{y}) = -\\sum_{i=1}^N y_i \\log(\\hat{y}_i)\n","$$\n","\n","**Disadvantages of cross-entropy loss function**:\n","1. Inapplicable to non-probabilistic tasks: Cross-entropy loss is not suitable for regression tasks, where the output is a continuous value rather than a probability distribution.\n","\n","2. Numerical instability: Cross-entropy loss involves logarithmic operations, which can lead to numerical instability when predicted probabilities are very close to 0 or 1. This issue can be mitigated by clipping the predicted probabilities to a small range (e.g., [1e-8, 1-1e-8]).\n","\n","3. Requires normalized outputs: Cross-entropy loss assumes that the model's output is a probability distribution. This requires the use of appropriate activation functions, such as softmax for multi-class classification or sigmoid for binary classification, in the output layer.\n","\n","When using the cross-entropy loss function, the gradient of the loss function is given by:\n","\n","$$\n","\\frac{\\partial L}{\\partial z_{j}} = \\hat{\\mathbf{y}}_i-\\mathbf{y}_i\n","$$\n","\n","This partial derivative take advantage of the fact that the Softmax activation function is a normalized exponential function. Therefore, the gradient of the cross-entropy loss function is simplified to the difference between the predicted and the actual class probabilities.\n","\n","Sources:\n","* [Derivative of Softmax and the Softmax Cross Entropy Loss | David Bieber](https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/)\n","\n","* [Killer Combo: Softmax and Cross Entropy | by Paolo Perrotta | Level Up Coding](https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba)\n","\n","* [A Gentle Introduction to Cross-Entropy for Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"64c9e083e43242509f13319090600730","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1682257936923,"source_hash":"4a6ffaf5"},"outputs":[],"source":["def cross_entropy(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent log(0) or log(1)\n","    loss = -(y_true * np.log(y_pred)).sum(axis=1).mean()\n","    return loss\n","\n","def cross_entropy_grad(y_pred, y_true):\n","    eps = 1e-15\n","    y_pred = np.clip(y_pred, eps, 1-eps) # clip y_pred to prevent division by 0\n","    grad = y_pred - y_true\n","    return grad"]},{"cell_type":"markdown","metadata":{"cell_id":"f756808663864e62ab26092f142c8076","deepnote_cell_type":"markdown"},"source":["### Kullback–Leibler divergence\n","Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n","\n","A KL divergence loss of 0 indicates that the distributions are identical. In practice, the behavior of KL divergence is very similar to cross entropy. It calculates how much information is lost when the predicted probability distribution is used to approximate the desired target probability distribution.\n","\n","**Advantages of KL divergence loss function**:\n","1. Probabilistic interpretation: KL divergence measures the difference between two probability distributions, making it suitable for tasks where the output is a probability distribution.\n","\n","2. Asymmetric: KL divergence is asymmetric, making it sensitive to the ordering of the true and predicted distributions. This can be advantageous when the direction of the divergence matters.\n","\n","$$\n","D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n","$$\n","\n","3. Differentiable: KL divergence is differentiable, which is important for gradient-based optimization methods like gradient descent.\n","\n","**Disadvantages of KL divergence loss function**:\n","1. Non-negative: KL divergence is non-negative, which may not be ideal for certain optimization algorithms that rely on negative values to indicate convergence.\n","\n","2. Inapplicable to non-probabilistic tasks: KL divergence is not suitable for regression tasks, where the output is a continuous value rather than a probability distribution.\n","\n","3. Not symmetric: KL divergence is not symmetric, meaning that $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$. In some applications, a symmetric measure of divergence might be more appropriate.\n","\n","4. Requires normalized outputs: KL divergence assumes that the model's output is a probability distribution. This requires the use of appropriate activation functions, such as softmax for multi-class classification or sigmoid for binary classification, in the output layer.\n","\n","5. Numerical instability: KL divergence involves logarithmic operations, which can lead to numerical instability when predicted probabilities are very close to 0. This issue can be mitigated by adding a small constant to the predicted probabilities or using a modified version of the KL divergence that avoids division by zero.\n","\n","Sources:\n","* [Kullback–Leibler divergence - Wikiwand](https://www.wikiwand.com/en/Kullback%E2%80%93Leibler_divergence)\n","\n","* [Understanding KL Divergence. A guide to the math, intuition, and… | by Aparna Dhinakaran | Towards Data Science](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254)"]},{"cell_type":"markdown","metadata":{"cell_id":"556e93bce2cb44398e3cee9ea60a1313","deepnote_cell_type":"markdown"},"source":["## Task 1 — Loading and inspecting the data\n","> Load the MNIST dataset using the torchvision package (Use the torchvision package for this task) and use matplotlib to get an overview of the data. Describe the basic properties of the dataset, such as how much and what data it contains. *Translated with [DeepL](https://www.deepl.com/translator)*\n","\n","This code performs several data preprocessing steps for the MNIST dataset. \n","\n","First, it defines a set of transformations to be applied to the data using the `transforms.Compose` function. The `ToTensor()` function converts the input data from a numpy array to a PyTorch tensor, and the `Normalize()` function normalizes the input tensor with a mean of 0.5 and a standard deviation of 0.5.\n","\n","Then, it loads the MNIST dataset using the `datasets.MNIST` function and applies the defined transformations to the data. It separates the training and testing data and labels into separate arrays `X_train`, `y_train`, `X_test`, and `y_test`.\n","\n","Next, it reshapes the data arrays `X_train` and `X_test` from 2D arrays of shape [N, 28, 28] to 1D arrays of shape [N, 784]. This step flattens each image into a 1D vector of 784 values.\n","\n","Finally, it normalizes the data arrays `X_train` and `X_test` to be between 0 and 1 by dividing each pixel value by 255.0, which is the maximum pixel value.\n","\n","The final line of code prints the shape of the data and label arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.168041Z","start_time":"2023-04-10T16:34:59.028883Z"},"cell_id":"0ce9731f1fc44736a5715924ae93ce29","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":183,"execution_start":1682257936928,"source_hash":"49fb5bd5"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Load the MNIST dataset\n","train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Extract the data and labels from the datasets\n","X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n","X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n","\n","# Reshape the data to be of size [N x 784]\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","# Normalize the data to be between 0 and 1\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.531688Z","start_time":"2023-04-10T16:34:59.168488Z"},"cell_id":"7ab6732e8ca743b688b87ac41eed7f16","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":474,"execution_start":1682257937110,"source_hash":"9230fd10"},"outputs":[],"source":["classes = np.unique(y_train)\n","\n","# Plot the images\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(X_train[y_train == classes[i]][0].reshape(28, 28), cmap='gray')\n","    ax.set_title(f\"Class: {classes[i]}\")\n","    ax.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.531965Z","start_time":"2023-04-10T16:34:59.484990Z"},"cell_id":"237346e68365445ea8fa331432631650","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":35,"execution_start":1682257937574,"source_hash":"d82631cb"},"outputs":[],"source":["print('Number of training examples: ', X_train.shape[0])\n","print('Number of testing examples: ', X_test.shape[0])\n","print('Each image is of size: ', X_train.shape[1])\n","print('There are {} classes: {}'.format(len(classes), classes))\n","print('The data is of type: ', X_train.dtype)\n","print('The labels are of type: ', y_train.dtype)\n","print('The range of the pixel values is [{}, {}]'.format(np.min(X_train), np.max(X_train)))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"920f76d52c7c47bfb422b9258382d782","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1682257937607,"source_hash":"73db116a"},"outputs":[],"source":["for i in range(len(classes)):\n","    print(f\"Class {classes[i]}: {np.sum(y_train == classes[i])} train examples, {np.sum(y_test == classes[i])} test examples\")"]},{"cell_type":"markdown","metadata":{"cell_id":"33ae04264eeb4b3b95a1fde939712cc6","deepnote_cell_type":"markdown"},"source":["## Task 2 — Implementing a linear layer\n","\n","> Create a class for a linear layer with any number of nodes. Implement the forward, backward, and update methods using numpy. Write appropriate unit tests to verify that the function works. *Translated with [DeepL](https://www.deepl.com/translator)*"]},{"cell_type":"markdown","metadata":{"cell_id":"31b776ea5cac4cadaf4a723f1eba5807","deepnote_cell_type":"markdown"},"source":["### Linear Layer\n","\n","This class represents a linear layer in a neural network. The `__init__` function initializes the layer's parameters, including the input size, output size, activation function, and its derivative. It also initializes the weights and biases randomly with small values.\n","\n","The `forward` function computes the output of the layer by taking the dot product of the input with the layer's weights and adding the bias. If the activation function is not `None`, it applies it to the linear output and returns the result.\n","\n","The `backward` function computes the gradient of the loss with respect to the layer's weights, biases, and inputs. If an activation function exists, it computes the gradient of the loss with respect to the output before applying the activation function. Then it computes the gradient of the loss with respect to the input by taking the dot product of the gradient of the loss with respect to the output and the transpose of the layer's weights.\n","\n","The `update` function updates the weights and biases using gradient descent by subtracting the product of the learning rate and the gradients from the current values.\n","\n","Overall, this class implements a fully connected layer with an optional activation function that can be used in a neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532012Z","start_time":"2023-04-10T16:34:59.502315Z"},"cell_id":"38dd450dc7ce4c7292bb84884b25b524","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1682257937609,"source_hash":"4e425a9e"},"outputs":[],"source":["import numpy as np\n","\n","class LinearLayer:\n","    def __init__(self, input_size, output_size, activation_fn=None, activation_fn_grad=None):\n","        \"\"\"\n","        Initialize LinearLayer with given input_size, output_size, and activation functions.\n","\n","        :param input_size: number of input neurons\n","        :param output_size: number of output neurons\n","        :param activation_fn: activation function (default: None)\n","        :param activation_fn_grad: gradient of the activation function (default: None)\n","        \"\"\"\n","        self.x = None  # input to the layer\n","        self.bias_grad = None  # gradient of the bias\n","        self.weights_grad = None  # gradient of the weights\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.activation_fn = activation_fn\n","        self.activation_fn_grad = activation_fn_grad\n","        self.weights = np.random.randn(input_size, output_size) * 0.01  # initialize weights with small random values\n","        self.bias = np.zeros((1, output_size))  # initialize bias with zeros\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Perform forward pass of the layer.\n","\n","        :param x: input to the layer\n","        :return: output of the layer\n","        \"\"\"\n","        self.x = x\n","        linear_output = np.dot(x, self.weights) + self.bias  # compute linear output of the layer\n","        \n","        if self.activation_fn is None:\n","            return linear_output\n","        \n","        return self.activation_fn(linear_output)  # apply activation function to the linear output\n","\n","    def backward(self, grad_output, hidden_output=None):\n","        \"\"\"\n","        Perform backward pass of the layer.\n","\n","        :param grad_output: gradient of the output of the layer\n","        :param hidden_output: output of the layer before activation (default: None)\n","        :return: gradient of the input to the layer\n","        \"\"\"\n","        if self.activation_fn_grad is not None:\n","            if hidden_output is not None:\n","                grad_output = self.activation_fn_grad(hidden_output) * grad_output  # apply chain rule\n","            else:\n","                grad_output = self.activation_fn_grad(grad_output) * grad_output  # apply chain rule\n","        \n","        self.weights_grad = np.dot(self.x.T, grad_output)  # compute gradient of the weights\n","        self.bias_grad = np.sum(grad_output, axis=0, keepdims=True)  # compute gradient of the bias\n","        return np.dot(grad_output, self.weights.T)  # compute gradient of the input to the layer\n","\n","    def update(self, lr):\n","        \"\"\"\n","        Update the weights and bias of the layer using gradient descent.\n","\n","        :param lr: learning rate\n","        \"\"\"\n","        self.weights -= lr * self.weights_grad  # update weights\n","        self.bias -= lr * self.bias_grad  # update bias"]},{"cell_type":"markdown","metadata":{"cell_id":"a1794f2b8a164a81a6837513eb94958d","deepnote_cell_type":"markdown"},"source":["### Unit Testing\n","\n","This code defines a unit test for the `LinearLayer` class using the `unittest` module. The test class inherits from `unittest.TestCase`, and it contains several test functions:\n","\n","- `setUp` function initializes the parameters of the `LinearLayer` class for testing purposes.\n","- `test_forward` function tests if the `forward` function of the `LinearLayer` class returns an output with the correct shape.\n","- `test_backward` function tests if the `backward` function of the `LinearLayer` class returns an input gradient with the correct shape.\n","- `test_update` function tests if the `update` function of the `LinearLayer` class updates the weights and biases correctly.\n","- `test_shapes` function tests the shapes of the weight and bias gradients returned by the `backward` function.\n","\n","The `unittest.main` function runs the test cases and returns the results. By running these test cases, we can ensure that the `LinearLayer` class and its functions are working correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532292Z","start_time":"2023-04-10T16:34:59.506533Z"},"cell_id":"94db400b67564af0bf6eb58e9a508783","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":11,"execution_start":1682257937616,"source_hash":"1081b45e"},"outputs":[],"source":["import unittest\n","\n","class TestLinearLayer(unittest.TestCase):\n","    def setUp(self):\n","        self.input_size = 10\n","        self.output_size = 5\n","        self.linear_layer = LinearLayer(self.input_size, self.output_size, None, None)\n","\n","    def test_forward(self):\n","        x = np.random.randn(10, self.input_size) # 10 samples\n","        output = self.linear_layer.forward(x)\n","        self.assertEqual(output.shape, (10, self.output_size))\n","\n","    def test_backward(self):\n","        x = np.random.randn(10, self.input_size) # 10 samples\n","        output = self.linear_layer.forward(x)\n","        grad_output = np.random.randn(10, self.output_size)\n","        grad_input = self.linear_layer.backward(grad_output)\n","        self.assertEqual(grad_input.shape, (10, self.input_size))\n","\n","    def test_update(self):\n","        self.linear_layer.weights = np.array([[1, 2], [3, 4]], dtype=np.float32)\n","        self.linear_layer.bias = np.array([[1, 2]], dtype=np.float32)\n","        self.linear_layer.weights_grad = np.array([[1, 2], [3, 4]], dtype=np.float32)\n","        self.linear_layer.bias_grad = np.array([[1, 2]], dtype=np.float32)\n","\n","        self.linear_layer.update(0.1)\n","        self.assertTrue(np.allclose(self.linear_layer.weights, np.array([[0.9, 1.8], [2.7, 3.6]])))\n","        self.assertTrue(np.allclose(self.linear_layer.bias, np.array([[0.9, 1.8]])))\n","\n","    def test_shapes(self):\n","        x = np.random.randn(1, self.input_size)\n","        output = self.linear_layer.forward(x)\n","        grad_output = np.random.randn(1, self.output_size)\n","        grad_input = self.linear_layer.backward(grad_output)\n","\n","        self.assertEqual(self.linear_layer.weights_grad.shape, (self.input_size, self.output_size))\n","        self.assertEqual(self.linear_layer.bias_grad.shape, (1, self.output_size))\n","\n","unittest.main(argv=[''], verbosity=2, exit=False)"]},{"cell_type":"markdown","metadata":{"cell_id":"d0b9250617ab4c56a245be63572f7627","deepnote_cell_type":"markdown"},"source":["## Task 3 — Creating a single layer network\n","\n","> Create a neural network in numpy with a hidden linear layer and an output node. Train the network to correctly identify the digit 4 (i.e., the output should be 1 for this digit and 0 for all other digits). Train the network on the training data and evaluate it on test data. Use a suitable loss function as well as accuracy function and give their mathematical definition. Justify your choice by weighing the advantages and disadvantages. Briefly discuss other options for Loss and Accuracy. *Translated with [DeepL](https://www.deepl.com/translator)*"]},{"cell_type":"markdown","metadata":{"cell_id":"883cd30899904e9eb098dc17c964a79e","deepnote_cell_type":"markdown"},"source":["### Loss function\n","**Binary Cross Entropy (BCE)** is a suitable loss function for the MNIST binary classification task of classifying if a digit is a 4 or not, mainly because it is specifically designed for binary classification problems. Mathematically, BCE calculates the negative log likelihood of the correct class, penalizing the model heavily when its prediction is far from the ground truth. This characteristic makes BCE sensitive to the difference between predicted probabilities and actual labels, thereby promoting accurate classification.\n","\n","In comparison, Focal loss is designed to address class imbalance issues by down-weighting the loss contribution from easily classified examples. While this property makes Focal loss an optimal choice for the given task with a 9:91 class distribution, the inability to implement it necessitated the use of BCE. Nonetheless, BCE still provides a reasonable performance in such scenarios. To address the imbalance issue, remixing the training and test set can be considered. \n","\n","Mean Squared Error (MSE) loss, on the other hand, is suboptimal for classification tasks, as it focuses on the squared difference between predictions and ground truth. This leads to less effective gradient updates, particularly in the presence of saturating activation functions like sigmoid. Moreover, MSE is more suitable for regression tasks rather than binary classification, and its use may result in subpar performance for the given task.\n","\n","In summary, Binary Cross Entropy is a suitable choice for the MNIST binary classification task, especially when Focal loss is not implementable. While BCE may not directly address class imbalance, its mathematical properties make surely a solid choice.\n","\n","\n","> The mathematical definition can be viewed in the chapter \"Loss functions > Binary Cross-Entropy\""]},{"cell_type":"markdown","metadata":{"cell_id":"a49dc6b6bdfd4623bfd0433e333fec33","deepnote_cell_type":"markdown"},"source":["### Accuracy function\n","The **F1 score** is an appropriate accuracy metric for the MNIST binary classification task of identifying 4s due to the nature of the task and what it expects. The goal is to identify 4s and non-4s alike. In order to make accuracy a reflective method of judging the model, it is important to understand if the model is successful in identifying 4s (TP). The F1 score combines precision (the proportion of true positive 4s among all predicted 4s) and recall (the proportion of true positive 4s among all actual 4s) through their harmonic mean. This balances false positives and false negatives, ensuring that both are adequately addressed.\n","\n","In contrast, metrics like naive-accuracy can be misleading, as a classifier that predicts all non-4s will achieve high accuracy (91%) despite failing to identify any 4s (imbalanced dataset 1:10 which is handled in a chapter further down). While other metrics like precision, recall, or the area under the ROC curve (AUC-ROC) can provide useful insights, the F1 score remains a more comprehensive and balanced choice for this specific classification task. The F1 score is calculated like this:\n","\n","$$\n","\\begin{aligned}\n","& \\text { Precision }=\\frac{T P}{T P+F P} \\\\\n","& \\text { Recall }=\\frac{T P}{T P+F N} \\\\\n","& F 1 \\text { score }=2 * \\frac{\\text { Precision } * \\text { Recall }}{\\text { Precision }+ \\text { Recall }}\n","\\end{aligned}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1a56b2c656a54b2c88e4385a25e74b4b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10,"execution_start":1682257937646,"source_hash":"481c7574"},"outputs":[],"source":["def f1_score(y_pred, y_true):\n","    \"\"\"\n","    Compute the F1 score of the predictions.\n","\n","    :param y_pred: predicted labels\n","    :param y_true: true labels\n","    :return: F1 score\n","    \"\"\"\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n","\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","    fn = np.sum((y_true == 1) & (y_pred == 0))\n","\n","    if tp + fp == 0 or tp + fn == 0:\n","        return 0\n","\n","    precision = tp / (tp + fp)\n","    recall = tp / (tp + fn)\n","\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","    return f1"]},{"cell_type":"markdown","metadata":{"cell_id":"54b19a3a39954c43a1100b02d6eafc01","deepnote_cell_type":"markdown"},"source":["Confusion matrices are useful for evaluating binary classification models because they provide information on true positives, true negatives, false positives, and false negatives. While the F1 score is a useful metric, confusion matrices provide additional insights for improving model performance and understanding in which direct the model is predicting."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5c8d0259425749d6894f88fdce88ea82","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":11,"execution_start":1682257937646,"source_hash":"53aa4b89"},"outputs":[],"source":["def plot_confusion_matrix(cm, class_labels=None, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n","    \"\"\"\n","    Plot a confusion matrix.\n","\n","    :param cm: confusion matrix\n","    :param class_labels: class labels (default: None)\n","    :param normalize: normalize the confusion matrix (default: False)\n","    :param title: title of the plot (default: 'Confusion Matrix')\n","    :param cmap: colormap (default: plt.cm.Blues)\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        fmt = '.2f'\n","    else:\n","        fmt = 'd'\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","\n","    if class_labels is None:\n","        class_labels = ['0', '1']\n","\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           xticklabels=class_labels, yticklabels=class_labels,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    fig.tight_layout()\n","    plt.show()\n","\n","def confusion_matrix(y_true, y_pred, plot=True, plot_title='Confusion Matrix'):\n","    \"\"\"\n","    Compute the confusion matrix of the predictions.\n","\n","    :param y_true: true labels\n","    :param y_pred: predicted labels\n","    :param plot: plot the confusion matrix (default: True)\n","    :param plot_title: title of the plot (default: 'Confusion Matrix')\n","    :return: confusion matrix\n","    \"\"\"\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n","\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","    tn = np.sum((y_true == 0) & (y_pred == 0))\n","    fn = np.sum((y_true == 1) & (y_pred == 0))\n","\n","    cm = np.array([[tn, fp], [fn, tp]])\n","\n","    if plot:\n","        plot_confusion_matrix(cm, title=plot_title)\n","\n","    return cm"]},{"cell_type":"markdown","metadata":{"cell_id":"2e14485d9ad749569e704ae3c0ee2188","collapsed":false,"deepnote_cell_type":"markdown"},"source":["### Tracking network development\n","In order to track the development of the training process, we define a class that stores the training and validation loss and accuracy for each epoch. We also define a function that plots the training and validation loss and accuracy for each epoch. This allows us to see how the training process develops over time.\n","\n","The `NetworkDevelopment` class is responsible for storing the training and validation loss and accuracy for each epoch during the training process of a neural network. It has the following methods:\n","\n","- `__init__(self, total_epochs)`: Initializes the class with the total number of epochs the network will be trained for. It initializes empty lists to store the training and validation losses and accuracies for each epoch.\n","- `add_epoch(self, epoch_number, loss, acc_train, acc_test)`: Adds a new development step to the network development by appending the loss, accuracy for training data, and accuracy for validation data of the current epoch to their respective lists. This method returns a string containing the epoch number, loss, and accuracies for training and validation data.\n","- `plot(self)`: Plots the training and validation loss and accuracy for each epoch using matplotlib. It creates two subplots, one for loss over epochs and another for accuracy over epochs. The method returns the plot.\n","- `summary(self)`: Prints the final loss and accuracy of the training and validation data, and the average improvements per epoch. It calls the `plot()` method to plot the loss and accuracy data, and then prints the average accuracy change per epoch for both the training and validation data, and the average loss change per epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5a9cb490bd964d86a9f7975302632b97","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1682257937653,"source_hash":"de324cd"},"outputs":[],"source":["class NetworkDevelopment:\n","    \"\"\"\n","    Class to manage and visualize the development of a neural network over epochs.\n","\n","    :param total_epochs: total number of epochs\n","    \"\"\"\n","    def __init__(self, total_epochs):\n","        self.total_epochs = total_epochs\n","        self.losses = []\n","        self.accuracies_train = []\n","        self.accuracies_test = []\n","\n","    def add_epoch(self, epoch_number, loss, acc_train, acc_test):\n","        \"\"\"\n","        Add the loss and accuracy for an epoch.\n","\n","        :param epoch_number: epoch number\n","        :param loss: loss value\n","        :param acc_train: accuracy value on training set\n","        :param acc_test: accuracy value on test set\n","        :return: a string with information about the epoch\n","        \"\"\"\n","        self.losses.append(loss)\n","        self.accuracies_train.append(acc_train)\n","        self.accuracies_test.append(acc_test)\n","\n","        return f'Epoch {epoch_number}/{self.total_epochs} - loss: {loss:.4f} - acc_train: {acc_train:.4f} - acc_test: {acc_test:.4f}'\n","\n","    def plot(self):\n","        \"\"\"\n","        Plot the loss and accuracy over epochs.\n","        \"\"\"\n","        epochs = np.arange(1, self.total_epochs + 1)\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","        ax1.plot(epochs, self.losses)\n","        ax1.set_title('Loss over Epochs')\n","        ax1.set_xlabel('Epoch')\n","        ax1.set_ylabel('Loss')\n","        ax2.plot(epochs, self.accuracies_train, label='Training')\n","        ax2.plot(epochs, self.accuracies_test, label='Test')\n","        ax2.set_title('Accuracy over Epochs')\n","        ax2.set_xlabel('Epoch')\n","        ax2.set_ylabel('Accuracy')\n","        ax2.legend()\n","        plt.show()\n","\n","    def summary(self):\n","        \"\"\"\n","        Print a summary of the network's development over epochs.\n","        \"\"\"\n","        self.plot()\n","\n","        print(f'avg acc change / epoch (Training set): {np.mean(np.diff(self.accuracies_train)):.4f}')\n","        print(f'avg acc change / epoch (Test set): {np.mean(np.diff(self.accuracies_test)):.4f}')\n","        print(f'avg loss change / epoch: {np.mean(np.diff(self.losses)):.4f}')\n"]},{"cell_type":"markdown","metadata":{"cell_id":"2cf7b6612d244d069af11050f77a4a42","deepnote_cell_type":"markdown"},"source":["### Batch Training\n","\n","Training a neural network on large datasets can be computationally expensive and time-consuming. One way to mitigate this challenge is to use batch training, where instead of updating the model's weights after every single data point, the model's parameters are updated after processing a fixed number of data points, known as a batch.\n","\n","The function `get_batches(x, y, batch_size)` creates batches of images and labels from the training set. The number of images in a batch is defined by the batch size. The batch size is a hyperparameter that can be tuned to improve the training process. The batch size is a trade-off between the number of images used for training and the number of training steps per epoch. A larger batch size results in fewer training steps per epoch, but the training process is less accurate. A smaller batch size results in more training steps per epoch, but the training process is more accurate.\n","\n","- `x`: A numpy array containing the input data.\n","- `y`: A numpy array containing the target data.\n","- `batch_size`: An integer representing the size of each batch.\n","\n","The function first calculates the number of batches needed to cover the entire dataset by dividing the length of the input data by the batch size. The function then shuffles the input and target data using a random permutation of indices to ensure that the data is not ordered in any particular way. \n","\n","The function then uses a generator to yield batches of input and target data. It does this by iterating over the range of indices from 0 to the number of batches times the batch size, with a step size of batch size. For each iteration, it slices the input and target data arrays to extract a batch of size batch_size, and then yields this batch as a tuple of (x_batch, y_batch)."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"01edf8fdd92847fabdae8a44f0642c30","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1682257937663,"source_hash":"6e1238e4"},"outputs":[],"source":["def get_batches(x, y, batch_size):\n","    \"\"\"\n","    Generate batches of data.\n","\n","    :param x: input data\n","    :param y: target data\n","    :param batch_size: size of each batch\n","    :return: a generator object that yields batches of data\n","    \"\"\"\n","    n_batches = len(x) // batch_size\n","    \n","    # shuffle data before creating batches\n","    idx = np.random.permutation(len(x))\n","    x = x[idx]\n","    y = y[idx]\n","\n","    for i in range(0, n_batches * batch_size, batch_size):\n","        x_batch = x[i:i+batch_size]\n","        y_batch = y[i:i+batch_size]\n","        yield x_batch, y_batch"]},{"cell_type":"markdown","metadata":{"cell_id":"16885f69dd1149bf8be4f36f0b868fb1","deepnote_cell_type":"markdown"},"source":["### Single-Layer Network\n","\n","This code defines a single-layer neural network class `SingleLayerNetwork` that inherits from the `LinearLayer` class. The `SingleLayerNetwork` class has several methods:\n","\n","- `__init__` initializes the network's parameters, including the input size, hidden layer size, output size, loss function, and its derivative. It also initializes the hidden and output layers using `LinearLayer` with the appropriate activation functions.\n","- `forward` computes the forward pass of the network by propagating the input through the hidden and output layers using their `forward` functions.\n","- `backward` computes the backward pass of the network by propagating the gradient of the loss with respect to the output through the output and hidden layers using their `backward` functions.\n","- `update` updates the weights and biases of the hidden and output layers using their `update` functions and the learning rate.\n","- `train` trains the network on the given training data `X_train` and labels `y_train` for a specified number of epochs using mini-batch gradient descent. It computes the accuracy and loss on the training and test data at each epoch and outputs the results if `output` is `True`.\n","- `predict` makes predictions on the given input data `X` by calling the `forward` function and returning the predictions.\n","- `evaluate` evaluates the accuracy of the network's predictions on the given input data `X` and labels `y`.\n","- `summary` prints a summary of the network's development using the `NetworkDevelopment` class.\n","\n","Overall, this class defines a simple single-layer neural network and its training and evaluation methods."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.532515Z","start_time":"2023-04-10T16:34:59.522530Z"},"cell_id":"cdf28d7921184470bf514a8d7e9398fc","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1682257937668,"source_hash":"f30c9725"},"outputs":[],"source":["class SingleLayerNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, loss_fn, loss_fn_grad, acc_fn):\n","        \"\"\"\n","        Initialize the SingleLayerNetwork.\n","\n","        :param input_size: size of input layer\n","        :param hidden_size: size of hidden layer\n","        :param output_size: size of output layer\n","        :param loss_fn: loss function\n","        :param loss_fn_grad: gradient of the loss function\n","        :param acc_fn: accuracy function\n","        \"\"\"\n","        self.loss_fn = loss_fn\n","        self.loss_fn_grad = loss_fn_grad\n","        self.acc_fn = acc_fn\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.hidden_layer = LinearLayer(input_size, hidden_size, relu, relu_grad)\n","        self.output_layer = LinearLayer(hidden_size, output_size, sigmoid, sigmoid_grad)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Compute the forward pass of the network.\n","\n","        :param x: input data\n","        :return: output prediction\n","        \"\"\"\n","        self.x = x\n","        self.h = self.hidden_layer.forward(x)\n","        self.y_pred = self.output_layer.forward(self.h)\n","        return self.y_pred\n","\n","    def backward(self, y_true):\n","        \"\"\"\n","        Compute the backward pass of the network.\n","\n","        :param y_true: true target data\n","        :return: gradient of the output\n","        \"\"\"\n","        y_true = y_true.reshape(-1, 1)\n","        self.y_pred = self.y_pred.reshape(-1, 1)\n","\n","        grad_output = self.loss_fn_grad(self.y_pred, y_true)\n","\n","        grad_output = self.output_layer.backward(grad_output)\n","        grad_output = self.hidden_layer.backward(grad_output)\n","\n","        return grad_output\n","\n","    def update(self, lr):\n","        \"\"\"\n","        Update the weights and biases of the network.\n","\n","        :param lr: learning rate\n","        \"\"\"\n","        self.hidden_layer.update(lr)\n","        self.output_layer.update(lr)\n","\n","    def train(self, X_train, y_train, X_test, y_test, lr, epochs, batch_size, output=True):\n","        \"\"\"\n","        Train the network.\n","\n","        :param X_train: input data for the training set\n","        :param y_train: target data for the training set\n","        :param X_test: input data for the test set\n","        :param y_test: target data for the test set\n","        :param lr: learning rate\n","        :param epochs: number of epochs to train for\n","        :param batch_size: size of each batch\n","        :param output: whether or not to print output during training\n","        \"\"\"\n","        self.dev = NetworkDevelopment(total_epochs=epochs)\n","        \n","        for epoch in range(epochs):\n","            loss_list = []\n","            for x_batch, y_batch in get_batches(X_train, y_train, batch_size):\n","                y_pred = self.forward(x_batch)\n","\n","                loss = self.loss_fn(y_pred, y_batch)\n","                loss_list.append(loss)\n","\n","                self.backward(y_batch)\n","                self.update(lr)\n","\n","            acc_train = self.evaluate(X_train, y_train)\n","            acc_test = self.evaluate(X_test, y_test)\n","            avg_loss = np.mean(loss_list)\n","\n","            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n","            if output:\n","                print(epoch_str)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Make predictions using the trained network.\n","\n","        :param X: input data\n","        :return: predicted outputs\n","        \"\"\"\n","        return (self.forward(X) > .5).astype(int).reshape(-1, 1) # threshold at .5\n","\n","    def evaluate(self, X, y, acc_fn=None):\n","        \"\"\"\n","        Evaluate the accuracy of the network on a given dataset.\n","\n","        :param X: input data for the dataset\n","        :param y: target data for the dataset\n","        :param acc_fn: (optional) custom accuracy function to use\n","        :return: accuracy of the network on the dataset\n","        \"\"\"\n","        y_pred = self.predict(X).flatten()\n","\n","        if acc_fn is not None:\n","            return acc_fn(y_pred, y)\n","        \n","        return self.acc_fn(y_pred, y)\n","\n","    def summary(self):\n","        \"\"\"\n","        Plot the development of loss and accuracy during training and print a summary of the model's performance.\n","        \"\"\"\n","        self.dev.summary()\n"]},{"cell_type":"markdown","metadata":{"cell_id":"bcc1b384bb834097b5be2c452a19d750","deepnote_cell_type":"markdown"},"source":["### Training & Testing\n","\n","In order to train the model for the task at hand, we need to convert the $y$ data into a binary format. Any label representing a $4$ is converted into a $1$. Any label that does not represent a $4$ will be converted to a $0$."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f5ccf52699164379a95cd4de3ea51702","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1682257937668,"source_hash":"2f634c8"},"outputs":[],"source":["y_train_4_binary = (y_train == 4).astype(int)\n","y_test_4_binary = (y_test == 4).astype(int)"]},{"cell_type":"markdown","metadata":{"cell_id":"72b767dd03ac4ba5a2d7c6c3a021c107","deepnote_cell_type":"markdown"},"source":["We now train the Single-Layer Network on the train and test data with binary encoded labels."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"935a2ad2cf854aff97824bb761ebb575","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":64628,"execution_start":1682257937669,"source_hash":"4df15bb3"},"outputs":[],"source":["slp = SingleLayerNetwork(input_size=784, hidden_size=512, output_size=1, loss_fn=binary_cross_entropy, loss_fn_grad=binary_cross_entropy_grad, acc_fn=f1_score)\n","slp.train(X_train, y_train_4_binary, X_test , y_test_4_binary, lr=0.00001, epochs=15, batch_size=256)\n","slp.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"e97ef3e1317549cb9c9ab3215ca3d972","deepnote_cell_type":"markdown"},"source":["You can see immediately that the Accuracy on Test and Trainset with the F1 score used is constantly 0. The loss, on the other hand, seems to be constantly minimized."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"262dc6fb889e4ec3815486f1a90689ec","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13995109,"execution_start":1682244007203,"source_hash":"514835d7"},"outputs":[],"source":["slp.evaluate(X_test, y_test_4_binary)"]},{"cell_type":"markdown","metadata":{"cell_id":"a6d5c64588224dee87da1010deb03eb4","deepnote_cell_type":"markdown"},"source":["To understand why the model performs poorly, a confusion matrix can help to understand which class the model chooses for which label:"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3dcc5b4f2093458db92b5d3ee9a6fdca","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13994917,"execution_start":1682244007396,"source_hash":"4ea145a2"},"outputs":[],"source":["confusion_matrix(y_test_4_binary, slp.predict(X_test), plot=True, plot_title='Confusion Matrix (Test Set)')"]},{"cell_type":"markdown","metadata":{"cell_id":"682a541edb304456a7cc6bd68208d42d","deepnote_cell_type":"markdown"},"source":["The model always guesses a not-4 therefore not performing at all. Given the nature of the dataset though it represents the majority of the data present."]},{"cell_type":"markdown","metadata":{"cell_id":"f6aed784de894a46979a7fdc0553f7a5","deepnote_cell_type":"markdown"},"source":["### Modifying datasets to counter bias\n","\n","As we can see, training on the whole dataset leads to a strong bias against guessing non-4s. Although the loss is constantly decreasing, the model is not able to generalise the fact that there are also 4s to classify.\n","\n","To make sure that our model can identify 4s and non-4s equally well we created some subsets which will allow to train and test the model in a less biased way. To accomplish this, a mixed train and test set with equal distribution of 4s and non-4s is created (mix). All non-4 digits have equal representation in the set. \n","\n","The model will be trained on this mixed set and then evaluated against two other sets: 4s only and non-4s. The goal is to reach a high accuracy in all 3 test sets, since this means that the model can correctly identify 4s and non-4s and doesn't have strong bias towards either one. Afterwards the model will evaluate the default test set.\n","\n","Only after having a high accuracy in all 4 test sets(mix, 4s, non-4s and default), can we conclude the models success.\n","\n","The code is preparing three datasets, one containing only images of the number 4 (y=1), one with only images of other digits (y=0), and one with an equal representation of both. \n","\n","First, it separates the images of each digit in the training and testing sets. It calculates the number of samples to be selected from each class, which will be the same as the number of images of 4 in each subset, and selects those from all digits except for 4. This results in two subsets containing an equal number of 4 and non-4 images. \n","\n","The code shuffles the two subsets separately and converts the labels to binary. \n","\n","Afterward, the code selects only the 4 images from the original training and testing sets, shuffles them, and also converts their labels to binary. \n","\n","Finally, the code creates a new mixed dataset by taking an equal number of samples from both the 4s-only and non-4s-only subsets, shuffles it, and converts its labels to binary. The output of the total number of 4s and non-4s in the training set is printed."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:34:59.802387Z","start_time":"2023-04-10T16:34:59.023452Z"},"cell_id":"6ab0b52013d64143af5fc4850a7c4c91","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13994367,"execution_start":1682244007961,"source_hash":"4fb260e1"},"outputs":[],"source":["# Separate the digits\n","digits = [X_train[y_train == i] for i in range(10)]\n","digits_test = [X_test[y_test == i] for i in range(10)]\n","\n","# Calculate the number of samples for each not 4 class\n","samples_per_class = len(digits[4]) // 9\n","samples_per_class_test = len(digits_test[4]) // 9\n","\n","# Prepare the not 4s subset with equal representation of every other digit\n","X_train_not_4 = np.concatenate([digits[i][:samples_per_class] for i in range(10) if i != 4])\n","y_train_not_4 = np.concatenate([np.full(samples_per_class, i) for i in range(10) if i != 4])\n","\n","X_test_not_4 = np.concatenate([digits_test[i][:samples_per_class_test] for i in range(10) if i != 4])\n","y_test_not_4 = np.concatenate([np.full(samples_per_class_test, i) for i in range(10) if i != 4])\n","\n","# Shuffle the not 4s subset\n","indices_not_4 = np.random.permutation(len(X_train_not_4))\n","X_train_not_4 = X_train_not_4[indices_not_4]\n","y_train_not_4 = y_train_not_4[indices_not_4]\n","\n","indices_not_4_test = np.random.permutation(len(X_test_not_4))\n","X_test_not_4 = X_test_not_4[indices_not_4_test]\n","y_test_not_4 = y_test_not_4[indices_not_4_test]\n","\n","y_train_not_4_binary = (y_train_not_4 == 4).astype(int)\n","y_test_not_4_binary = (y_test_not_4 == 4).astype(int)\n","\n","\n","# Shuffle the 4s subset\n","indices_4 = np.random.permutation(len(digits[4]))\n","X_train_4_only = digits[4][indices_4]\n","y_train_4_only = np.full(len(digits[4]), 4)\n","\n","\n","indices_4_test = np.random.permutation(len(digits_test[4]))\n","X_test_4_only = digits_test[4][indices_4_test]\n","y_test_4_only = np.full(len(digits_test[4]), 4)\n","\n","y_train_4_only_binary = (y_train_4_only == 4).astype(int)\n","y_test_4_only_binary = (y_test_4_only == 4).astype(int)\n","\n","\n","# Create the mixed dataset with a 50:50 split\n","X_train_mix = np.concatenate((X_train_4_only[:len(X_train_not_4)], X_train_not_4))\n","y_train_mix = np.concatenate((y_train_4_only[:len(y_train_not_4)], y_train_not_4))\n","\n","X_test_mix = np.concatenate((X_test_4_only[:len(X_test_not_4)], X_test_not_4))\n","y_test_mix = np.concatenate((y_test_4_only[:len(y_test_not_4)], y_test_not_4))\n","\n","\n","# Shuffle the mixed dataset\n","indices_mix = np.random.permutation(len(X_train_mix))\n","X_train_mix = X_train_mix[indices_mix]\n","y_train_mix = y_train_mix[indices_mix]\n","\n","indices_mix_test = np.random.permutation(len(X_test_mix))\n","X_test_mix = X_test_mix[indices_mix_test]\n","y_test_mix = y_test_mix[indices_mix_test]\n","\n","# Convert the labels to binary\n","y_train_mix_binary = (y_train_mix == 4).astype(int)\n","y_test_mix_binary = (y_test_mix == 4).astype(int)\n","\n","# Print the count of 4s and not 4s in the train set\n","print(f\"4s: {np.sum(y_train_mix_binary == 1)} - Not 4s: {np.sum(y_train_mix_binary == 0)}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"da57850b45234166abc997b62a8a11a3","deepnote_cell_type":"markdown"},"source":["### Training & Testing with modified datasets\n","\n","With the new data sets at hand, let's see how the model performs when trained on the mixed set:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:35:41.520284Z","start_time":"2023-04-10T16:34:59.802503Z"},"cell_id":"7fde17044a684a5393d839f70c7aa318","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13994164,"execution_start":1682244008171,"source_hash":"17f866f0"},"outputs":[],"source":["slpm = SingleLayerNetwork(input_size=784, hidden_size=512, output_size=1, loss_fn=binary_cross_entropy, loss_fn_grad=binary_cross_entropy_grad, acc_fn=f1_score)\n","slpm.train(X_train_mix, y_train_mix_binary, X_test_mix, y_test_mix_binary, lr=0.00001, epochs=30, batch_size=128)\n","slpm.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"62eb9b594e984155a52424c825558988","deepnote_cell_type":"markdown"},"source":["We can observe, that the loss is fluctuating during training. There can be several reasons why that might be the case:\n","\n","- **Learning rate**: A high learning rate might cause the model to overshoot the optimal weights during gradient descent, causing oscillations in the loss function.\n","- **Stochastic nature**: When using mini-batch gradient descent, the random selection of samples can cause fluctuations in the loss function.\n","- **Model complexity**: If the model is too complex, it might overfit the training data, causing small fluctuations in the loss function.\n","- **Local minima**: The optimizer might be stuck in a local minimum, causing the loss to oscillate in a small range instead of converging to a global minimum."]},{"cell_type":"markdown","metadata":{"cell_id":"5ba7b679ac96441eb80b828a0d9d9da3","deepnote_cell_type":"markdown"},"source":["Now testing the model on the mixed data set the f1 score returns a high accuracy:"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"67eebd6b654b459a8e3c854d1884c70e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13900227,"execution_start":1682244102108,"source_hash":"cbc8eade"},"outputs":[],"source":["slpm.evaluate(X_test_mix, y_test_mix_binary)"]},{"cell_type":"markdown","metadata":{"cell_id":"755a26cafc69401badf17738c8f3441b","deepnote_cell_type":"markdown"},"source":["Looking at the confusion matrix, the classification is much more balanced, but still with a lot of false positives."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3bae65698dd34b63a9d532db20826cce","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13900215,"execution_start":1682244102127,"source_hash":"778528c8"},"outputs":[],"source":["confusion_matrix(y_test_4_binary, slpm.predict(X_test), plot_title=\"Single Layer Perceptron - Mixed\")"]},{"cell_type":"markdown","metadata":{"cell_id":"c4705741ee0f4916bf6694298ac3859a","deepnote_cell_type":"markdown"},"source":["Now, we check the performance on the other variations of the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a2bbe343276343fa8f2c747812779cc0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13899749,"execution_start":1682244102594,"source_hash":"6d63cc0c"},"outputs":[],"source":["acc_4_only = slpm.evaluate(X_test_4_only, y_test_4_only_binary)\n","print(f\"Accuracy on 4s: {acc_4_only}\")\n","\n","normal_acc = slpm.evaluate(X_test, y_test_4_binary)\n","print(f\"Accuracy on normal test set: {normal_acc}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"cb98ab51cc8e47aa9d38f95e47109d06","deepnote_cell_type":"markdown"},"source":["The neural network seems to perform well when tested on only the images of 4s (X_test_4_only), as indicated by the f1 score. This means that it is able to correctly identify most of the images of 4s in the test set.\n","\n","Finally, when the neural net is tested on the normal test set (X_test), which contains both 4s and non-4s, the f1 score means that it is not performing well in general, and is only slightly better than random guessing. This suggests that the model is not able to learn the patterns and features that distinguish 4s from non-4s, and thus is not able to accurately classify them."]},{"cell_type":"markdown","metadata":{"cell_id":"09c3ea78c6b6490783ed8a474c0fb0a6","deepnote_cell_type":"markdown"},"source":["In comparison, the model trained on the entire dataset does not generalise at all and performs poorly on each dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8b238b330af7427fb124f0df8c280f7e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13899466,"execution_start":1682244102898,"source_hash":"a4dbb3ec"},"outputs":[],"source":["acc_4_only = slp.evaluate(X_test_4_only, y_test_4_only_binary)\n","print(f\"Accuracy on 4s: {acc_4_only}\")\n","\n","normal_acc = slp.evaluate(X_test, y_test_4_binary)\n","print(f\"Accuracy on normal test set: {normal_acc}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"a3f6f3caf8854b7da9e1ab8a30db8dee","deepnote_cell_type":"markdown"},"source":["## Task 4 — Optimizing the single layer network\n","\n","> Train the network with different learning rates and hidden layer sizes. During training, track the evolution of the loss and accuracy functions on training and test data sets and decide which choice of learning rate and hidden layer size provides the best results in the least amount of time. *Translated with [DeepL](https://www.deepl.com/translator)*"]},{"cell_type":"markdown","metadata":{"cell_id":"37ddc3b060a74cb1a43b600a64ec2513","deepnote_cell_type":"markdown"},"source":["In order to find the right hyperparameters, we define a function that trains the network with different learning rates, epochs, hidden layer sizes and batch sizes. The result for each combination is then stored and analyzed.\n","\n","This brute force approach is not very efficient, but it allows us to find the best hyperparameters for the network.\n","\n","> **Note:** The brute force process takes a long time to complete. Therefore, we have commented out the code that runs the brute force process. The best result is described further below."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320020Z","start_time":"2023-04-10T16:41:02.154130Z"},"cell_id":"a09a603c4ae7447f9c4f2c81d5d1af20","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":7472002,"execution_start":1682250530363,"source_hash":"62af83d4"},"outputs":[],"source":["def get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes):\n","    return [(e, b, l, h) for e in epochs for b in batch_sizes for l in learning_rates for h in hidden_sizes]\n","\n","epochs = [10, 20, 30]\n","batch_sizes = [64, 256, 1024]\n","learning_rates = [0.001, 0.0001, 0.00001]\n","hidden_sizes = [256, 512]\n","\n","def test_brute_force_single_layer(param_combos, X_train, y_train, X_test, y_test):\n","    results = []\n","    best_acc = 0\n","    best_param_combo = None\n","\n","    for i in range(len(param_combos)):\n","        e, b, l, h = param_combos[i]\n","        try:\n","            network = SingleLayerNetwork(input_size=784, hidden_size=h, output_size=1, \n","                                        loss_fn=binary_cross_entropy, loss_fn_grad=binary_cross_entropy_grad,\n","                                        acc_fn=f1_score)\n","\n","            network.train(X_train, y_train, X_test, y_test, lr=l, epochs=e, batch_size=b, output=False)\n","            acc = network.evaluate(X_test, y_test)\n","\n","            results.append((e, b, l, h, acc))\n","            if acc > best_acc:\n","                best_acc = acc\n","                best_param_combo = param_combos[i]\n","\n","            print(f\"combo {i+1}/{len(param_combos)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {acc:.4f}\")\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            continue\n","\n","    return best_acc, best_param_combo\n","\n","sl_combos = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","print(f\"Testing {len(sl_combos)} parameter combinations\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"97c513a3df4f49b0a9d73c1ff8b6f4ac","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":7468999,"execution_start":1682250533365,"source_hash":"c06dbd86"},"outputs":[],"source":["# test_brute_force_single_layer(sl_combos, X_train_mix, y_train_mix_binary, X_test_mix, y_test_mix_binary)"]},{"cell_type":"markdown","metadata":{"cell_id":"65a4936ddac84b2db77dcb9c60b785e0","deepnote_cell_type":"markdown"},"source":["The best result was achieved with the following hyperparameters:\n","\n","Acc: 0.8811050814010852\n","(epochs=20, batch_size=64, lr=1e-05, hidden_size=512))\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"705cca42b65945a096544dc4d0fc678a","deepnote_cell_type":"markdown"},"source":["## Task 5 — Classifying all 10 numbers\n","\n","> Extend the network to 3 hidden layers with the same size and 10 outputs. The goal is the correct classification of all digits. Use a suitable loss function as well as accuracy function and give their mathematical definition. Justify your choice and briefly discuss other possibilities. Vary the learning rate and the size of the hidden layers and choose the best result. *Translated with [DeepL](https://www.deepl.com/translator)*"]},{"cell_type":"markdown","metadata":{"cell_id":"bff220d8278644abbfb71438cd460892","deepnote_cell_type":"markdown"},"source":["### Loss function\n","\n","**Cross Entropy loss** is a preferred choice for the MNIST classification task due to its ability to handle probabilistic outputs effectively. This is especially useful when dealing with multi-class classification problems like MNIST, which involves 10 different digit classes (0-9). Mathematically, Cross Entropy loss quantifies the difference between the predicted probability distribution and the true distribution, incentivizing the model to increase the probability of correct predictions.\n","\n","Comparatively, Mean Squared Error (MSE) is more suited for regression problems and lacks the same discriminative power as Cross Entropy when handling classification tasks. MSE doesn't exploit the probabilistic nature of class predictions, which can lead to slower convergence and less accurate results.\n","\n","While Kullback-Leibler (KL) Divergence also measures the dissimilarity between two probability distributions, it is asymmetric and can be computationally expensive. Cross Entropy, on the other hand, is a symmetric and more computationally efficient alternative for classification tasks. \n","\n","In conclusion, Cross Entropy loss is an optimal choice for the MNIST classification problem due to its capability to handle probabilistic outputs, faster convergence, and computational efficiency compared to alternatives like MSE and KL Divergence.\n","\n","> The mathematical definition can be viewed in the chapter \"Loss functions > Cross-Entropy\""]},{"cell_type":"markdown","metadata":{"cell_id":"54d1cdf03b554735bcc134df690d7bae","deepnote_cell_type":"markdown"},"source":["### Accurcay function\n","The **standard accuracy metric** is the most suitable choice for the MNIST classification task, as it effectively captures the proportion of correctly classified examples. Mathematically, accuracy is calculated by dividing the number of correct predictions by the total number of predictions, yielding a value between 0 and 1. This simple metric is easy to interpret and understand, making it ideal for evaluating the performance of a classifier on a balanced dataset like MNIST, where each digit class is equally represented.\n","\n","$$\n","\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{\\sum_{i=1}^{n} I(y_i = \\hat{y}_i)}{n}\n","$$\n","\n","Alternatives like precision, recall, and F1-score are generally more informative for imbalanced datasets, where certain classes are underrepresented. However, since the MNIST dataset for all classes (see exercise 1) is balanced, these metrics provide little additional value compared to standard accuracy. Moreover, the Area Under the Receiver Operating Characteristic curve (AUROC) and Area Under the Precision-Recall curve (AUPRC) are more relevant for binary classification problems or when a probabilistic output is required, which is not the case for MNIST.\n","\n","In conclusion, the standard accuracy metric is the most appropriate choice for the MNIST classification task due to its simplicity, interpretability, and effectiveness in capturing the classifier's performance on balanced datasets. Alternative metrics like precision, recall, F1-score, AUROC, and AUPRC provide limited additional insight for this specific problem."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5cfcf117efdc479f80289d142278ae74","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"27c5fcba"},"outputs":[],"source":["def naive_acc(y_pred, y_true):\n","    return np.mean(y_pred == y_true)"]},{"cell_type":"markdown","metadata":{"cell_id":"3e3e6c3d21d240919ac36c69b3f32290","deepnote_cell_type":"markdown"},"source":["### One-Hot Encoding\n","\n","One-hot encoding is a way to represent categorical data in a numerical format. In the case of MNIST, the target variable is the digit that each image represents, which can take on values from 0 to 9. One-hot encoding converts each digit into a binary vector of length 10, where each element of the vector represents a possible digit value. For example, the digit 3 would be represented as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], where the fourth element is a 1 and the other elements are 0."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"cff6da3bc5134eec83408f32fae67585","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":153472503,"execution_start":1682104529861,"source_hash":"52b66065"},"outputs":[],"source":["def one_hot_encode(y):\n","    y_one_hot = np.zeros((len(y), 10))\n","    y_one_hot[np.arange(len(y)), y] = 1\n","    return y_one_hot\n","\n","y_train_one_hot = one_hot_encode(y_train)\n","y_test_one_hot = one_hot_encode(y_test)"]},{"cell_type":"markdown","metadata":{"cell_id":"5b5fc2b8bcc64577be71a7e4091be690","deepnote_cell_type":"markdown"},"source":["### Multi-Layer Network\n","\n","The `MultiLayerNetwork` class represents a neural network with multiple layers, and is used for training and evaluating the model on a given dataset. The class implements the functions necessary for forward and backward propagation in the network. \n","\n","The `__init__` function initializes the network with the given input size, hidden size, output size, loss function, loss function gradient and the accuracy function used. It also initializes the various layers of the network, including three hidden layers and an output layer.\n","\n","The `forward` function performs forward propagation on a given input `x`, computing the predicted output of the network. The `backward` function performs backward propagation to compute the gradients of the network's weights and biases with respect to the loss, given the true output `y_true`. The `update` function updates the weights and biases of the network based on the gradients computed in the backward pass.\n","\n","The `train` function trains the network on the given training data `X_train` and `y_train`, using the given learning rate, number of epochs, and batch size. It also computes the training and validation accuracy and loss for each epoch using the `evaluate` function, and stores these values in a `NetworkDevelopment` object. \n","\n","The `predict` function takes a set of input data `X` and returns the predicted classes of each input based on the current weights and biases of the network. The `evaluate` function takes a set of input data `X` and corresponding true output `y`, and computes the accuracy of the predicted classes relative to the true classes.\n","\n","Finally, the `summary` function prints a summary of the network's training and validation accuracy and loss for each epoch, as well as the average improvements per epoch, using the `NetworkDevelopment` object."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:41:02.320088Z","start_time":"2023-04-10T16:41:02.154747Z"},"cell_id":"76ff4df53cf4459297da44c4a0676433","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":14159165,"execution_start":1682243843201,"source_hash":"1eb9a744"},"outputs":[],"source":["class MultiLayerNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, loss_fn, loss_fn_grad, acc_fn):\n","        \"\"\"\n","        Initialize the MultiLayerNetwork.\n","        :param input_size: size of input layer\n","        :param hidden_size: size of hidden layer\n","        :param output_size: size of output layer\n","        :param loss_fn: loss function\n","        :param loss_fn_grad: gradient of the loss function\n","        :param acc_fn: accuracy function\n","        \"\"\"\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.loss_fn = loss_fn\n","        self.loss_fn_grad = loss_fn_grad\n","        self.acc_fn = acc_fn\n","\n","        self.hidden_layer1 = LinearLayer(input_size, hidden_size, relu, relu_grad)\n","        self.hidden_layer2 = LinearLayer(hidden_size, hidden_size, relu, relu_grad)\n","        self.hidden_layer3 = LinearLayer(hidden_size, hidden_size, relu, relu_grad)\n","        self.output_layer = LinearLayer(hidden_size, output_size, softmax, softmax_grad)\n","\n","        self.dev = None\n","        self.hidden_output1 = None\n","        self.hidden_output2 = None\n","        self.hidden_output3 = None\n","        self.predicted_output = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Compute the forward pass of the network.\n","\n","        :param x: input data\n","        :return: output prediction\n","        \"\"\"\n","        self.hidden_output1 = self.hidden_layer1.forward(x)\n","        self.hidden_output2 = self.hidden_layer2.forward(self.hidden_output1)\n","        self.hidden_output3 = self.hidden_layer3.forward(self.hidden_output2)\n","        self.predicted_output = self.output_layer.forward(self.hidden_output3)\n","        return self.predicted_output\n","\n","    def backward(self, y_true):\n","        \"\"\"\n","        Compute the backward pass of the network.\n","\n","        :param y_true: true target data\n","        :return: gradient of the output\n","        \"\"\"\n","        gradient_output = self.loss_fn_grad(self.predicted_output, y_true)\n","\n","        gradient_output = self.output_layer.backward(gradient_output, self.predicted_output)\n","        gradient_output = self.hidden_layer3.backward(gradient_output, self.hidden_output3)\n","        gradient_output = self.hidden_layer2.backward(gradient_output, self.hidden_output2)\n","        gradient_output = self.hidden_layer1.backward(gradient_output, self.hidden_output1)\n","\n","        return gradient_output\n","\n","    def update(self, learning_rate):\n","        \"\"\"\n","        Update the weights of the network.\n","\n","        :param learning_rate: learning rate\n","        \"\"\"\n","        self.hidden_layer1.update(learning_rate)\n","        self.hidden_layer2.update(learning_rate)\n","        self.hidden_layer3.update(learning_rate)\n","        self.output_layer.update(learning_rate)\n","\n","    def train(self, X_train, y_train, X_test, y_test, learning_rate, epochs, batch_size, output=True):\n","        \"\"\"\n","        Train the network.\n","\n","        :param X_train: training data\n","        :param y_train: training target data\n","        :param X_test: test data\n","        :param y_test: test target data\n","        :param learning_rate: learning rate\n","        :param epochs: number of epochs\n","        :param batch_size: batch size\n","        :param output: whether to print the results\n","        \"\"\"\n","        self.dev = NetworkDevelopment(total_epochs=epochs)\n","\n","        for epoch in range(epochs):\n","            loss_list = []\n","            for X_batch, y_batch in get_batches(X_train, y_train, batch_size):\n","                predicted_output = self.forward(X_batch)\n","\n","                loss = self.loss_fn(predicted_output, y_batch)\n","                loss_list.append(loss)\n","\n","                self.backward(y_batch)\n","                self.update(learning_rate)\n","\n","            avg_loss = np.mean(loss_list)\n","            acc_train = self.evaluate(X_train, y_train)\n","            acc_test = self.evaluate(X_test, y_test)\n","\n","            epoch_str = self.dev.add_epoch(epoch+1, avg_loss, acc_train, acc_test)\n","            if output:\n","                print(epoch_str)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict the output of the network.\n","\n","        :param X: input data\n","        :return: predicted output\n","        \"\"\"\n","        predicted_output = self.forward(X)\n","        return np.argmax(predicted_output, axis=1)\n","\n","    def evaluate(self, X, y):\n","        \"\"\"\n","        Evaluate the network.\n","\n","        :param X: input data\n","        :param y: target data\n","        :return: accuracy\n","        \"\"\"\n","        predicted_classes = self.predict(X)\n","        true_classes = np.argmax(y, axis=1)\n","        return self.acc_fn(predicted_classes, true_classes)\n","\n","    def summary(self):\n","        \"\"\"\n","        Print a summary of the network development.\n","        \"\"\"\n","        self.dev.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"c6190640c0474799bbc60c598b9704ec","deepnote_cell_type":"markdown"},"source":["### Training & Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:43:57.487217Z","start_time":"2023-04-10T16:41:02.154802Z"},"cell_id":"df728b2730004304b30f415eb11d9449","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":153472489,"execution_start":1682104529876,"source_hash":"f2c18c71"},"outputs":[],"source":["mln = MultiLayerNetwork(input_size=784, hidden_size=256, output_size=10, loss_fn=cross_entropy, loss_fn_grad=cross_entropy_grad, acc_fn=naive_acc)\n","mln.train(X_train, y_train_one_hot, X_test, y_test_one_hot, learning_rate=0.01, epochs=10, batch_size=64)\n","mln.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"c959c05c47464f15853d1716b281c1ab","deepnote_cell_type":"markdown"},"source":["We can see that the loss is constantly decreasing. Overall, the model learns very well from the data, which is reflected in the final accuracy score:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-04-10T16:43:57.716061Z","start_time":"2023-04-10T16:43:57.492708Z"},"cell_id":"5bc5d357a1b14d7c9fbf6b33977a0b66","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":153256174,"execution_start":1682104746191,"source_hash":"c784f5a5"},"outputs":[],"source":["mln.evaluate(X_test, y_test_one_hot)"]},{"cell_type":"markdown","metadata":{"cell_id":"ae0c94f8e4404d2ca996eb274c22a0ae","deepnote_cell_type":"markdown"},"source":["The accuracy indicates a good generalization of the data."]},{"cell_type":"markdown","metadata":{"cell_id":"a53b937b37184f5e82b22d8dc5989a50","deepnote_cell_type":"markdown"},"source":["### Finding the right hyperparameters"]},{"cell_type":"markdown","metadata":{"cell_id":"f203a49729d343c2964bd5d585f81c60","deepnote_cell_type":"markdown"},"source":["To find the right hyperparameters, we again perform a brute force grid search, testing different combinations of parameters. The best combination based on accuracy is then saved."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"624d97e63db24951880b3cf6bf790d99","collapsed":false,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":153255890,"execution_start":1682104746476,"source_hash":"e7147c7d"},"outputs":[],"source":["epochs = [10, 15 ,20]\n","batch_sizes = [16, 64, 256, 512]\n","learning_rates = [0.1, 0.01, 0.0001]\n","hidden_sizes = [256, 512]\n","combinations = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","\n","def test_brute_force_multi_layer(param_combos, X_train, y_train, X_test, y_test):\n","    \"\"\"\n","    Tests the MultiLayerNetwork model with a brute-force approach by iterating over all parameter combinations and\n","    evaluating their accuracy.\n","\n","    :param param_combos: List of parameter combinations to be tested.\n","    :param X_train: Training data.\n","    :param y_train: Training labels.\n","    :param X_test: Test data.\n","    :param y_test: Test labels.\n","\n","    :returns: A tuple containing the highest accuracy achieved and the corresponding parameter combination.\n","    \"\"\"\n","    results = []\n","    best_acc = 0\n","    best_param_combo = None\n","\n","    for i in range(len(param_combos)):\n","        try:\n","            e, b, l, h = param_combos[i]\n","            net = MultiLayerNetwork(input_size=784, hidden_size=h, output_size=10, loss_fn=cross_entropy, loss_fn_grad=cross_entropy_grad, acc_fn=naive_acc)\n","\n","            net.train(X_train, y_train, X_test, y_test, learning_rate=l, epochs=e, batch_size=b, output=False)\n","            acc = net.evaluate(X_test, y_test_one_hot)\n","\n","            results.append((e, b, l, h, acc))\n","            if acc > best_acc:\n","                best_acc = acc\n","                best_param_combo = param_combos[i]\n","\n","            print(f\"combo {i+1}/{len(param_combos)}: - Epochs: {e} - Batch size: {b} - Learning rate: {l} - Hidden size: {h} - Accuracy: {acc:.4f}\")\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            continue\n","\n","    return best_acc, best_param_combo\n","\n","ml_combos = get_param_combinations(epochs, batch_sizes, learning_rates, hidden_sizes)\n","print(f\"Testing {len(ml_combos)} parameter combinations\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8fef38a6d7854654a5d4feb6f03af9de","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":153255890,"execution_start":1682104746479,"source_hash":"cbbd23e2"},"outputs":[],"source":["# test_brute_force_multi_layer(ml_combos, X_train, y_train_one_hot, X_test, y_test_one_hot)"]},{"cell_type":"markdown","metadata":{"cell_id":"4d3a3626c95f47bdae11c555c4126a25","deepnote_cell_type":"markdown"},"source":["## Summary\n","This text consists of five tasks related to machine learning using the MNIST dataset. These tasks include data exploration, creating a neural network, training the model, evaluating its performance, and making improvements by changing learning rates and hidden layer sizes.\n","\n","In the first task, the MNIST dataset is processed, and the data is transformed, normalized, reshaped, and split into training and testing sets. The visualization gives us a first insight into how the dataset is built.\n","\n","The second task is to explain the implementation of a Linear Layer class that represents a linear layer in a neural network. The class has three main functions: `forward`, `backward`, and `update`. The `forward` function computes the output of the layer, the `backward` function computes the gradient of the loss with respect to the layer's weights, biases, and inputs, and the `update` function updates the weights and biases using gradient descent. The notebook also includes a unit test for the LinearLayer class using the `unittest` module to ensure the class and its functions are working correctly.\n","\n","In Task 3, the focus is on finding the best loss function and accuracy metric for binary classification of MNIST images. Specifically, the task is to classify whether a digit is a 4 or not. After evaluating several options, it was determined that the Binary Cross Entropy (BCE) loss function is best suited for this task. This is because it is intended for binary classification problems and provides accurate classification.\n","\n","The loss function Focal, while ideal for handling class imbalance problems, cannot be implemented in this particular case. However, BCE still provides reasonable performance. \n","\n","In terms of accuracy metrics, the F1 score is best suited for this task as it balances false positives and false negatives and ensures that both are adequately accounted for. Other metrics, such as naive accuracy, can be misleading because they do not reflect the accuracy of identifying 4s. The F1 is a more comprehensive and balanced choice for this particular classification task.\n","\n","The `SingleLayerNetwork` class is a simple neural network. It has a single hidden layer and is trained with mini-batch gradient descent. The methods of the network include initializing the network parameters, forward and backward passes, updating the weights and biases of the layers, training the network with given data, making predictions, and evaluating the accuracy of the network. The class inherits from the `LinearLayer` class and has some methods defined in it. The `NetworkDevelopment` class is utilized for generating a summary of the network's development.\n","\n","However, the model is heavily biased towards predicting non-4 digits, resulting in poor performance on test data. To counter this bias, the code creates three subsets of the MNIST dataset: one with only images of the number 4, one with only images of other digits, and one with an equal representation of both. The Single Layer model is trained on this modified dataset and evaluated against the three subsets individually, as well as the original test set. The goal is to reach a high accuracy in all four sets, indicating that the model can correctly identify 4s and non-4s and is not strongly biased towards either one.\n","\n","In the fourth task, the goal was to find the best combination of hyperparameters for a neural network. To accomplish this, a brute force method was used to train the network with different learning rates, epochs, hidden layer sizes, and batch sizes. The results for each combination were then stored and analyzed to determine the best combination of hyperparameters. After a long process, the best result was achieved with 20 epochs, batch size of 64, learning rate of 1e-05, and hidden size of 512, with an accuracy of 0.8811."]},{"cell_type":"markdown","metadata":{"cell_id":"25b3157c29c34b7d949e57d6ddf9016d","deepnote_cell_type":"markdown"},"source":["## Learning diary"]},{"cell_type":"markdown","metadata":{"cell_id":"a8fe97a7096e4fde9764566586eb922c","deepnote_cell_type":"markdown"},"source":["### KW13 — Week 1\n","\n","#### Accomplishments\n","First implementations of neural networks with `numpy` only (ChatGPT assisted) in order to get a better understanding of the concepts and logic. It turns out that ChatGPT is not always as great as it seems to mostly provide generalized solutions and it needs a lot of prompt engineering to get a desired output.\n","\n","#### Resources\n","- [ChatGPT](https://chat.openai.com) — <em>used for all exercises</em>\n","- [Neural networks Playlist by 3blue1brown](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) — <em>for getting a general understanding of NNs</em>\n","- [MIT Introduction to Deep Learning](https://www.youtube.com/watch?v=QDX-1M5Nj7s) — <em>for getting a general understanding of NNs</em>"]},{"cell_type":"markdown","metadata":{"cell_id":"4379364d436b42cab990da45fa1ace51","deepnote_cell_type":"markdown"},"source":["### KW14 — Week 2\n","\n","#### Accomplishments\n","Finished first <em>tentative</em> iteration of all exercises. We found that some weird behavior is still present like spiking in the loss-graph. Its implementation could be improved as we mostly copied the ChatGPT answer. Some further things like mathematical formulas and explanations are missing.\n","\n","#### Resources\n","- [ChatGPT](https://chat.openai.com) — <em>used for the first iteration of mathematical fromulas/explanations</em>\n"]},{"cell_type":"markdown","metadata":{"cell_id":"adfc5a75ab0a4b31b23a67d88daea60a","deepnote_cell_type":"markdown"},"source":["### KW15 — Week 3\n","\n","#### Accomplishments\n","Further research was done to identify better approaches: We figured out that **Focal Loss** should in theory be a better fit for the `SingleLayerNetwork` since it counteracts imbalanced sets (like in Task 3). However, we were unable to get it up and running with `numpy` only - All libraries use advanced techniques that require other dependencies which we are not allowed to use.\n","\n","#### Resources\n","- [ChatGPT](https://chat.openai.com) — <em>used for Focal Loss implementation/description</em>\n","- [Tsung-Yi Lin et al. “Focal Loss for Dense Object Detection”](https://arxiv.org/abs/1708.02002) — <em>to acquire a mathematical understanding of Focal Loss together with ChatGPT output(s)</em>"]},{"cell_type":"markdown","metadata":{"cell_id":"8f1ddb55d29e4b3c9b3eb875ed28b77c","deepnote_cell_type":"markdown"},"source":["### KW16 — Week 4\n","\n","#### Accomplishments\n","This week we wanted to finally decide on a robust loss function for both the `SingleLayerNetwork` and the `MultiLayerNetwork`: We found that there are a bunch of different loss functions that are used in the context of neural networks. We wanted to take a deeper look into how each of those functions would perform in the case of our `SingleLayerNetwork` and `MultiLayerNetwork`.\n","\n","**Results that lead to our decision**:\n","- **Kullback-Leibler**: Too complicated as its implementation has many requirements\n","- **Mean squared error (MSE)**: Performed weakly, focuses too hard on regression\n","- **Focal Loss**: Great in class imbalance scenarios (like Task 3) but seems to be extremely laborious to implement with just `numpy`\n","- **Cross-Entropy**: Ideal for Task 4 and 5 because of its probabilistic approach \n","    - **We finally decided to stick to Cross-Entropy for our `MultiLayerNetwork` and Binary Cross-Entropy for the `SingleLayerNetwork`**\n","<em>Further notes on each tested loss function were made in the corresponding sections in the notebook.</em>\n","\n","To measure performance we needed the right method. We first implemented a simple **Accuracy-Function** which just measured $\\frac{TP + TN}{TP + TN + FN + FP}$. After some further research and a conversation with Stefan we decided to implement the **F1-Score** as its more suited towards the task 3 because of the imbalance in the dataset.\n","\n","The F1 score is always 0 when training. To understand this result, we used a confusion matrix to understand in which direction the model was trained. We've learned that the model is constantly guessing non-4s in the dataset, leading to the high simple accuracy but low F1 score.\n","\n","Furthermore we also dug deeper into the subject of following **Activation-Functions**: \n","- **ReLU**: Seems to be best practice in most cases. It's very efficient and has no sigmoid-related issues like <em>vanishing gradients</em> which would lead to slower learning or convergence.\n","- **Leaky ReLU**: Vanilla ReLU but takes better care of dead neurons (neurons which are updated to never activate again).\n","- **Parametric ReLU (PReLU)**: Similar to Leaky ReLU but it implements a negative slope as a learnable parameter. However, we observed that PReLU is more prone to overfitting (at least in our case).\n","- **Sigmoid**: The main problem lies in the vanishing gradient problem which causes slow or stalled learning in neural networks. Vanishing gradients consist of a product of derivatives (chain rule) which approaches a value close to zero which makes the gradient <em>vanish</em>.\n","\n","We invested a lot in the implementation of the Foca loss function. The problem was certainly the gradient of the function. Our main problem was that the model loss did not decrease continuously and did not lead to better results. Also, testing with different hyperparameters was not successful. ChatGPT gave us different answers, and when asked if an implementation was correct, always corrected itself, leading to endless iterations of unsuccessful attempts to get a working gradient.\n","\n","In the end, we stuck with the BCE, as mentioned above. To address the imbalance, we remixed the datasets. This is far from optimal for the proportions of the dataset. When training on the mixed set, the BCE can reach its full potential and good results were obtained.\n","\n","#### Resources\n","- [ChatGPT](https://chat.openai.com) — <em>used to find alternative loss functions and their `numpy`-less implementations</em>\n","- [Kullback-Leibler Divergence (Wikipedia)](https://www.wikiwand.com/en/Kullback%E2%80%93Leibler_divergence) — <em>for getting a general mathematical understanding of KL-Divergence</em>\n","- [Understanding KL-Divergence (towardsdatascience.com)](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254) — <em>for getting a general practical understanding of KL-Divergence</em>\n","\n"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=88bc6171-47da-4cbc-96f1-ee851c7ac9ec' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"9a0b193b71ac4269a4d8b7f5d261604b","kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
