{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:17:53.083523Z",
     "end_time": "2023-04-04T16:17:54.438907Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:17:54.439528Z",
     "end_time": "2023-04-04T16:17:54.484384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
    "        self.bias = np.zeros(output_size)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # Backward pass of the layer\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0)\n",
    "        return grad_input, grad_weights, grad_bias\n",
    "\n",
    "    # Update the weights and bias based on the outcome of the backward pass\n",
    "    def update(self, grad_weights, grad_bias, learning_rate):\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:17:54.486049Z",
     "end_time": "2023-04-04T16:17:54.487671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8), axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    :param x: input\n",
    "    :return sigmoid(x)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8), axis=1, keepdims=True)\n",
    "\n",
    "def binary_cross_entropy_derivative(y_pred, y_true):\n",
    "    return -y_true / y_pred + (1 - y_true) / (1 - y_pred)\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    \"\"\"\n",
    "    Converts a vector of integers into a one-hot matrix.\n",
    "    :param Y: A vector of shape (1, m)\n",
    "    :return: A one-hot matrix of shape (m, n_classes)\n",
    "\n",
    "    Example:\n",
    "    >>> one_hot_encode(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "    array([ [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            ...,\n",
    "            ]\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot[np.arange(Y.size), Y] = 1\n",
    "    return one_hot.T\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:17:54.489611Z",
     "end_time": "2023-04-04T16:17:54.493215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_layer = LinearLayer(input_size, hidden_size)\n",
    "        # todo check if important\n",
    "        # self.hidden_layer = LinearLayer(hidden_size, hidden_size)\n",
    "\n",
    "        self.output_layer = LinearLayer(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z1 = self.input_layer.forward(X)\n",
    "        A1 = relu(Z1)\n",
    "\n",
    "        # A2 = self.hidden_layer.forward(A1)\n",
    "        # Z2 = relu()\n",
    "\n",
    "        Z2 = self.output_layer.forward(A1)\n",
    "        A2 = softmax(Z2)\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def backward(self, Z1, A1, Z2, A2, W2, X, Y):\n",
    "        m = Y.shape[0]\n",
    "        one_hot_Y = one_hot_encode(Y)\n",
    "\n",
    "        dZ2 = A2 - one_hot_Y # loss function\n",
    "        dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "        db2 = 1 / m * np.sum(dZ2)\n",
    "\n",
    "        dZ1 = W2.T.dot(dZ2) * relu_derivative(Z1)\n",
    "        dW1 = 1 / m * dZ1.dot(X.T)\n",
    "        db1 = 1 / m * np.sum(dZ1)\n",
    "\n",
    "    def update(self, learning_rate, grad_weights, grad_bias):\n",
    "        self.input_layer.update(grad_weights, grad_bias, learning_rate)\n",
    "        # self.hidden_layer.update(grad_weights, grad_bias, learning_rate)\n",
    "        self.output_layer.update(grad_weights, grad_bias, learning_rate)\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            hot_one = np.zeros((1, 10))\n",
    "            hot_one[0][y] = 1\n",
    "            loss = cross_entropy_loss(hot_one, y_pred)\n",
    "            print(f\"Epoch: {epoch}, Loss: {np.mean(loss)}\")\n",
    "            grad_input, grad_weights, grad_bias = self.backward(y_pred, y)\n",
    "            self.update(learning_rate, grad_weights, grad_bias)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:17:54.494977Z",
     "end_time": "2023-04-04T16:17:54.496868Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X_train = train_dataset.data.numpy().reshape(-1, 784) / 255\n",
    "X_train = X_train.T\n",
    "y_train = train_dataset.targets.numpy()\n",
    "\n",
    "# X_train_4 which only contains 4s\n",
    "X_train_4 = X_train[:, y_train == 4]\n",
    "y_train_4 = y_train[y_train == 4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28*28) / 255\n",
    "X_test = X_test.T\n",
    "y_test = test_dataset.targets.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:17:54.497860Z",
     "end_time": "2023-04-04T16:17:54.603037Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (784,60000) and (784,128) not aligned: 60000 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# train\u001B[39;00m\n\u001B[1;32m      2\u001B[0m nn \u001B[38;5;241m=\u001B[39m Network(\u001B[38;5;241m28\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 39\u001B[0m, in \u001B[0;36mNetwork.train\u001B[0;34m(self, X, y, learning_rate, epochs)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, learning_rate, epochs):\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m---> 39\u001B[0m         y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m         hot_one \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m))\n\u001B[1;32m     41\u001B[0m         hot_one[\u001B[38;5;241m0\u001B[39m][y] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[5], line 10\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m---> 10\u001B[0m     Z1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     A1 \u001B[38;5;241m=\u001B[39m relu(Z1)\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# A2 = self.hidden_layer.forward(A1)\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# Z2 = relu()\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[3], line 11\u001B[0m, in \u001B[0;36mLinearLayer.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[0;32m---> 11\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (784,60000) and (784,128) not aligned: 60000 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "# train\n",
    "nn = Network(28*28, 128, 10)\n",
    "nn.train(X_train, y_train, 0.05, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test\n",
    "y_pred = nn.forward(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
